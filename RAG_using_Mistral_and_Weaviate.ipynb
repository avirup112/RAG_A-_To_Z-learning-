{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7dbae5802c42476c99ee4041624673b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e4ac004e7cda4d178ada6d4a9d1d3785",
              "IPY_MODEL_ec90cde5df12411a85a78a8fe9637d95",
              "IPY_MODEL_d9d4cba9283945f8b68cd483e423f338"
            ],
            "layout": "IPY_MODEL_476f65e9b3644e359f139c81ed76566a"
          }
        },
        "e4ac004e7cda4d178ada6d4a9d1d3785": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c913f8c60a944d5a8ab5694f42d3d08a",
            "placeholder": "​",
            "style": "IPY_MODEL_13160718a40b48a68c93d2c78a7133c1",
            "value": "modules.json: 100%"
          }
        },
        "ec90cde5df12411a85a78a8fe9637d95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9eda7b2d037a4d35a1a0d9399aba22b1",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_47840cb7ed9a4740aabeb45afa925100",
            "value": 349
          }
        },
        "d9d4cba9283945f8b68cd483e423f338": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_438d3f0727754aacaf3eb21d506e64b7",
            "placeholder": "​",
            "style": "IPY_MODEL_9e5b4d91e5574bc1a54e8236cd2012de",
            "value": " 349/349 [00:00&lt;00:00, 32.7kB/s]"
          }
        },
        "476f65e9b3644e359f139c81ed76566a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c913f8c60a944d5a8ab5694f42d3d08a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13160718a40b48a68c93d2c78a7133c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9eda7b2d037a4d35a1a0d9399aba22b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47840cb7ed9a4740aabeb45afa925100": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "438d3f0727754aacaf3eb21d506e64b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e5b4d91e5574bc1a54e8236cd2012de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91d8dcae20ef4ff78ac104755cd8a023": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4752ece09fa04c879abf58d43bd13586",
              "IPY_MODEL_f7bdb35c5086421cb7f8282dfd772802",
              "IPY_MODEL_fcae5bd2d58a44f28a104d16b6750366"
            ],
            "layout": "IPY_MODEL_92362cc001a4427c9317d2fc0ab4483a"
          }
        },
        "4752ece09fa04c879abf58d43bd13586": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8be92ef60924ab199d18218006be3f1",
            "placeholder": "​",
            "style": "IPY_MODEL_d3b858f3f1774556ab9ea70016a3bae0",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "f7bdb35c5086421cb7f8282dfd772802": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76093b1827224034805e3a77a1b1fc80",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_efc1246f852245b0973fc71ef11a6a82",
            "value": 116
          }
        },
        "fcae5bd2d58a44f28a104d16b6750366": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88ecf0be3f764e5492ca556c9649b038",
            "placeholder": "​",
            "style": "IPY_MODEL_9eec71b51326410da37e5c2edfcb7e08",
            "value": " 116/116 [00:00&lt;00:00, 12.4kB/s]"
          }
        },
        "92362cc001a4427c9317d2fc0ab4483a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8be92ef60924ab199d18218006be3f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3b858f3f1774556ab9ea70016a3bae0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76093b1827224034805e3a77a1b1fc80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efc1246f852245b0973fc71ef11a6a82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "88ecf0be3f764e5492ca556c9649b038": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9eec71b51326410da37e5c2edfcb7e08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81b97280c4e5462a95ac1f8c3084e5ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ced55ca2bfab4df08eecd3e66b026ffc",
              "IPY_MODEL_2c691816970d40518eb9389ec1d3a29f",
              "IPY_MODEL_1afe2bdf1dc24909b986d3045ea711a2"
            ],
            "layout": "IPY_MODEL_dc342293a3134ceda5f2e65b2140d52a"
          }
        },
        "ced55ca2bfab4df08eecd3e66b026ffc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20b28a9816be4b1286823e9bae6bedb1",
            "placeholder": "​",
            "style": "IPY_MODEL_bd2852a757384a489ee29250668fc41f",
            "value": "README.md: "
          }
        },
        "2c691816970d40518eb9389ec1d3a29f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_083464f5c2ec48b5ac77d57b0753e5de",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fde9adc74d744ae3bd59074068729ed8",
            "value": 1
          }
        },
        "1afe2bdf1dc24909b986d3045ea711a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cbc4eca3cf74d5097b1efa5611b8913",
            "placeholder": "​",
            "style": "IPY_MODEL_897e6650cfa3493b90d3dbe6d75bbe4b",
            "value": " 11.6k/? [00:00&lt;00:00, 956kB/s]"
          }
        },
        "dc342293a3134ceda5f2e65b2140d52a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20b28a9816be4b1286823e9bae6bedb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd2852a757384a489ee29250668fc41f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "083464f5c2ec48b5ac77d57b0753e5de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "fde9adc74d744ae3bd59074068729ed8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9cbc4eca3cf74d5097b1efa5611b8913": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "897e6650cfa3493b90d3dbe6d75bbe4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e85969b5931c463db94cb1a829943481": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c415d1c611345d7961fa8f88308d49a",
              "IPY_MODEL_f657e827a47f49e6bac47b4ba7ab391e",
              "IPY_MODEL_dc7e1c2f37dc4b0eb9df9ec88fcac088"
            ],
            "layout": "IPY_MODEL_5d4ec26e528c491e9eda03f11c9b1917"
          }
        },
        "2c415d1c611345d7961fa8f88308d49a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_629bbaa76d3c416290be9b26f632171e",
            "placeholder": "​",
            "style": "IPY_MODEL_caa1cccd4a484b83927c786b03058862",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "f657e827a47f49e6bac47b4ba7ab391e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21d1e52587b2456a906dbfca579f0fb9",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e0860e85978e41d58de1bb018ea07f0e",
            "value": 53
          }
        },
        "dc7e1c2f37dc4b0eb9df9ec88fcac088": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63314609fb7b4fd98a7c92648df7532c",
            "placeholder": "​",
            "style": "IPY_MODEL_c887f68b136d4ed290e0d1963c4a48fe",
            "value": " 53.0/53.0 [00:00&lt;00:00, 6.13kB/s]"
          }
        },
        "5d4ec26e528c491e9eda03f11c9b1917": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "629bbaa76d3c416290be9b26f632171e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "caa1cccd4a484b83927c786b03058862": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21d1e52587b2456a906dbfca579f0fb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0860e85978e41d58de1bb018ea07f0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "63314609fb7b4fd98a7c92648df7532c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c887f68b136d4ed290e0d1963c4a48fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a5c8305e99a4fa982f8678b2ace40ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0236288dfbd04f9cbd4a2d3321028017",
              "IPY_MODEL_c0dadbeb992d439d85d50a07bb7a4956",
              "IPY_MODEL_1ae4fba191df4ec4b11e6a526a0cc8aa"
            ],
            "layout": "IPY_MODEL_66cc72f5c859409cb6842824e7053a69"
          }
        },
        "0236288dfbd04f9cbd4a2d3321028017": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3f1e709c3d04c9a94695508c70df172",
            "placeholder": "​",
            "style": "IPY_MODEL_3b4ff5e7b9b14495a1a81a6bde26b792",
            "value": "config.json: 100%"
          }
        },
        "c0dadbeb992d439d85d50a07bb7a4956": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b02235df6cf4ea79e4c718bf19fe718",
            "max": 571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b07ac377d194960871ff3f61cd000a3",
            "value": 571
          }
        },
        "1ae4fba191df4ec4b11e6a526a0cc8aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ed573e8db994ff383dd2d43feea3afe",
            "placeholder": "​",
            "style": "IPY_MODEL_8a5f4280f6e741c6b346d0333234979e",
            "value": " 571/571 [00:00&lt;00:00, 75.3kB/s]"
          }
        },
        "66cc72f5c859409cb6842824e7053a69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3f1e709c3d04c9a94695508c70df172": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b4ff5e7b9b14495a1a81a6bde26b792": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b02235df6cf4ea79e4c718bf19fe718": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b07ac377d194960871ff3f61cd000a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6ed573e8db994ff383dd2d43feea3afe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a5f4280f6e741c6b346d0333234979e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "060a441463e4443db463cbe52e8422ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ac9a5dc72aa34317a79941a2cacab034",
              "IPY_MODEL_87fd981dd4c643c387fcf6524d19b10f",
              "IPY_MODEL_dd13503524424bd187040ccc4e971983"
            ],
            "layout": "IPY_MODEL_f5777a1146db46f2b22ef85a03398bde"
          }
        },
        "ac9a5dc72aa34317a79941a2cacab034": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41ca854612f54c30aa9a941632d7b62f",
            "placeholder": "​",
            "style": "IPY_MODEL_6dc1b285a36f41c49b8da43a770c0ed1",
            "value": "model.safetensors: 100%"
          }
        },
        "87fd981dd4c643c387fcf6524d19b10f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1c404fc2afb4d20b3e95aa316a8eca6",
            "max": 437971872,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aa37d96711ee4004bf8614ed159b48b7",
            "value": 437971872
          }
        },
        "dd13503524424bd187040ccc4e971983": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a9cf94b282b4ba5a958816dbc61c5cb",
            "placeholder": "​",
            "style": "IPY_MODEL_aec60272e09d4c6fb4d36b295e60dbc6",
            "value": " 438M/438M [00:08&lt;00:00, 48.1MB/s]"
          }
        },
        "f5777a1146db46f2b22ef85a03398bde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41ca854612f54c30aa9a941632d7b62f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dc1b285a36f41c49b8da43a770c0ed1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1c404fc2afb4d20b3e95aa316a8eca6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa37d96711ee4004bf8614ed159b48b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5a9cf94b282b4ba5a958816dbc61c5cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aec60272e09d4c6fb4d36b295e60dbc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "504684364c6145f9aa893cb0df26ec11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c542732168164a50ac6bd22df3ec34c6",
              "IPY_MODEL_13634572aa044d24beafbf1d87206a2d",
              "IPY_MODEL_96c5a7607f634f3eafac827ad458ea5d"
            ],
            "layout": "IPY_MODEL_344323160db94af4ad89cce45c5ebefd"
          }
        },
        "c542732168164a50ac6bd22df3ec34c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_442bed23275b40fe850786302e05bde5",
            "placeholder": "​",
            "style": "IPY_MODEL_4d6643a9a60d42048938c270993e4905",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "13634572aa044d24beafbf1d87206a2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e27cfcc9cba04ca3919047db981a9b99",
            "max": 363,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6c74c1ddf673487e8abb6bb0248b76bf",
            "value": 363
          }
        },
        "96c5a7607f634f3eafac827ad458ea5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_551290b524434203b0257cb66b5060a9",
            "placeholder": "​",
            "style": "IPY_MODEL_4c0b3b26a5cf437eb2a56ba7fd65a1a6",
            "value": " 363/363 [00:00&lt;00:00, 41.1kB/s]"
          }
        },
        "344323160db94af4ad89cce45c5ebefd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "442bed23275b40fe850786302e05bde5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d6643a9a60d42048938c270993e4905": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e27cfcc9cba04ca3919047db981a9b99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c74c1ddf673487e8abb6bb0248b76bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "551290b524434203b0257cb66b5060a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c0b3b26a5cf437eb2a56ba7fd65a1a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24f1443743e9484199a54dea09032891": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db95c33e1abd4493adc99fba6673432c",
              "IPY_MODEL_1a5b0406bbdd43a78522d015b15e026d",
              "IPY_MODEL_9303241f7df64ff1b65531b009cd777f"
            ],
            "layout": "IPY_MODEL_c31d5a48111a41a6a4dd7ee02101b5f5"
          }
        },
        "db95c33e1abd4493adc99fba6673432c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03601afb5e5a4e7d86d7ac507986aefa",
            "placeholder": "​",
            "style": "IPY_MODEL_818bc40ea5a648d59e8c29e9bf11833d",
            "value": "vocab.txt: "
          }
        },
        "1a5b0406bbdd43a78522d015b15e026d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_206fc6854e7747e3aee781153e0d5256",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_089090f1a31f4e5482d9b4e57af7650b",
            "value": 1
          }
        },
        "9303241f7df64ff1b65531b009cd777f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad4a949d14a54312b2150a62a1fc991b",
            "placeholder": "​",
            "style": "IPY_MODEL_43bb407c1d00450fb49750b8832ad06a",
            "value": " 232k/? [00:00&lt;00:00, 7.12MB/s]"
          }
        },
        "c31d5a48111a41a6a4dd7ee02101b5f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03601afb5e5a4e7d86d7ac507986aefa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "818bc40ea5a648d59e8c29e9bf11833d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "206fc6854e7747e3aee781153e0d5256": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "089090f1a31f4e5482d9b4e57af7650b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ad4a949d14a54312b2150a62a1fc991b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43bb407c1d00450fb49750b8832ad06a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d34377ca79274d8db176c148cdd0f62d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2501203b37554f58b3a4feadda8d8b62",
              "IPY_MODEL_432062e7c3ca4f63b49b8315b9204961",
              "IPY_MODEL_0fd25f994c9f467f86dd4b643b06eac9"
            ],
            "layout": "IPY_MODEL_ddcd8799db764121881d9168c60de0aa"
          }
        },
        "2501203b37554f58b3a4feadda8d8b62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c68a1258dcb9454785b53e086474faaa",
            "placeholder": "​",
            "style": "IPY_MODEL_fb159c2fc4a94b79947f48f5bd01e71a",
            "value": "tokenizer.json: "
          }
        },
        "432062e7c3ca4f63b49b8315b9204961": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_725f6ddb7dc24c9bb91c1063d5bf1ef3",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6235fce292334d39b92569c50a186e29",
            "value": 1
          }
        },
        "0fd25f994c9f467f86dd4b643b06eac9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82be5f9f4ddd490e8fee1e268e78a2ad",
            "placeholder": "​",
            "style": "IPY_MODEL_262396c2a29a4c4ea2193c066151de3a",
            "value": " 466k/? [00:00&lt;00:00, 10.7MB/s]"
          }
        },
        "ddcd8799db764121881d9168c60de0aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c68a1258dcb9454785b53e086474faaa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb159c2fc4a94b79947f48f5bd01e71a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "725f6ddb7dc24c9bb91c1063d5bf1ef3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "6235fce292334d39b92569c50a186e29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "82be5f9f4ddd490e8fee1e268e78a2ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "262396c2a29a4c4ea2193c066151de3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1cbe061e091741e89ae45e942235d760": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4b4295784d604180a723f1910c7ab27f",
              "IPY_MODEL_ed815e60963c45868b41f37d047a3bc9",
              "IPY_MODEL_fa3395acd996451facf52a4c0157d32d"
            ],
            "layout": "IPY_MODEL_a2f322ed39764d5c9e2cdf2c6f793c70"
          }
        },
        "4b4295784d604180a723f1910c7ab27f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b66851b28d844b8188df23a4f35a06cf",
            "placeholder": "​",
            "style": "IPY_MODEL_33e5bafbbd6b4e9eb8f74ccdbd153be1",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "ed815e60963c45868b41f37d047a3bc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee3d4dd867724b1ab71e1aadb79ca7d7",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a54417489ed748c4b327439490481ec8",
            "value": 239
          }
        },
        "fa3395acd996451facf52a4c0157d32d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94b77da4aafa4b318f4eb7028f7abe70",
            "placeholder": "​",
            "style": "IPY_MODEL_c0b071f4b5784bc8ae0964f653cb0451",
            "value": " 239/239 [00:00&lt;00:00, 25.7kB/s]"
          }
        },
        "a2f322ed39764d5c9e2cdf2c6f793c70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b66851b28d844b8188df23a4f35a06cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33e5bafbbd6b4e9eb8f74ccdbd153be1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee3d4dd867724b1ab71e1aadb79ca7d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a54417489ed748c4b327439490481ec8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "94b77da4aafa4b318f4eb7028f7abe70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0b071f4b5784bc8ae0964f653cb0451": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32e0642d300b463d9303970f517d5d82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a18bce19fabb492b99eeee1371093a58",
              "IPY_MODEL_3eb33a704fb34c4e84790531f77248cd",
              "IPY_MODEL_a2d4c80adbe84628b74cac0041cd9674"
            ],
            "layout": "IPY_MODEL_b01a4967da614e4ebc40c2633285fe8d"
          }
        },
        "a18bce19fabb492b99eeee1371093a58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_487905d1145249a7beb010788adc8c4c",
            "placeholder": "​",
            "style": "IPY_MODEL_39bcd036734f4555bc26e5af457fe7c2",
            "value": "config.json: 100%"
          }
        },
        "3eb33a704fb34c4e84790531f77248cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b79ef8fbac4946f0b458152bd0e7c4a8",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b6a150ae9ab743ce90a62aa6e5acd0de",
            "value": 190
          }
        },
        "a2d4c80adbe84628b74cac0041cd9674": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e43289edaa94b7db071cb036826d259",
            "placeholder": "​",
            "style": "IPY_MODEL_69aef2231f124d3c984150d9c253abae",
            "value": " 190/190 [00:00&lt;00:00, 21.4kB/s]"
          }
        },
        "b01a4967da614e4ebc40c2633285fe8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "487905d1145249a7beb010788adc8c4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39bcd036734f4555bc26e5af457fe7c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b79ef8fbac4946f0b458152bd0e7c4a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6a150ae9ab743ce90a62aa6e5acd0de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5e43289edaa94b7db071cb036826d259": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69aef2231f124d3c984150d9c253abae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAM57AfbYD-2",
        "outputId": "05d61c9e-133b-4863-d2ae-460c7192999d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: weaviate-client in /usr/local/lib/python3.12/dist-packages (4.17.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.1.1)\n",
            "Requirement already satisfied: rapidocr-onnxruntime in /usr/local/lib/python3.12/dist-packages (1.4.4)\n",
            "Requirement already satisfied: httpx<0.29.0,>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from weaviate-client) (0.28.1)\n",
            "Requirement already satisfied: validators<1.0.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from weaviate-client) (0.35.0)\n",
            "Requirement already satisfied: authlib<2.0.0,>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from weaviate-client) (1.6.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from weaviate-client) (2.11.9)\n",
            "Requirement already satisfied: grpcio<1.80.0,>=1.59.5 in /usr/local/lib/python3.12/dist-packages (from weaviate-client) (1.75.1)\n",
            "Requirement already satisfied: protobuf<7.0.0,>=4.21.6 in /usr/local/lib/python3.12/dist-packages (from weaviate-client) (5.29.5)\n",
            "Requirement already satisfied: deprecation<3.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from weaviate-client) (2.1.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.5)\n",
            "Requirement already satisfied: pyclipper>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from rapidocr-onnxruntime) (1.3.0.post6)\n",
            "Requirement already satisfied: opencv-python>=4.5.1.48 in /usr/local/lib/python3.12/dist-packages (from rapidocr-onnxruntime) (4.12.0.88)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from rapidocr-onnxruntime) (2.0.2)\n",
            "Requirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.12/dist-packages (from rapidocr-onnxruntime) (1.17.0)\n",
            "Requirement already satisfied: Shapely!=2.0.4,>=1.7.1 in /usr/local/lib/python3.12/dist-packages (from rapidocr-onnxruntime) (2.1.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from rapidocr-onnxruntime) (6.0.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from rapidocr-onnxruntime) (11.3.0)\n",
            "Requirement already satisfied: onnxruntime>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from rapidocr-onnxruntime) (1.23.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from rapidocr-onnxruntime) (4.67.1)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.12/dist-packages (from authlib<2.0.0,>=1.2.1->weaviate-client) (43.0.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from deprecation<3.0.0,>=2.1.0->weaviate-client) (25.0)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio<1.80.0,>=1.59.5->weaviate-client) (4.15.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<0.29.0,>=0.26.0->weaviate-client) (0.16.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (25.9.23)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (1.13.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.8.0->weaviate-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.8.0->weaviate-client) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.8.0->weaviate-client) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<0.29.0,>=0.26.0->weaviate-client) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.7.0->rapidocr-onnxruntime) (10.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography->authlib<2.0.0,>=1.2.1->weaviate-client) (2.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.7.0->rapidocr-onnxruntime) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography->authlib<2.0.0,>=1.2.1->weaviate-client) (2.23)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.3.31)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=0.3.78 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.78)\n",
            "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.43)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.11.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.31)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (2.11.9)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.78->langchain-community) (1.33)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.78->langchain-community) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.78->langchain-community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=0.3.78->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install weaviate-client tiktoken pypdf rapidocr-onnxruntime\n",
        "!pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "WEAVIATE_CLUSTER = userdata.get('WEAVIATE_CLUSTER')"
      ],
      "metadata": {
        "id": "QjEoqlNfpyW9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "WEAVIATE_API_KEY= userdata.get('WEAVIATE_API_KEY')"
      ],
      "metadata": {
        "id": "1hDpDKf8pzOQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Weaviate\n",
        "import weaviate\n",
        "from weaviate.auth import AuthApiKey\n",
        "\n",
        "WEAVIATE_URL = WEAVIATE_CLUSTER\n",
        "WEAVIATE_API_KEY = WEAVIATE_API_KEY\n",
        "\n",
        "client = weaviate.connect_to_weaviate_cloud(\n",
        "    cluster_url = WEAVIATE_URL,auth_credentials=AuthApiKey(WEAVIATE_API_KEY)\n",
        ")"
      ],
      "metadata": {
        "id": "hMrM_IaLnFvK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings"
      ],
      "metadata": {
        "id": "_wh93MmitThV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMGoKe9Uz7TD",
        "outputId": "b7123c04-13af-4a3f-bc4a-2878f882b9ce"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.56.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.35.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "embedding_model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
        "#model_kwargs = {\"device\": \"cuda\"}\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=embedding_model_name,\n",
        "    #model_kwargs=model_kwargs\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424,
          "referenced_widgets": [
            "7dbae5802c42476c99ee4041624673b5",
            "e4ac004e7cda4d178ada6d4a9d1d3785",
            "ec90cde5df12411a85a78a8fe9637d95",
            "d9d4cba9283945f8b68cd483e423f338",
            "476f65e9b3644e359f139c81ed76566a",
            "c913f8c60a944d5a8ab5694f42d3d08a",
            "13160718a40b48a68c93d2c78a7133c1",
            "9eda7b2d037a4d35a1a0d9399aba22b1",
            "47840cb7ed9a4740aabeb45afa925100",
            "438d3f0727754aacaf3eb21d506e64b7",
            "9e5b4d91e5574bc1a54e8236cd2012de",
            "91d8dcae20ef4ff78ac104755cd8a023",
            "4752ece09fa04c879abf58d43bd13586",
            "f7bdb35c5086421cb7f8282dfd772802",
            "fcae5bd2d58a44f28a104d16b6750366",
            "92362cc001a4427c9317d2fc0ab4483a",
            "e8be92ef60924ab199d18218006be3f1",
            "d3b858f3f1774556ab9ea70016a3bae0",
            "76093b1827224034805e3a77a1b1fc80",
            "efc1246f852245b0973fc71ef11a6a82",
            "88ecf0be3f764e5492ca556c9649b038",
            "9eec71b51326410da37e5c2edfcb7e08",
            "81b97280c4e5462a95ac1f8c3084e5ad",
            "ced55ca2bfab4df08eecd3e66b026ffc",
            "2c691816970d40518eb9389ec1d3a29f",
            "1afe2bdf1dc24909b986d3045ea711a2",
            "dc342293a3134ceda5f2e65b2140d52a",
            "20b28a9816be4b1286823e9bae6bedb1",
            "bd2852a757384a489ee29250668fc41f",
            "083464f5c2ec48b5ac77d57b0753e5de",
            "fde9adc74d744ae3bd59074068729ed8",
            "9cbc4eca3cf74d5097b1efa5611b8913",
            "897e6650cfa3493b90d3dbe6d75bbe4b",
            "e85969b5931c463db94cb1a829943481",
            "2c415d1c611345d7961fa8f88308d49a",
            "f657e827a47f49e6bac47b4ba7ab391e",
            "dc7e1c2f37dc4b0eb9df9ec88fcac088",
            "5d4ec26e528c491e9eda03f11c9b1917",
            "629bbaa76d3c416290be9b26f632171e",
            "caa1cccd4a484b83927c786b03058862",
            "21d1e52587b2456a906dbfca579f0fb9",
            "e0860e85978e41d58de1bb018ea07f0e",
            "63314609fb7b4fd98a7c92648df7532c",
            "c887f68b136d4ed290e0d1963c4a48fe",
            "3a5c8305e99a4fa982f8678b2ace40ef",
            "0236288dfbd04f9cbd4a2d3321028017",
            "c0dadbeb992d439d85d50a07bb7a4956",
            "1ae4fba191df4ec4b11e6a526a0cc8aa",
            "66cc72f5c859409cb6842824e7053a69",
            "b3f1e709c3d04c9a94695508c70df172",
            "3b4ff5e7b9b14495a1a81a6bde26b792",
            "1b02235df6cf4ea79e4c718bf19fe718",
            "6b07ac377d194960871ff3f61cd000a3",
            "6ed573e8db994ff383dd2d43feea3afe",
            "8a5f4280f6e741c6b346d0333234979e",
            "060a441463e4443db463cbe52e8422ea",
            "ac9a5dc72aa34317a79941a2cacab034",
            "87fd981dd4c643c387fcf6524d19b10f",
            "dd13503524424bd187040ccc4e971983",
            "f5777a1146db46f2b22ef85a03398bde",
            "41ca854612f54c30aa9a941632d7b62f",
            "6dc1b285a36f41c49b8da43a770c0ed1",
            "a1c404fc2afb4d20b3e95aa316a8eca6",
            "aa37d96711ee4004bf8614ed159b48b7",
            "5a9cf94b282b4ba5a958816dbc61c5cb",
            "aec60272e09d4c6fb4d36b295e60dbc6",
            "504684364c6145f9aa893cb0df26ec11",
            "c542732168164a50ac6bd22df3ec34c6",
            "13634572aa044d24beafbf1d87206a2d",
            "96c5a7607f634f3eafac827ad458ea5d",
            "344323160db94af4ad89cce45c5ebefd",
            "442bed23275b40fe850786302e05bde5",
            "4d6643a9a60d42048938c270993e4905",
            "e27cfcc9cba04ca3919047db981a9b99",
            "6c74c1ddf673487e8abb6bb0248b76bf",
            "551290b524434203b0257cb66b5060a9",
            "4c0b3b26a5cf437eb2a56ba7fd65a1a6",
            "24f1443743e9484199a54dea09032891",
            "db95c33e1abd4493adc99fba6673432c",
            "1a5b0406bbdd43a78522d015b15e026d",
            "9303241f7df64ff1b65531b009cd777f",
            "c31d5a48111a41a6a4dd7ee02101b5f5",
            "03601afb5e5a4e7d86d7ac507986aefa",
            "818bc40ea5a648d59e8c29e9bf11833d",
            "206fc6854e7747e3aee781153e0d5256",
            "089090f1a31f4e5482d9b4e57af7650b",
            "ad4a949d14a54312b2150a62a1fc991b",
            "43bb407c1d00450fb49750b8832ad06a",
            "d34377ca79274d8db176c148cdd0f62d",
            "2501203b37554f58b3a4feadda8d8b62",
            "432062e7c3ca4f63b49b8315b9204961",
            "0fd25f994c9f467f86dd4b643b06eac9",
            "ddcd8799db764121881d9168c60de0aa",
            "c68a1258dcb9454785b53e086474faaa",
            "fb159c2fc4a94b79947f48f5bd01e71a",
            "725f6ddb7dc24c9bb91c1063d5bf1ef3",
            "6235fce292334d39b92569c50a186e29",
            "82be5f9f4ddd490e8fee1e268e78a2ad",
            "262396c2a29a4c4ea2193c066151de3a",
            "1cbe061e091741e89ae45e942235d760",
            "4b4295784d604180a723f1910c7ab27f",
            "ed815e60963c45868b41f37d047a3bc9",
            "fa3395acd996451facf52a4c0157d32d",
            "a2f322ed39764d5c9e2cdf2c6f793c70",
            "b66851b28d844b8188df23a4f35a06cf",
            "33e5bafbbd6b4e9eb8f74ccdbd153be1",
            "ee3d4dd867724b1ab71e1aadb79ca7d7",
            "a54417489ed748c4b327439490481ec8",
            "94b77da4aafa4b318f4eb7028f7abe70",
            "c0b071f4b5784bc8ae0964f653cb0451",
            "32e0642d300b463d9303970f517d5d82",
            "a18bce19fabb492b99eeee1371093a58",
            "3eb33a704fb34c4e84790531f77248cd",
            "a2d4c80adbe84628b74cac0041cd9674",
            "b01a4967da614e4ebc40c2633285fe8d",
            "487905d1145249a7beb010788adc8c4c",
            "39bcd036734f4555bc26e5af457fe7c2",
            "b79ef8fbac4946f0b458152bd0e7c4a8",
            "b6a150ae9ab743ce90a62aa6e5acd0de",
            "5e43289edaa94b7db071cb036826d259",
            "69aef2231f124d3c984150d9c253abae"
          ]
        },
        "id": "YUNtk2kGqI4-",
        "outputId": "8745025a-27f0-4c50-eb5c-d967dbddd32d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1059231990.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7dbae5802c42476c99ee4041624673b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91d8dcae20ef4ff78ac104755cd8a023"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "81b97280c4e5462a95ac1f8c3084e5ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e85969b5931c463db94cb1a829943481"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a5c8305e99a4fa982f8678b2ace40ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "060a441463e4443db463cbe52e8422ea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "504684364c6145f9aa893cb0df26ec11"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24f1443743e9484199a54dea09032891"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d34377ca79274d8db176c148cdd0f62d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1cbe061e091741e89ae45e942235d760"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "32e0642d300b463d9303970f517d5d82"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Create a folder named 'pdfs'\n",
        "os.makedirs(\"pdfs\", exist_ok=True)\n",
        "print(\"Folder created:\", os.listdir())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BV0q3htt9Vm",
        "outputId": "710415c8-44a7-455f-918c-b4338ee7dd04"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder created: ['.config', 'pdfs', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Move uploaded PDFs into the 'pdfs' folder\n",
        "import shutil\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    shutil.move(filename, \"pdfs/\" + filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "aRc1SXA6xvIL",
        "outputId": "48d7559b-4952-49b7-8619-302d1aaaf770"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1a273205-9091-482c-8218-63f8247bc7ca\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1a273205-9091-482c-8218-63f8247bc7ca\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving RAG-6.pdf to RAG-6.pdf\n",
            "Saving RAG-5.pdf to RAG-5.pdf\n",
            "Saving RAG-4.pdf to RAG-4.pdf\n",
            "Saving RAG-3.pdf to RAG-3.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "\n",
        "# load all PDFs from a folder\n",
        "loader = DirectoryLoader(\n",
        "    path=\"/content/pdfs\",\n",
        "    glob=\"*.pdf\",\n",
        "    loader_cls=PyPDFLoader,\n",
        ")\n",
        "\n",
        "pages = loader.load()"
      ],
      "metadata": {
        "id": "sv0R5iKox8-L"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uADw13r9zVXt",
        "outputId": "132e3bf8-177e-4635-db5f-12c2aa34b184"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='Preprint, Under Review\\nRAFT: Adapting Language Model to Domain Specific RAG\\nTianjun Zhang ∗\\nDepartment of Computer Science\\nUC Berkeley\\nBerkeley, CA 94720, USA\\n{tianjunz}@berkeley.edu\\nShishir G. Patil, Naman Jain, Sheng Shen\\nDepartment of Computer Science\\nUC Berkeley\\nBerkeley, CA 94720, USA\\n{shishirpatil,naman_jain,sheng.s}@berkeley.edu\\nMatei Zaharia, Ion Stoica, Joseph E. Gonzalez\\nDepartment of Computer Science\\nUC Berkeley\\nBerkeley, CA 94720, USA\\n{matei,istoica,jegonzal}@berkeley.edu\\nAbstract\\nPretraining Large Language Models (LLMs) on large corpora of textual\\ndata is now a standard paradigm. When using these LLMs for many\\ndownstream applications, it is common to additionally incorporate new in-\\nformation into the pretrained model either through RAG-based-prompting,\\nor finetuning. However, the best methodology to incorporate information\\nremains an open question. In this paper, we present Retrieval Augmented\\nFine Tuning (RAFT), a training recipe which improves the model’s ability\\nto answer questions in \"open-book\" in-domain settings. In training RAFT,\\ngiven a question, and a set of retrieved documents, we train the model to\\nignore those documents that don’t help in answering the question, which\\nwe call, distractor documents. RAFT accomplishes this by citing verbatim\\nthe right sequence from the relevant document to help answer the question.\\nThis coupled with RAFT’s chain-of-thought-style response helps improve\\nthe model’s ability to reason. In domain specific RAG, RAFT consistently\\nimproves the model’s performance across PubMed, HotpotQA, and Gorilla\\ndatasets, presenting a post-training recipe to improve pre-trained LLMs to\\nin-domain RAG.\\n1 Introduction\\nTrained on vast quantities of public data, Large Language Models LLMs have achieved\\nsignificant advances in a wide range of general knowledge reasoning tasks Brown et al.\\n(2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized\\ndomains to support tasks ranging from code completion for specific software frameworks\\nto question answering on specific document collections (e.g., legal or medical documents).\\nIn these settings, general knowledge reasoning is less critical and instead the primary goal\\nis to maximize accuracy based on a given set of documents. Indeed, adapting LLMs to the\\nspecialized domains (e.g., recent news, enterprise private documents, or program resources\\nconstructed after the training cutoff) is essential to many emerging applications (Vu et al.,\\n2023; Lazaridou et al., 2022) and is the focus of this work.\\nThis paper studies the following question – How do we adapt pre-trained LLMs for Retrieval\\nAugmented Generation (RAG) in specialized domains?\\nWhen it comes to adapting LLMs to specialized domains, we consider the following two\\ncandidates: in-context learning through Retrieval-Augmented Generation (RAG) and super-\\nvised fine-tuning. RAG based methods allow the LLM to reference the documents when\\n∗Corresponding author, personal website: tianjunz.github.io\\n1\\narXiv:2403.10131v2  [cs.CL]  5 Jun 2024'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='Preprint, Under Review\\n“Open book”\\nquery\\n answer\\n“Closed book”\\nquery\\n answer\\nBake in Knowledge at Train Time Model can use External Docs at Test\\nRAFT (Proposed)\\nquery\\n answer\\nTeach Model to use External Docs at Test\\nFigure 1: How best to prepare for an Exam?(a) Fine-tuning based approaches implement\\n\"studying\" by either directly \"memorizing\" the input documents or answering practice\\nQA without referencing the documents. (b) Alternatively, in-context retrieval methods fail\\nto leverage the learning opportunity afforded by the fixed domain and are equivalent to\\ntaking an open-book exam without studying. In contrast, our approach (c) RAFT leverages\\nfine-tuning with question-answer pairs while referencing the documents in a simulated\\nimperfect retrieval setting — thereby effectively preparing for the open-book exam setting.\\nanswering questions. However, RAG based in-context learning methods fail to leverage\\nthe learning opportunity afforded by the fixed domain setting and early access to the test\\ndocuments. Alternatively, supervised fine-tuning offers the opportunity to learn more\\ngeneral patterns in the documents and better align to end tasks and user preferences Zhou\\net al. (2023). However, existing fine-tuning based approaches either fail to leverage the\\ndocuments at test time (don’t incorporate RAG) or fail to account for the imperfections in\\nretrieval process during training.\\nWe can draw an analogy to an open-book exam. Existing in-context retrieval methods are\\nequivalent to taking an open-book exam without studying. Alternatively, existing fine-\\ntuning based approaches implement “studying\" by either directly “memorizing\" Xiong\\net al. (2023) the input documents or answering practice questions Wang et al. (2022) without\\nreferencing the documents. While these approaches leverage in-domain learning they fail to\\nprepare for the open-book nature of the test setting.\\nIn this paper, we study how to combine instruction fine-tuning (IFT) with retrieval aug-\\nmented generation (RAG). We propose a novel adaptation strategy – Retrieval-Augmented\\nFine Tuning (RAFT). RAFT specifically addresses the challenge of fine-tuning LLMs to both\\nincorporate domain knowledge while also improving in-domain RAG performance. RAFT\\naims to not only enable models to learn domain-specific knowledge through fine-tuning,\\nbut also to ensure robustness against distracting retrieved information. This is achieved\\nby training the models to understand the dynamics between the question (prompt), the\\ndomain-specific documents retrieved, and the right answer. Going back to our analogy to\\nthe open book exam, our approach is analogous to studying for an open-book exam by\\nrecognizing relevant, and irrelevant retrieved documents.\\nIn RAFT, we train the model to answer the question (Q) from Document(s) (D*) to generate\\nanswer (A*), where A* includes chain-of-thought reasoning Wei et al. (2022); Anthropic\\n(2023), and in the presence of distractor documents (Dk). We explain the methodology in\\nSection 3 and analyze the sensitivity to the number of distractor documents (k) at train- and\\ntest- time in Section 5. RAFT consistently outperforms Supervised-finetuning both with-\\nand without- RAG across PubMed Dernoncourt & Lee (2017), HotPot QA Yang et al. (2018),\\nand HuggingFace Hub, Torch Hub, and Tensorflow Hub Gorilla datasets Patil et al. (2023),\\npresenting a novel, yet simple technique to improve pre-trained LLMs for in-domain RAG.\\nOur code is available at https://github.com/ShishirPatil/gorilla.\\n2 LLMs for Open-Book Exam\\nTo understand our goal better, we expand on our analogy between training an LLM with\\nthe real-world setting of prepararing for an exam.\\nClosed-Book Exam A closed book exam often refers to the scenario where the LLMs do\\nnot have access to any additional documents or references to answer the questions during\\n2'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='Preprint, Under Review\\nFigure 2: Overview of our RAFT method. The top-left figure depicts our approach of\\nadapting LLMs to reading solution from a set of positive and distractor documents in\\ncontrast to standard RAG setup where models are trained based on the retriever outputs,\\nwhich is a mixture of both memorization and reading. At test time, all methods follow the\\nstandard RAG setting, provided with a top-k retrieved documents in the context.\\nthe exam. For LLMs, this is equivalent to the scenario, for example, in which the LLM is\\nused as a chatbot. In this scenario the LLM draws from the knowledge baked in during\\npre-training and supervised-finetuning to respond to the users’ prompt.\\nOpen Book Exam In contrast, we liken the open-book exam setting to the scenario in\\nwhich the LLM can refer to external sources of information (e.g., a website or a book chapter).\\nIn such scenarios, typically, the LLM is paired with retriever which retrieves ‘k’ documents\\n(or specific segments of the document) which are appended to the users’ prompt. It is\\nonly through these documents retrieved that the LLM gains access to “domain-specific\\ninformation”. As a result, we argue that the LLM’s performance in these settings, where it\\nis trained as a general-purpose LLM is largely dependent on the quality of the retriever and\\nhow accurately the retriever can identify the most relevant piece of information.\\nDomain-Specific Open-Book Exam In this paper, we focus on the narrower but increas-\\ningly popular domain than the general open book exam, which we call the domain-specific\\nopen-book exam. Here, we know apriori the domain in which the LLM will be tested. The\\nLLM can respond to the users’ prompt using use any and all information from this specific\\ndomain, which it has been fine-tuned on. Examples of domain specific examples include\\nenterprise documents, code repositories belonging to an organization, etc. In all these\\nscenarios, the LLM will be used to respond to the questions, whose answers can be found\\nwithin a collection of documents. The retrieval technique itself has little to no-impact on the\\nmechanism (though it may impact the accuracy). This paper studies the domain-specific\\nopen-book setting and how to adapt a pretrained LLM to this specific domain, including\\nhow to make it more robust to a varying number of retrieved documents and distractors.\\n3 RAFT\\nIn this section, we present RAFT, a novel way of training LLMs for domain-specific open-\\nbook exams. We first introduce the classical technique of supervised fine-tuning, followed\\nwith the key takeaways from our experiments. Then, we introduce RAFT , a modified\\nversion of general instruction tuning. Lastly, we provide an overview of the experiments to\\nexpect in the later sections.\\nSupervised Finetuning\\nConsider the supervised fine-tuning (SFT) setting for a Question-Answer dataset. The\\nformulation consists of the Dataset (D) from which a set of Question (Q) and corresponding\\nanswer (A) pairs are derived or already available. In the classical SFT setting, the model is\\ntrained to improve it’s ability to answer the questions based on it’s knowledge - obtained\\neither during pre-training, or during the SFT training phase. The model so trained can also\\n3'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content='Preprint, Under Review\\nbe used at test-time with Retrieval Augmented Generation (RAG) setting, where additional\\ndocuments can be introduced in the prompt to help the model answer the question. This\\ncan be represented as follows:\\n{Train: Q → A}, {0-shot Inference: Q → A}, {RAG Inference: Q + D → A}\\nRAFT: Retrieval Augmented Fine-Tuning (RAFT), presents a novel recipe to prepare fine-\\ntuning data to tailor the models for domain-specific open-book setting, equivalent to in-\\ndomain RAG In RAFT, we prepare the training data such that each data point contains a\\nquestion (Q), a set of documents (Dk), and a corresponding Chain-of-though style answer\\n(A∗) generated from one of the document ( D∗). We differentiate between two types of\\ndocuments: ‘golden’ documents ( D∗) i.e. the documents from which the answer to the\\nquestion can be deduced, and ‘distractor’ documents ( Di) that do not contain answer-\\nrelevant information. As an implementation detail, the ‘golden’ document doesn’t need to\\nbe a single document, but can be more than one document, as is the case in HotpotQA Yang\\net al. (2018). Then, for P fraction of the questions (qi) in the dataset, we retain the golden\\ndocument (d∗\\ni ) along with distractor documents (dk−1). For (1 − P) fraction of the questions\\n(qi) in the dataset, we include no golden document and only include distractor documents\\n(dk). We then fine-tune the language model using standard supervised training (SFT)\\ntechnique, training it to generate answers from the provided documents and question. Fig. 2\\nillustrates the high-level design principal for RAFT .\\nWe demonstrate that our RAG approach trains the model to perform better RAG on the set\\nof documents it is trained on i.e., in-domain. By removing the golden documents in some\\ninstances, we are compelling the model to memorize answers instead of deriving them from\\nthe context. The training data for RAFT is as follows, and an example training data can be\\nseen in Fig. 3:\\nP % of data: Q + D∗ + D1 + D2 + . . . + Dk → A∗\\n(1 − P) % of data: Q + D1 + D2 + . . . + Dk → A∗\\nSubsequently, for the test scenario, the model is provided with the Q and top-k documents\\nretrieved by the RAG pipeline. Note that RAFT is independent of the retriever used.\\nA key factor in enhancing training quality is the generation of a reasoning process, such\\nas Chain-of-Thought, to explain the provided answers. RAFT approach is similar: we\\ndemonstrate that creating a full reasoning chain and in-addition, clearly citing sources\\nenhances the model’s accuracy in answering questions. In Fig. 3, we illustrate this set-\\nup. Generating the training data in this fashion, involves presenting the model with a\\nquestion, context, and verified answers, and then requesting it to form a reasoning chain\\nthat appropriately references the original context.\\nFor all the datasets in our experiments, we generate the answers using the technique\\ndescribed above. Note that the Gorilla APIBench dataset, already includes reasoning\\nin the answers. We provide an example of the generation step in Fig. 3, the detailed\\nreasoning answer includes a citation from the original context inside ##begin_quote## and\\n##end_quote## as well as the detailed explanation on how to reach the conclusion based on\\nthe citations. We demonstrate that adding detailed reasoning paragraphs can help boost the\\nmodel’s performance in our experiment section.\\n4 Evaluation\\nWe design our experiments to study how well RAFT performs compared to various base-\\nlines. We find that the RAFT-7B model (a finetuned version of LlaMA-2) is better at reading\\nand extracting information from in-domain documents, than domain-specific finetuned\\nmodel, and general-purpose model with RAG. As an ablation, we also demonstrate how\\nimportant it is for the model to learn with Chain-of-Thought responses. In this section,\\nwe will first introduce all the datasets we used in the experiments, then all the baseline\\nmodel/fine-tuning techniques that we benchmark against.\\n4'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5'}, page_content='Preprint, Under Review\\nQuestion: The Oberoi family is part of a hotel company that has a head office\\nin what city?\\ncontext: [The Oberoi family is an Indian family that is famous for its\\ninvolvement in hotels, namely through The Oberoi Group]...[It is located in\\ncity center of Jakarta, near Mega Kuningan, adjacent to the sister JW Marriott\\nHotel. It is operated by The Ritz-Carlton Hotel Company. The complex has two\\ntowers that comprises a hotel and the Airlangga Apartment respectively]...[The\\nOberoi Group is a hotel company with its head office in Delhi.]\\nInstruction: Given the question, context and answer above, provide a logical\\nreasoning for that answer. Please use the format of: ##Reason: {reason}\\n##Answer: {answer}.\\n--------------------------------------------------------------------------------------\\nCoT Answer: ##Reason: The document ##begin_quote## The Oberoi family is an\\nIndian family that is famous for its involvement in hotels, namely through The\\nOberoi Group. ##end_quote## establishes that the Oberoi family is involved in\\nthe Oberoi group, and the document ##begin_quote## The Oberoi Group is a hotel\\ncompany with its head office in Delhi. ##end_quote## establishes the head\\noffice of The Oberoi Group. Therefore, the Oberoi family is part of a hotel\\ncompany whose head office is in Delhi. ##Answer: Delhi\\nFigure 3: RAFT prompt to help LLM evaluate its own generated reasoning and answers,\\ncontrasting them with the correct reasoning and answers. The LLM is prompted to identify\\nerrors in its reasoning and extract key insights for improvement. This figure specifically\\nrepresents the ‘GenerateExplanation‘ step in the RAFT algorithm (Section 3).\\nTable 1: RAFT improves RAG performance for all specialized domains: Across PubMed,\\nHotPot, HuggingFace, Torch Hub, and Tensorflow Hub, we see that Domain-specific Fine-\\ntuning improves significantly of the performance of the base model, RAFT consistently\\noutperforms the existing domain-specific finetuning method with or without RAG. This\\nsuggests the need to train the model with context. We compare our model with LLaMA\\nfinetuning receipes, and provide GPT-3.5 for reference.\\nPubMed HotPot HuggingFace Torch Hub TensorFlow\\nGPT-3.5 + RAG 71.60 41.5 29.08 60.21 65.59\\nLLaMA2-7B 56.5 0.54 0.22 0 0\\nLLaMA2-7B + RAG 58.8 0.03 26.43 08.60 43.06\\nDSF 59.7 6.38 61.06 84.94 86.56\\nDSF + RAG 71.6 4.41 42.59 82.80 60.29\\nRAFT (LLaMA2-7B) 73.30 35.28 74.00 84.95 86.86\\nDatasets In our experiments, we use the following datasets to evaluate our model and\\nall baselines. We selected these datasets to represent both popular and diverse domains\\nincluding Wikipedia, Coding/API documents, and question-answering on medical docu-\\nments. Natural Questions (NQ) Kwiatkowski et al. (2019), Trivia QA Joshi et al. (2017) and\\nHotpotQA Yang et al. (2018) are the open-domain question-answers based on Wikipedia,\\nmainly focused on common knowledge (e.g., movies, sports, etc). HuggingFace, Torch Hub,\\nand TensorFlow Hub are from the APIBench Patil et al. (2023) proposed in the Gorilla paper.\\nThese benchmarks measure how to generate the correct, functional, and executable API\\ncalls based on the documentation. PubMed QA Jin et al. (2019) is a question-answering\\ndataset tailored only for biomedical-research question-answering. It mainly focuses on\\nanswering medical and biology questions based on a given set of documents. We would\\n5'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='Preprint, Under Review\\nlike to highlight that (NQ, Trivia QA, and HotpotQA) are relatively general domain whereas\\nthe latter two domains are on domain-specific documents.\\nBaselines We consider the following baselines for our experiments:\\n• LlaMA2-7B-chat model with 0-shot prompting: this is the commonly used\\ninstruction-finetuned model for QA tasks, where we provide clearly written instruc-\\ntions, but no reference documentation.\\n• LlaMA2-7B-chat model with RAG (Llama2 + RAG): similar to the previous setting,\\nexcept here we include reference documents. This is a popular technique when\\ndealing with domain-specific QA tasks.\\n• Domain-Specific Finetuning with 0-shot prompting (DSF): Standard supervised-\\nfinetuning, without documents in context. We find that its mostly useful to align\\nthe answering style of the model as well as get familiar with the domain context.\\n• Domain-Specific Finetuning with RAG (DSF + RAG): Equip a domain-specific\\nfinetuned-model with external knowledge using RAG. So, for the “knowledge” the\\nmodel does not know, it can still refer to the context.\\n4.1 Results\\nUsing the above datasets and baselines, we evaluate our model RAFT and demonstrate\\nthe effectiveness of RAFT in Tab. 1. We see that RAFT consistently and significantly\\noutperforms the baselines. Compared with the base Llama-2 instruction-tuned model,\\nRAFT with RAG does much better in terms of extracting information as well as being\\nrobust towards distractors. The gain can be as big as 35.25% on Hotpot QA and 76.35% on\\nTorch Hub evaluation. Compared with DSF on the specific dataset, our model does better at\\nrelying on the provided context to solve the problem. RAFT does much better on the tasks\\nlike Hotpot and HuggingFace datasets (30.87% on Hotpot and 31.41% on HuggingFace).\\nNote that for PubMed QA, since it is a binary yes/no question, we don’t observe significant\\ngains when we compare our model with DSF + RAG. Even compared with a much larger\\nand better model GPT-3.5, RAFT demonstrates significant advantages.\\nOverall, the LLaMA-7B model, both with and without the RAG, performs poorly due to its\\nanswering style not aligning with the ground truth. By applying domain-specific tuning,\\nwe significantly enhance its performance. This process enables the model to learn and adopt\\nthe appropriate style of answering. However, introducing RAG to a domain-specifically\\nfine-tuned (DSF) model doesn’t invariably lead to better outcomes. This might indicate that\\nthe model lacks training in context processing and extracting useful information from it. By\\nincorporating our method, RAFT , we train the model not only to match its answering style\\nwith that required but also to improve its document processing capabilities. Consequently,\\nour approach outperforms all others.\\n4.2 Effect of CoT\\nWe also conduct an analysis to evaluate the effectiveness of the Chain-of-Thought approach\\nin enhancing the model’s performance. As indicated in Table 2, simply providing the answer\\nto a question may not always be adequate. This approach can lead to a rapid decrease\\nin loss, resulting in the model beginning to overfit. Incorporating a reasoning chain that\\nnot only guides the model to the answer but also enriches the model’s understanding can\\nimprove the overall accuracy and prevent overfitting to concise answers. In our experiments,\\nintegrating the Chain-of-Thought significantly enhances training robustness. We employ\\nGPT-4-1106 to generate our Chain-of-Thought prompts and include an example of the\\nprompt we used in Figure 3.\\n4.3 Qualitative Analysis\\nTo illustrate the potential advantages of RAFT over the domain-specifically fine-tuned\\n(DSF) approach, we present a comparative example in Figure 4. This example qualitatively\\n6'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7'}, page_content='Preprint, Under Review\\nTable 2: Ablation on Chain-of-Thought : The numbers of RAFT and RAFT without\\nCoT. Results on various datasets show that adding CoT can significantly improve the\\nperformance of the finetuned model. With a gains of 9.66% and 14.93% in the Hotpot QA\\nand HuggingFace datasets respectively.\\nPubMed HotpotQA HuggingFace Torch Hub TensorFlow\\nRAFT w.o CoT 68.30 25.62 59.07 86.56 83.21\\nRAFT 73.30 35.28 74.00 84.95 86.86\\ndemonstrates a scenario where the DSF model becomes confused by a question asking for\\nthe identity of a screenwriter. Instead of providing the correct name, it mistakenly cites one\\nof the films written by the screenwriter. In contrast, the RAFT model accurately answers the\\nquestion. This discrepancy suggests that training a model solely with question-answer pairs\\nmay impair its ability to derive relevant context from provided documents. The comparison\\nunderscores the importance of incorporating both standard instructional tuning and context\\ncomprehension into the training dataset to preserve and enhance the model’s ability to\\nprocess text effectively.\\nHotPot QA\\nQuestion: What screenwriter with credits for “Evolution”[0/1879]e a film starring\\nNicolas Cage and Téa Leoni?\\nDocuments: . . . David Weissman is a screenwriter and director. His film credits\\ninclude “The Family Man” (2000), “Evolution” (2001), and “When in Rome” (2010).\\n. . .\\nThe Family Man is a 2000 American romantic comedy-drama film directed by Brett\\nRatner, written by David Diamond and David Weissman, and starringNicolas Cage\\nand Téa Leoni. . . .\\n—————————————————————————————————————\\n—————————–\\nRAFT: ##Reason: The screenwriter with credits for the film “Evolution,” starring\\nNicolas Cage and Téa Leoni, is David Weissman. This information is provided in the\\nreference documents where it mentions David Weissman as a screenwriter with film\\ncredits including “The Family Man” (2000), “Evolution” (2001), and “When in Rome”\\n(2010). Therefore, the screenwriter for “Evolution” is David Weissman. ##Answer:\\nDavid Weissman ✓\\nDSF: The Family Man ✗\\nFigure 4: Comparison of RAFT and DSF : On the HotPot QA dataset, we can see that DSF\\nmodel extracts the wrong information from the context when the question is asking who is\\nthe screen writer and it answers a film name. RAFT manages to get the accurate results .\\n4.4 Should we train the LLM always with the golden context for RAG?\\nIn our exploration of whether large language models (LLMs) should always be trained with\\nthe golden context for Retrieval-Augmented Generation (RAG), we address a key question:\\nwhat proportion (p%) of the training data should include golden documents? Intuitively,\\none might assume that for effective training in reading and extracting information from\\ncontext (e.g., RAG tasks), the golden document should always be included during training\\n(P = 100%). However, our findings challenge this assumption: incorporating a portion of\\nthe training data without the golden document in the context (P = 80%) appears to enhance\\nthe model’s performance on RAG tasks.\\n7'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='Preprint, Under Review\\n0 20 40 60 80 100\\nP % Golden Retrieved Context at Training\\n0.25\\n0.30\\n0.35\\n0.40\\n0.45Final Accuracy\\nT est Domain: NQ\\n0 20 40 60 80 100\\n% Golden Retrieved Context at Training\\n0.50\\n0.55\\n0.60\\n0.65Final Accuracy\\nT est Domain: TQA\\n0 20 40 60 80 100\\nP % Golden Retrieved Context at Training\\n0.40\\n0.45\\n0.50\\n0.55\\n0.60Final Accuracy\\nT est Domain: HoPo\\nFigure 5: How many golden documents to involve? We study the hyperparameter P%\\nwhere it indicates how much portion of training data is with golden document. Results\\non NQ, TQA and HotpotQA suggest that mixing some amount of data that the golden\\ndocument is not put in the context is helpful for in-domain RAG.\\nFigure 5 presents our investigation into the hyperparameter P%, which represents the\\npercentage of training instances that should include golden documents. We find that the\\noptimal proportion varies across datasets, with P% ranging from 40%, 60%, and 100%. This\\nindicates that training your LLM without the correct corresponding context at times can be\\nbeneficial for the downstream task of answering questions related to the documents. In our\\ntraining setup, we include four distractor documents alongside the golden document, and at\\ntest time, we maintain this format by providing the golden document with four distractors.\\nOur findings suggest that, for domain-specific RAG tasks, including a certain percentage of\\ntraining data without the golden documents in the context proves to be advantageous.\\n5 RAFT Generalizes to Top-K RAG\\nWe now study another important problem: How does the number of distractor documents\\nin RAFT affect the model’s performance when augmented with top-k RAG results during\\nevaluation? Previous research has highlighted the vulnerability of LLMs to irrelevant text\\n(see studies (Shi et al., 2023a; Weston & Sukhbaatar, 2023; Liu et al., 2023)). This issue is\\nparticularly critical for LLMs + RAG since top-k RAG is frequently employed at test time to\\nensure high recall. Such a scenario necessitates the model to have the ability to discern and\\ndisregard irrelevant content, focusing solely on pertinent information.\\n5.1 Making Model Robust to top-K RAG\\nTo tackle the challenge of enhancing large language models’ (LLMs) ability to sift through\\nirrelevant text within the retrieval pipeline, our analysis revealed that training solely with\\ngolden (highly relevant) documents can inadvertently diminish the model’s ability to dis-\\ncern and disregard irrelevant information. To address this, our algorithm, RAFT , adopts\\na strategy that integrates golden documents with a mix of irrelevant ones. This method-\\nology prompts us to investigate the ideal fraction of distractor (irrelevant) documents to\\nincorporate throughout the training process and to assess how well this training approach\\nadapts to different volumes of documents encountered by the Retrieval-Augmented Gen-\\neration (RAG) during the test phase. Our aim is to refine the balance between relevant\\nand irrelevant information to strenghten the model’s efficiency in identifying and utilizing\\npertinent content. Notice that Sec 4.4 looked what what P% of training data should include\\ndistractors, while in this section, we study test-time scenarios.\\nTraining with Distractor Documents To enhance the robustness of LLMs against irrelevant\\ntext in retrieved documents, we adopted a finetuning approach that incorporates both\\ngolden (highly relevant) documents and distractor (irrelevant) documents. The model was\\ntrained with varying numbers of distractor documents, but consistently evaluated using\\nthe top-3 documents obtained from the retriever - not to be confused with p. Our findings,\\ndetailed in Fig. 6, reveal that finetuning with only the golden document frequently results in\\ninferior performance compared to configurations that include a greater number of distractor\\ndocuments. As we can see in the figure, the better performance for Natural Questions is\\n8'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='Preprint, Under Review\\n2 4 6 8 10\\n# T est Documents (T op-k)\\n0.22\\n0.24\\n0.26\\n0.28\\n0.30\\n0.32Final Accuracy\\nNatural Questions\\nTrain D*\\nTrain D* + 1D\\nTrain D* + 2D\\nTrain D* + 3D\\n2 4 6 8 10\\n# T est Documents (T op-k)\\n0.125\\n0.150\\n0.175\\n0.200\\n0.225\\n0.250Final Accuracy\\nHotpot QA\\nTrain D*\\nTrain D* + 1D\\nTrain D* + 2D\\nTrain D* + 3D\\nFigure 6: Test-Time Documents Varying: To analyze how robust RAFT is to varying number\\nof test-time documents, we study three domains – NQ, Trivia QA and HotPot QA. In NQ,\\nwe find that training with 4 documents leads to optimal performance, and this changes to 3\\nand 2 for for Trivia QA and HotPot QA respectively. However, we see that training with\\nonly golden documents leads to poor performance.\\ntraining with D∗ + 3D and it is D∗ + 1D documents with Hotpot QA. This insight has been\\nparticularly beneficial for our algorithm, RAFT . In our experiments, we consistently employ\\na training setup consisting of one golden document alongside four distractor documents.\\nGeneralization to a variable number of test-time documents. We extended our research\\nto examine the impact of different quantities of test-time documents on the model’s per-\\nformance. Specifically, our experiments focused on assessing how models, trained with\\nvarying numbers of distractor documents, respond to changes in the number of documents\\npresented at test time. The results, illustrated in Fig. 6, confirm that the inclusion of distrac-\\ntor documents during training indeed makes the model more resilient to fluctuations in the\\nnumber of documents encountered during testing. This ability to maintain consistent perfor-\\nmance despite variations in test-time document numbers further validates the robustness of\\nour approach, RAFT . This finding underscores the importance of a well-calibrated training\\nenvironment to prepare the model for a range of scenarios it may encounter in real-world.\\n6 Related Works\\nRetrieval-Augmented Language Models Retrieval-Augmented Language Models (RALMs)\\nenhance LLMs by integrating a retrieval module that sources relevant information from\\nexternal knowledge bases, significantly improving performance across various NLP tasks,\\nincluding language modeling (Guu et al., 2020; Borgeaud et al., 2022; Khandelwal et al.,\\n2019; Shi et al., 2023d; Lin et al., 2023b; Shi et al., 2023c; Asai et al., 2023; Xu et al., 2023;\\nWang et al., 2023) and open-domain question answering (Izacard et al., 2023; Lewis et al.,\\n2020). For instance, Atlas (Izacard et al., 2023) fine-tunes T5 models with the retriever,\\ntreating documents as latent variables, while RETRO (Borgeaud et al., 2022) modifies the\\ndecoder-only architecture to include retrieved texts and conducts pre-training from scratch.\\nkNN-LM (Khandelwal et al., 2019) interpolates between the LM’s next token distribution\\nand distributions computed from retrieved tokens at inference. (Shi et al., 2023d; Ram\\net al., 2023) assume black-box access to an LLM, combining it with either off-the-shelf or\\nfine-tuned retriever.\\nMemorization A key question around large neural language models is whether they truly\\n“understand” text (Feldman, 2020; Power et al., 2022) or simply rely on surface pattern\\nmemorization (Carlini et al., 2019; Tänzer et al., 2022). (Feldman, 2020; Carlini et al., 2019;\\n2022) develop methodologies to quantify the extent of memorization in neural models.\\n(Brown et al., 2020; Power et al., 2022; Liu et al., 2022) further explored how memorization\\nimpacts the models’ generalization capabilities. (Carlini et al., 2021; Shi et al., 2023b)\\ndemonstrated the ability of language models to memorize and regurgitate training data,\\nraising significant privacy concerns (Kandpal et al., 2022; Pan et al., 2020).\\nFinetuning for RAG More recently, several papers have been exploring the idea of fine-\\ntuning a pretrained LLM to be better at RAG tasks (Lin et al., 2023a; Wang et al., 2023; Xu\\n9'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content='Preprint, Under Review\\net al., 2023; Liu et al., 2024). These works focus on constructing a combination of finetuning\\ndataset for RAG and train a model to perform well on these tasks. In particular, in their\\nsettings, at test time, the domain or documents can be different than the training time;\\nwhereas our paper studies a slightly opposite scenario where we only care about testing the\\nLLM on the same set of documents.\\n7 Conclusion\\nRAFT is a training strategy designed to enhance the model’s performance in answering\\nquestions within a specific domain, in \"open-book\" settings. We highlight several crucial\\ndesign decisions, such as training the model alongside distractor documents, organizing the\\ndataset so a portion lacks golden documents in their context, and formulating answers in a\\nchain-of-thought manner with direct quotations from the relevant text. Our evaluations on\\nPubMed, HotpotQA, and Gorilla API Bench underline RAFT’s significant potential.\\nReferences\\nAnthropic. Prompt engineering for claude’s long context window. 2023.\\nAsai, A., Wu, Z., Wang, Y., Sil, A., and Hajishirzi, H. Self-rag: Learning to retrieve, generate,\\nand critique through self-reflection. arXiv preprint arXiv:2310.11511, 2023.\\nBorgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., Van Den Driess-\\nche, G. B., Lespiau, J.-B., Damoc, B., Clark, A., et al. Improving language models by\\nretrieving from trillions of tokens. In International conference on machine learning, pp.\\n2206–2240. PMLR, 2022.\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P ., Neelakantan, A.,\\nShyam, P ., Sastry, G., Askell, A., et al. Language models are few-shot learners.Advances\\nin neural information processing systems, 33:1877–1901, 2020.\\nCarlini, N., Liu, C., Erlingsson, Ú., Kos, J., and Song, D. The secret sharer: Evaluating and\\ntesting unintended memorization in neural networks. In 28th USENIX Security Symposium\\n(USENIX Security 19), pp. 267–284, 2019.\\nCarlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., Roberts, A.,\\nBrown, T., Song, D., Erlingsson, U., et al. Extracting training data from large language\\nmodels. In 30th USENIX Security Symposium (USENIX Security 21), pp. 2633–2650, 2021.\\nCarlini, N., Ippolito, D., Jagielski, M., Lee, K., Tramer, F., and Zhang, C. Quantifying\\nmemorization across neural language models. In The Eleventh International Conference on\\nLearning Representations, 2022.\\nDernoncourt, F. and Lee, J. Y. Pubmed 200k rct: a dataset for sequential sentence classification\\nin medical abstracts. arXiv preprint arXiv:1710.06071, 2017.\\nFeldman, V . Does learning require memorization? a short tale about a long tail. InProceedings\\nof the 52nd Annual ACM SIGACT Symposium on Theory of Computing, pp. 954–959, 2020.\\nGuu, K., Lee, K., Tung, Z., Pasupat, P ., and Chang, M. Retrieval augmented language model\\npre-training. In International conference on machine learning, pp. 3929–3938. PMLR, 2020.\\nIzacard, G., Lewis, P ., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J.,\\nJoulin, A., Riedel, S., and Grave, E. Atlas: Few-shot learning with retrieval augmented\\nlanguage models. Journal of Machine Learning Research, 24(251):1–43, 2023. URL http:\\n//jmlr.org/papers/v24/23-0037.html.\\nJin, Q., Dhingra, B., Liu, Z., Cohen, W. W., and Lu, X. Pubmedqa: A dataset for biomedical\\nresearch question answering. arXiv preprint arXiv:1909.06146, 2019.\\n10'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='Preprint, Under Review\\nJoshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551,\\n2017.\\nKandpal, N., Wallace, E., and Raffel, C. Deduplicating training data mitigates privacy risks\\nin language models. In International Conference on Machine Learning, pp. 10697–10707.\\nPMLR, 2022.\\nKhandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., and Lewis, M. General-\\nization through memorization: Nearest neighbor language models. arXiv preprint\\narXiv:1911.00172, 2019.\\nKwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D.,\\nPolosukhin, I., Devlin, J., Lee, K., et al. Natural questions: a benchmark for question\\nanswering research. Transactions of the Association for Computational Linguistics, 7:453–466,\\n2019.\\nLazaridou, A., Gribovskaya, E., Stokowiec, W., and Grigorev, N. Internet-augmented\\nlanguage models through few-shot prompting for open-domain question answering.\\narXiv preprint arXiv:2203.05115, 2022.\\nLewis, P ., Perez, E., Piktus, A., Petroni, F., Karpukhin, V ., Goyal, N., Küttler, H., Lewis, M.,\\nYih, W.-t., Rocktäschel, T., et al. Retrieval-augmented generation for knowledge-intensive\\nnlp tasks. Advances in Neural Information Processing Systems, 33:9459–9474, 2020.\\nLin, X. V ., Chen, X., Chen, M., Shi, W., Lomeli, M., James, R., Rodriguez, P ., Kahn, J., Szilvasy,\\nG., Lewis, M., et al. Ra-dit: Retrieval-augmented dual instruction tuning. arXiv preprint\\narXiv:2310.01352, 2023a.\\nLin, X. V ., Chen, X., Chen, M., Shi, W., Lomeli, M., James, R., Rodriguez, P ., Kahn, J., Szilvasy,\\nG., Lewis, M., et al. Ra-dit: Retrieval-augmented dual instruction tuning. arXiv preprint\\narXiv:2310.01352, 2023b.\\nLiu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P . Lost\\nin the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172,\\n2023.\\nLiu, Z., Kitouni, O., Nolte, N. S., Michaud, E., Tegmark, M., and Williams, M. Towards\\nunderstanding grokking: An effective theory of representation learning. Advances in\\nNeural Information Processing Systems, 35:34651–34663, 2022.\\nLiu, Z., Ping, W., Roy, R., Xu, P ., Shoeybi, M., and Catanzaro, B. Chatqa: Building gpt-4 level\\nconversational qa models. arXiv preprint arXiv:2401.10225, 2024.\\nPan, X., Zhang, M., Ji, S., and Yang, M. Privacy risks of general-purpose language models.\\nIn 2020 IEEE Symposium on Security and Privacy (SP), pp. 1314–1331. IEEE, 2020.\\nPatil, S. G., Zhang, T., Wang, X., and Gonzalez, J. E. Gorilla: Large language model connected\\nwith massive apis. arXiv preprint arXiv:2305.15334, 2023.\\nPower, A., Burda, Y., Edwards, H., Babuschkin, I., and Misra, V . Grokking: Generalization\\nbeyond overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177, 2022.\\nRam, O., Levine, Y., Dalmedigos, I., Muhlgay, D., Shashua, A., Leyton-Brown, K.,\\nand Shoham, Y. In-context retrieval-augmented language models. arXiv preprint\\narXiv:2302.00083, 2023.\\nShi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, E. H., Schärli, N., and Zhou, D. Large\\nlanguage models can be easily distracted by irrelevant context. In International Conference\\non Machine Learning, pp. 31210–31227. PMLR, 2023a.\\nShi, W., Ajith, A., Xia, M., Huang, Y., Liu, D., Blevins, T., Chen, D., and Zettlemoyer, L.\\nDetecting pretraining data from large language models. arXiv preprint arXiv:2310.16789,\\n2023b.\\n11'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12'}, page_content='Preprint, Under Review\\nShi, W., Min, S., Lomeli, M., Zhou, C., Li, M., Lin, V ., Smith, N. A., Zettlemoyer, L., Yih, S.,\\nand Lewis, M. In-context pretraining: Language modeling beyond document boundaries.\\narXiv preprint arXiv:2310.10638, 2023c.\\nShi, W., Min, S., Yasunaga, M., Seo, M., James, R., Lewis, M., Zettlemoyer, L., and Yih, W.-t.\\nReplug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652,\\n2023d.\\nTänzer, M., Ruder, S., and Rei, M. Memorisation versus generalisation in pre-trained lan-\\nguage models. In Proceedings of the 60th Annual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers), pp. 7564–7578, 2022.\\nVu, T., Iyyer, M., Wang, X., Constant, N., Wei, J., Wei, J., Tar, C., Sung, Y.-H., Zhou, D., Le,\\nQ., et al. Freshllms: Refreshing large language models with search engine augmentation.\\narXiv preprint arXiv:2310.03214, 2023.\\nWang, B., Ping, W., McAfee, L., Xu, P ., Li, B., Shoeybi, M., and Catanzaro, B. Instructretro:\\nInstruction tuning post retrieval-augmented pretraining. arXiv preprint arXiv:2310.07713,\\n2023.\\nWang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H.\\nSelf-instruct: Aligning language models with self-generated instructions. arXiv preprint\\narXiv:2212.10560, 2022.\\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V ., Zhou, D., et al.\\nChain-of-thought prompting elicits reasoning in large language models. Advances in\\nNeural Information Processing Systems, 35:24824–24837, 2022.\\nWeston, J. and Sukhbaatar, S. System 2 attention (is something you might need too). arXiv\\npreprint arXiv:2311.11829, 2023.\\nXiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P ., Hou, R., Martin, L., Rungta, R.,\\nSankararaman, K. A., Oguz, B., et al. Effective long-context scaling of foundation models.\\narXiv preprint arXiv:2309.16039, 2023.\\nXu, P ., Ping, W., Wu, X., McAfee, L., Zhu, C., Liu, Z., Subramanian, S., Bakhturina, E.,\\nShoeybi, M., and Catanzaro, B. Retrieval meets long context large language models. arXiv\\npreprint arXiv:2310.03025, 2023.\\nYang, Z., Qi, P ., Zhang, S., Bengio, Y., Cohen, W. W., Salakhutdinov, R., and Manning, C. D.\\nHotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint\\narXiv:1809.09600, 2018.\\nZhou, C., Liu, P ., Xu, P ., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P ., Yu, L., et al. Lima:\\nLess is more for alignment. arXiv preprint arXiv:2305.11206, 2023.\\n12'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1'}, page_content='Retrieval-Augmented Generation for\\nKnowledge-Intensive NLP Tasks\\nPatrick Lewis†‡, Ethan Perez⋆,\\nAleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\\nMike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†\\n†Facebook AI Research; ‡University College London; ⋆New York University;\\nplewis@fb.com\\nAbstract\\nLarge pre-trained language models have been shown to store factual knowledge\\nin their parameters, and achieve state-of-the-art results when ﬁne-tuned on down-\\nstream NLP tasks. However, their ability to access and precisely manipulate knowl-\\nedge is still limited, and hence on knowledge-intensive tasks, their performance\\nlags behind task-speciﬁc architectures. Additionally, providing provenance for their\\ndecisions and updating their world knowledge remain open research problems. Pre-\\ntrained models with a differentiable access mechanism to explicit non-parametric\\nmemory have so far been only investigated for extractive downstream tasks. We\\nexplore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation\\n(RAG) — models which combine pre-trained parametric and non-parametric mem-\\nory for language generation. We introduce RAG models where the parametric\\nmemory is a pre-trained seq2seq model and the non-parametric memory is a dense\\nvector index of Wikipedia, accessed with a pre-trained neural retriever. We com-\\npare two RAG formulations, one which conditions on the same retrieved passages\\nacross the whole generated sequence, and another which can use different passages\\nper token. We ﬁne-tune and evaluate our models on a wide range of knowledge-\\nintensive NLP tasks and set the state of the art on three open domain QA tasks,\\noutperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract\\narchitectures. For language generation tasks, we ﬁnd that RAG models generate\\nmore speciﬁc, diverse and factual language than a state-of-the-art parametric-only\\nseq2seq baseline.\\n1 Introduction\\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowl-\\nedge from data [47]. They can do so without any access to an external memory, as a parameterized\\nimplicit knowledge base [51, 52]. While this development is exciting, such models do have down-\\nsides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into\\ntheir predictions, and may produce “hallucinations” [38]. Hybrid models that combine parametric\\nmemory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these\\nissues because knowledge can be directly revised and expanded, and accessed knowledge can be\\ninspected and interpreted. REALM [ 20] and ORQA [ 31], two recently introduced models that\\ncombine masked language models [8] with a differentiable retriever, have shown promising results,\\narXiv:2005.11401v4  [cs.CL]  12 Apr 2021'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 1, 'page_label': '2'}, page_content='The\\tDivine\\nComedy\\t(x) q \\nQuery \\nEncoder \\nq(x) \\nMIPS p θ \\nGenerator\\xa0pθ\\n(Parametric) \\nMargin- \\nalize \\nThis\\t14th\\tcentury\\twork\\nis\\tdivided\\tinto\\t3\\nsections:\\t\"Inferno\",\\n\"Purgatorio\"\\t&\\n\"Paradiso\"\\t\\t\\t\\t\\t\\t\\t\\t\\t(y)\\nEnd-to-End Backprop through q  and\\xa0p θ \\nBarack\\tObama\\twas\\nborn\\tin\\tHawaii.(x)\\nFact Veriﬁcation: Fact Query\\nsupports\\t(y)\\nQuestion Generation\\nFact Veriﬁcation:\\nLabel Generation\\nDocument \\nIndex \\nDefine\\t\"middle\\tear\"(x)\\nQuestion Answering:\\nQuestion Query\\nThe\\tmiddle\\tear\\tincludes\\nthe\\ttympanic\\tcavity\\tand\\nthe\\tthree\\tossicles.\\t\\t(y)\\nQuestion Answering:\\nAnswer GenerationRetriever pη \\n(Non-Parametric) \\nz 4 \\nz 3 \\nz 2 \\nz 1 \\nd(z) \\nJeopardy Question\\nGeneration:\\nAnswer Query\\nFigure 1: Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document\\nIndex) with a pre-trained seq2seq model (Generator) and ﬁne-tune end-to-end. For query x, we use\\nMaximum Inner Product Search (MIPS) to ﬁnd the top-K documents zi. For ﬁnal prediction y, we\\ntreat zas a latent variable and marginalize over seq2seq predictions given different documents.\\nbut have only explored open-domain extractive question answering. Here, we bring hybrid parametric\\nand non-parametric memory to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models.\\nWe endow pre-trained, parametric-memory generation models with a non-parametric memory through\\na general-purpose ﬁne-tuning approach which we refer to as retrieval-augmented generation (RAG).\\nWe build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the\\nnon-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural\\nretriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The\\nretriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents conditioned on\\nthe input, and the seq2seq model (BART [32]) then conditions on these latent documents together with\\nthe input to generate the output. We marginalize the latent documents with a top-K approximation,\\neither on a per-output basis (assuming the same document is responsible for all tokens) or a per-token\\nbasis (where different documents are responsible for different tokens). Like T5 [51] or BART, RAG\\ncan be ﬁne-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.\\nThere has been extensive previous work proposing architectures to enrich systems with non-parametric\\nmemory which are trained from scratch for speciﬁc tasks, e.g. memory networks [ 64, 55], stack-\\naugmented networks [25] and memory layers [ 30]. In contrast, we explore a setting where both\\nparametric and non-parametric memory components are pre-trained and pre-loaded with extensive\\nknowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is\\npresent without additional training.\\nOur results highlight the beneﬁts of combining parametric and non-parametric memory with genera-\\ntion for knowledge-intensive tasks—tasks that humans could not reasonably be expected to perform\\nwithout access to an external knowledge source. Our RAG models achieve state-of-the-art results\\non open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform\\nrecent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being\\nextractive tasks, we ﬁnd that unconstrained generation outperforms previous extractive approaches.\\nFor knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question\\ngeneration, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and\\ndiverse than a BART baseline. For FEVER [56] fact veriﬁcation, we achieve results within 4.3% of\\nstate-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that\\nthe non-parametric memory can be replaced to update the models’ knowledge as the world changes.1\\n2 Methods\\nWe explore RAG models, which use the input sequencexto retrieve text documents zand use them\\nas additional context when generating the target sequence y. As shown in Figure 1, our models\\nleverage two components: (i) a retriever pη(z|x) with parameters ηthat returns (top-K truncated)\\ndistributions over text passages given a query xand (ii) a generator pθ(yi|x,z,y 1:i−1) parametrized\\n1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-\\ners Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/\\nexamples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/\\n2'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 2, 'page_label': '3'}, page_content='by θthat generates a current token based on a context of the previous i−1 tokens y1:i−1, the original\\ninput xand a retrieved passage z.\\nTo train the retriever and generator end-to-end, we treat the retrieved document as a latent variable.\\nWe propose two models that marginalize over the latent documents in different ways to produce a\\ndistribution over generated text. In one approach, RAG-Sequence, the model uses the same document\\nto predict each target token. The second approach, RAG-Token, can predict each target token based\\non a different document. In the following, we formally introduce both models and then describe the\\npη and pθ components, as well as the training and decoding procedure.\\n2.1 Models\\nRAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate\\nthe complete sequence. Technically, it treats the retrieved document as a single latent variable that\\nis marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the\\ntop K documents are retrieved using the retriever, and the generator produces the output sequence\\nprobability for each document, which are then marginalized,\\npRAG-Sequence(y|x) ≈\\n∑\\nz∈top-k(p(·|x))\\npη(z|x)pθ(y|x,z) =\\n∑\\nz∈top-k(p(·|x))\\npη(z|x)\\nN∏\\ni\\npθ(yi|x,z,y 1:i−1)\\nRAG-Token Model In the RAG-Token model we can draw a different latent document for each\\ntarget token and marginalize accordingly. This allows the generator to choose content from several\\ndocuments when producing an answer. Concretely, the top K documents are retrieved using the\\nretriever, and then the generator produces a distribution for the next output token for each document,\\nbefore marginalizing, and repeating the process with the following output token, Formally, we deﬁne:\\npRAG-Token(y|x) ≈\\nN∏\\ni\\n∑\\nz∈top-k(p(·|x))\\npη(z|x)pθ(yi|x,z,y 1:i−1)\\nFinally, we note that RAG can be used for sequence classiﬁcation tasks by considering the target class\\nas a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.\\n2.2 Retriever: DPR\\nThe retrieval component pη(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:\\npη(z|x) ∝exp\\n(\\nd(z)⊤q(x)\\n)\\nd(z) =BERTd(z), q(x) =BERTq(x)\\nwhere d(z) is a dense representation of a document produced by a BERTBASE document encoder [8],\\nand q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating\\ntop-k(pη(·|x)), the list of kdocuments zwith highest prior probability pη(z|x), is a Maximum Inner\\nProduct Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use\\na pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This\\nretriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and\\nNatural Questions [29]. We refer to the document index as the non-parametric memory.\\n2.3 Generator: BART\\nThe generator component pθ(yi|x,z,y 1:i−1) could be modelled using any encoder-decoder. We use\\nBART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input\\nxwith the retrieved content zwhen generating from BART, we simply concatenate them. BART was\\npre-trained using a denoising objective and a variety of different noising functions. It has obtained\\nstate-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5\\nmodels [32]. We refer to the BART generator parameters θas the parametric memory henceforth.\\n2.4 Training\\nWe jointly train the retriever and generator components without any direct supervision on what\\ndocument should be retrieved. Given a ﬁne-tuning training corpus of input/output pairs (xj,yj), we\\n3'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 3, 'page_label': '4'}, page_content='minimize the negative marginal log-likelihood of each target, ∑\\nj−log p(yj|xj) using stochastic\\ngradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as\\nit requires the document index to be periodically updated as REALM does during pre-training [20].\\nWe do not ﬁnd this step necessary for strong performance, and keep the document encoder (and\\nindex) ﬁxed, only ﬁne-tuning the query encoder BERTq and the BART generator.\\n2.5 Decoding\\nAt test time, RAG-Sequence and RAG-Token require different ways to approximatearg maxyp(y|x).\\nRAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera-\\ntor with transition probability: p′\\nθ(yi|x,y1:i−1) = ∑\\nz∈top-k(p(·|x)) pη(zi|x)pθ(yi|x,zi,y1:i−1) To\\ndecode, we can plug p′\\nθ(yi|x,y1:i−1) into a standard beam decoder.\\nRAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per-\\ntoken likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for\\neach document z, scoring each hypothesis using pθ(yi|x,z,y 1:i−1). This yields a set of hypotheses\\nY, some of which may not have appeared in the beams of all documents. To estimate the probability\\nof an hypothesis y we run an additional forward pass for each document z for which y does not\\nappear in the beam, multiply generator probability with pη(z|x) and then sum the probabilities across\\nbeams for the marginals. We refer to this decoding procedure as “Thorough Decoding.” For longer\\noutput sequences, |Y|can become large, requiring many forward passes. For more efﬁcient decoding,\\nwe can make a further approximation that pθ(y|x,zi) ≈0 where ywas not generated during beam\\nsearch from x,zi. This avoids the need to run additional forward passes once the candidate set Y has\\nbeen generated. We refer to this decoding procedure as “Fast Decoding.”\\n3 Experiments\\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use\\na single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and\\nKarpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint\\n100-word chunks, to make a total of 21M documents. We use the document encoder to compute an\\nembedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical\\nNavigable Small World approximation for fast retrieval [37]. During training, we retrieve the top\\nkdocuments for each query. We consider k∈{5,10}for training and set kfor test time using dev\\ndata. We now discuss experimental details for each task.\\n3.1 Open-domain Question Answering\\nOpen-domain question answering (QA) is an important real-world application and common testbed\\nfor knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x,y)\\nand train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to\\nthe popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved\\ndocuments, relying primarily on non-parametric knowledge. We also compare to “Closed-Book\\nQA” approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead\\nrelying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural\\nQuestions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As\\nCT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG\\nmodel. We use the same train/dev/test splits as prior work [ 31, 26] and report Exact Match (EM)\\nscores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.\\n3.2 Abstractive Question Answering\\nRAG models can go beyond simple extractive QA and answer questions with free-form, abstractive\\ntext generation. To test RAG’s natural language generation (NLG) in a knowledge-intensive setting,\\nwe use the MSMARCO NLG task v2.1 [ 43]. The task consists of questions, ten gold passages\\nretrieved from a search engine for each question, and a full sentence answer annotated from the\\nretrieved passages. We do not use the supplied passages, only the questions and answers, to treat\\n4'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 4, 'page_label': '5'}, page_content='MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be\\nanswered in a way that matches the reference answer without access to the gold passages, such as\\n“What is the weather in V olcano, CA?” so performance will be lower without using gold passages.\\nWe also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here,\\nRAG can rely on parametric knowledge to generate reasonable responses.\\n3.3 Jeopardy Question Generation\\nTo evaluate RAG’s generation abilities in a non-QA setting, we study open-domain question gen-\\neration. Rather than use questions from standard open-domain QA tasks, which typically consist\\nof short, simple questions, we propose the more demanding task of generating Jeopardy questions.\\nJeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity.\\nFor example, “The World Cup” is the answer to the question “In 1986 Mexico scored as the ﬁrst\\ncountry to host this international sports competition twice.” As Jeopardy questions are precise,\\nfactual statements, generating Jeopardy questions conditioned on their answer entities constitutes a\\nchallenging knowledge-intensive generation task.\\nWe use the splits from SearchQA [ 10], with 100K train, 14K dev, and 27K test examples. As\\nthis is a new task, we train a BART model for comparison. Following [67], we evaluate using the\\nSQuAD-tuned Q-BLEU-1 metric [ 42]. Q-BLEU is a variant of BLEU with a higher weight for\\nmatching entities and has higher correlation with human judgment for question generation than\\nstandard metrics. We also perform two human evaluations, one to assess generation factuality, and\\none for speciﬁcity. We deﬁne factuality as whether a statement can be corroborated by trusted external\\nsources, and speciﬁcity as high mutual dependence between the input and output [ 33]. We follow\\nbest practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two\\ngenerated questions, one from BART and one from RAG. They are then asked to pick one of four\\noptions—quuestion A is better, question B is better, both are good, or neither is good.\\n3.4 Fact Veriﬁcation\\nFEVER [ 56] requires classifying whether a natural language claim is supported or refuted by\\nWikipedia, or whether there is not enough information to decide. The task requires retrieving\\nevidence from Wikipedia relating to the claim and then reasoning over this evidence to classify\\nwhether the claim is true, false, or unveriﬁable from Wikipedia alone. FEVER is a retrieval problem\\ncoupled with an challenging entailment reasoning task. It also provides an appropriate testbed for\\nexploring the RAG models’ ability to handle classiﬁcation rather than generation. We map FEVER\\nclass labels (supports, refutes, or not enough info) to single output tokens and directly train with\\nclaim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on\\nretrieved evidence. In many real-world applications, retrieval supervision signals aren’t available, and\\nmodels that do not require such supervision will be applicable to a wider range of tasks. We explore\\ntwo variants: the standard 3-way classiﬁcation task (supports/refutes/not enough info) and the 2-way\\n(supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.\\n4 Results\\n4.1 Open-domain Question Answering\\nTable 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA\\ntasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines\\nthe generation ﬂexibility of the “closed-book” (parametric only) approaches and the performance of\\n\"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results\\nwithout expensive, specialized “salient span masking” pre-training [20]. It is worth noting that RAG’s\\nretriever is initialized using DPR’s retriever, which uses retrieval supervision on Natural Questions\\nand TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based “cross-\\nencoder” to re-rank documents, along with an extractive reader. RAG demonstrates that neither a\\nre-ranker nor extractive reader is necessary for state-of-the-art performance.\\nThere are several advantages to generating answers even when it is possible to extract them. Docu-\\nments with clues about the answer but do not contain the answer verbatim can still contribute towards\\na correct answer being generated, which is not possible with standard extractive approaches, leading\\n5'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 5, 'page_label': '6'}, page_content='Table 1: Open-Domain QA Test Scores. For TQA,\\nleft column uses the standard test set for Open-\\nDomain QA, right column uses the TQA-Wiki\\ntest set. See Appendix D for further details.\\nModel NQ TQA WQ CT\\nClosed\\nBook\\nT5-11B [52] 34.5 - /50.1 37.4 -\\nT5-11B+SSM[52] 36.6 - /60.5 44.7 -\\nOpen\\nBook\\nREALM [20] 40.4 - / - 40.7 46.8\\nDPR [26] 41.5 57.9/ - 41.1 50.6\\nRAG-Token 44.1 55.2/66.1 45.5 50.0\\nRAG-Seq. 44.5 56.8/68.0 45.2 52.2\\nTable 2: Generation and classiﬁcation Test Scores.\\nMS-MARCO SotA is [4], FEVER-3 is [68] and\\nFEVER-2 is [ 57] *Uses gold context/evidence.\\nBest model without gold access underlined.\\nModel Jeopardy MSMARCO FVR3 FVR2\\nB-1 QB-1 R-L B-1 Label Acc.\\nSotA - - 49.8* 49.9* 76.8 92.2 *\\nBART 15.1 19.7 38.2 41.6 64.0 81.1\\nRAG-Tok. 17.3 22.2 40.1 41.5 72.5 89.5RAG-Seq. 14.7 21.4 40.8 44.2\\nto more effective marginalization over documents. Furthermore, RAG can generate correct answers\\neven when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such\\ncases for NQ, where an extractive model would score 0%.\\n4.2 Abstractive Question Answering\\nAs shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu\\npoints and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is\\nimpressive given that (i) those models access gold passages with speciﬁc information required to\\ngenerate the reference answer , (ii) many questions are unanswerable without the gold passages, and\\n(iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers\\nfrom our models. Qualitatively, we ﬁnd that RAG models hallucinate less and generate factually\\ncorrect text more often than BART. Later, we also show that RAG generations are more diverse than\\nBART generations (see §4.5).\\n4.3 Jeopardy Question Generation\\nTable 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation,\\nwith both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452\\npairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual\\nthan RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and\\nBART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on\\nthe task over a state-of-the-art generation model. Evaluators also ﬁnd RAG generations to be more\\nspeciﬁc by a large margin. Table 3 shows typical generations from each model.\\nJeopardy questions often contain two separate pieces of information, and RAG-Token may perform\\nbest because it can generate responses that combine content from several documents. Figure 2 shows\\nan example. When generating “Sun”, the posterior is high for document 2 which mentions “The\\nSun Also Rises”. Similarly, document 1 dominates the posterior when “A Farewell to Arms” is\\ngenerated. Intriguingly, after the ﬁrst token of each book is generated, the document posterior ﬂattens.\\nThis observation suggests that the generator can complete the titles without depending on speciﬁc\\ndocuments. In other words, the model’s parametric knowledge is sufﬁcient to complete the titles. We\\nﬁnd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding\"The\\nSun. BART completes the generation \"The Sun Also Rises\" is a novel by this author of \"The Sun\\nAlso Rises\" indicating the title \"The Sun Also Rises\" is stored in BART’s parameters. Similarly,\\nBART will complete the partial decoding \"The Sun Also Rises\" is a novel by this author of \"A\\nwith \"The Sun Also Rises\" is a novel by this author of \"A Farewell to Arms\". This example shows\\nhow parametric and non-parametric memories work together—the non-parametric component helps\\nto guide the generation, drawing out speciﬁc knowledge stored in the parametric memory.\\n4.4 Fact Veriﬁcation\\nTable 2 shows our results on FEVER. For 3-way classiﬁcation, RAG scores are within 4.3% of\\nstate-of-the-art models, which are complex pipeline systems with domain-speciﬁc architectures and\\nsubstantial engineering, trained using intermediate retrieval supervision, which RAG does not require.\\n6'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 6, 'page_label': '7'}, page_content='Document 1: his works are considered classics of American\\nliterature ... His wartime experiences formed the basis for his novel\\n”A Farewell to Arms”(1929) ...\\nDocument 2: ... artists of the 1920s ”Lost Generation” expatriate\\ncommunity. His debut novel,”The Sun Also Rises”, was published\\nin 1926.\\nBOS\\n”\\nTheSunAlso\\nR ises\\n” is a\\nnovel\\nby this\\nauthor\\nof ” A\\nFarewellto\\nArms\\n”\\nDoc 1\\nDoc 2\\nDoc 3\\nDoc 4\\nDoc 5\\nFigure 2: RAG-Token document posterior p(zi|x,yi,y−i) for each generated token for input “Hem-\\ningway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high\\nwhen generating “A Farewell to Arms\" and for document 2 when generating “The Sun Also Rises\".\\nTable 3: Examples from generation tasks. RAG models generate more speciﬁc and factually accurate\\nresponses. ‘?’ indicates factually incorrect responses, * indicates partially correct responses.\\nTask Input Model Generation\\nMS-\\nMARCO\\ndeﬁne middle\\near\\nBART ?The middle ear is the part of the ear between the middle ear and the nose.\\nRAG-T The middle ear is the portion of the ear internal to the eardrum.\\nRAG-S The middle ear includes the tympanic cavity and the three ossicles.\\nwhat currency\\nneeded in\\nscotland\\nBART The currency needed in Scotland is Pound sterling.\\nRAG-T Pound is the currency needed in Scotland.\\nRAG-S The currency needed in Scotland is the pound sterling.\\nJeopardy\\nQuestion\\nGener\\n-ation\\nWashington\\nBART ?This state has the largest number of counties in the U.S.\\nRAG-T It’s the only U.S. state named for a U.S. president\\nRAG-S It’s the state where you’ll ﬁnd Mount Rainier National Park\\nThe Divine\\nComedy\\nBART *This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio\\nRAG-T Dante’s \"Inferno\" is the ﬁrst part of this epic poem\\nRAG-S This 14th century work is divided into 3 sections: \"Inferno\", \"Purgatorio\" & \"Paradiso\"\\nFor 2-way classiﬁcation, we compare against Thorne and Vlachos [57], who train RoBERTa [35]\\nto classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy\\nwithin 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence.\\nWe also analyze whether documents retrieved by RAG correspond to documents annotated as gold\\nevidence in FEVER. We calculate the overlap in article titles between the topkdocuments retrieved\\nby RAG and gold evidence annotations. We ﬁnd that the top retrieved document is from a gold article\\nin 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.\\n4.5 Additional Results\\nGeneration Diversity Section 4.3 shows that RAG models are more factual and speciﬁc than\\nBART for Jeopardy question generation. Following recent work on diversity-promoting decoding\\n[33, 59, 39], we also investigate generation diversity by calculating the ratio of distinct ngrams to\\ntotal ngrams generated by different models. Table 5 shows that RAG-Sequence’s generations are\\nmore diverse than RAG-Token’s, and both are signiﬁcantly more diverse than BART without needing\\nany diversity-promoting decoding.\\nRetrieval Ablations A key feature of RAG is learning to retrieve relevant information for the task.\\nTo assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever\\nduring training. As shown in Table 6, learned retrieval improves results for all tasks.\\nWe compare RAG’s dense retriever to a word overlap-based BM25 retriever [53]. Here, we replace\\nRAG’s retriever with a ﬁxed BM25 system, and use BM25 retrieval scores as logits when calculating\\np(z|x). Table 6 shows the results. For FEVER, BM25 performs best, perhaps since FEVER claims are\\nheavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval\\nimproves results on all other tasks, especially for Open-Domain QA, where it is crucial.\\nIndex hot-swapping An advantage of non-parametric memory models like RAG is that knowledge\\ncan be easily updated at test time. Parametric-only models like T5 or BART need further training to\\nupdate their behavior as the world changes. To demonstrate, we build an index using the DrQA [5]\\nWikipedia dump from December 2016 and compare outputs from RAG using this index to the newer\\nindex from our main results (December 2018). We prepare a list of 82 world leaders who had changed\\n7'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 7, 'page_label': '8'}, page_content='Table 4: Human assessments for the Jeopardy\\nQuestion Generation Task.\\nFactuality Speciﬁcity\\nBART better 7.1% 16.8%\\nRAG better 42.7% 37.4%\\nBoth good 11.7% 11.8%\\nBoth poor 17.7% 6.9%\\nNo majority 20.8% 20.1%\\nTable 5: Ratio of distinct to total tri-grams for\\ngeneration tasks.\\nMSMARCO Jeopardy QGen\\nGold 89.6% 90.0%\\nBART 70.7% 32.4%\\nRAG-Token 77.8% 46.8%\\nRAG-Seq. 83.5% 53.8%\\nTable 6: Ablations on the dev set. As FEVER is a classiﬁcation task, both RAG models are equivalent.\\nModel NQ TQA WQ CT Jeopardy-QGen MSMarco FVR-3 FVR-2\\nExact Match B-1 QB-1 R-L B-1 Label Accuracy\\nRAG-Token-BM25 29.7 41.5 32.1 33.1 17.5 22.3 55.5 48.4 75.1 91.6RAG-Sequence-BM25 31.8 44.1 36.6 33.8 11.1 19.5 56.5 46.9\\nRAG-Token-Frozen 37.8 50.1 37.1 51.1 16.7 21.7 55.9 49.4 72.9 89.4RAG-Sequence-Frozen 41.2 52.1 41.8 52.6 11.8 19.6 56.7 47.3\\nRAG-Token 43.5 54.8 46.5 51.9 17.9 22.6 56.2 49.4 74.5 90.6RAG-Sequence 44.0 55.8 44.9 53.4 15.3 21.5 57.2 47.5\\nbetween these dates and use a template “Who is {position}?” (e.g. “Who is the President of Peru?”)\\nto query our NQ RAG model with each index. RAG answers 70% correctly using the 2016 index for\\n2016 world leaders and 68% using the 2018 index for 2018 world leaders. Accuracy with mismatched\\nindices is low (12% with the 2018 index and 2016 leaders, 4% with the 2016 index and 2018 leaders).\\nThis shows we can update RAG’s world knowledge by simply replacing its non-parametric memory.\\nEffect of Retrieving more documents Models are trained with either 5 or 10 retrieved latent\\ndocuments, and we do not observe signiﬁcant differences in performance between them. We have the\\nﬂexibility to adjust the number of retrieved documents at test time, which can affect performance and\\nruntime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves\\nOpen-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved\\ndocuments. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for\\nRAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.\\n10 20 30 40 50\\nKR e t r i e v e dD o c s\\n39\\n40\\n41\\n42\\n43\\n44NQ Exact Match RAG-Tok\\nRAG-Seq\\n10 20 30 40 50\\nKR e t r i e v e dD o c s\\n40\\n50\\n60\\n70\\n80NQ Answer Recall @ K\\nRAG-Tok\\nRAG-Seq\\nFixed DPR\\nBM25\\n10 20 30 40 50\\nKR e t r i e v e dD o c s\\n48\\n50\\n52\\n54\\n56Bleu-1 / Rouge-L score\\nRAG-Tok R-L\\nRAG-Tok B-1\\nRAG-Seq R-L\\nRAG-Seq B-1\\nFigure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall perfor-\\nmance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.\\n5 Related Work\\nSingle-Task Retrieval Prior work has shown that retrieval improves performance across a variety of\\nNLP tasks when considered in isolation. Such tasks include open-domain question answering [5, 29],\\nfact checking [ 56], fact completion [ 48], long-form question answering [ 12], Wikipedia article\\ngeneration [36], dialogue [ 41, 65, 9, 13], translation [ 17], and language modeling [ 19, 27]. Our\\nwork uniﬁes previous successes in incorporating retrieval into individual tasks, showing that a single\\nretrieval-based architecture is capable of achieving strong performance across several tasks.\\n8'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 8, 'page_label': '9'}, page_content='General-Purpose Architectures for NLP Prior work on general-purpose architectures for NLP\\ntasks has shown great success without the use of retrieval. A single, pre-trained language model\\nhas been shown to achieve strong performance on various classiﬁcation tasks in the GLUE bench-\\nmarks [60, 61] after ﬁne-tuning [49, 8]. GPT-2 [50] later showed that a single, left-to-right, pre-trained\\nlanguage model could achieve strong performance across both discriminative and generative tasks.\\nFor further improvement, BART [32] and T5 [51, 52] propose a single, pre-trained encoder-decoder\\nmodel that leverages bi-directional attention to achieve stronger performance on discriminative\\nand generative tasks. Our work aims to expand the space of possible tasks with a single, uniﬁed\\narchitecture, by learning a retrieval module to augment pre-trained, generative language models.\\nLearned Retrieval There is signiﬁcant work on learning to retrieve documents in information\\nretrieval, more recently with pre-trained, neural language models [ 44, 26] similar to ours. Some\\nwork optimizes the retrieval module to aid in a speciﬁc, downstream task such as question answering,\\nusing search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our\\nwork. These successes leverage different retrieval-based architectures and optimization techniques to\\nachieve strong performance on a single task, while we show that a single retrieval-based architecture\\ncan be ﬁne-tuned for strong performance on a variety of tasks.\\nMemory-based Architectures Our document index can be seen as a large external memory for\\nneural networks to attend to, analogous to memory networks [64, 55]. Concurrent work [14] learns\\nto retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our\\nwork. Other work improves the ability of dialog models to generate factual text by attending over\\nfact embeddings [15, 13]. A key feature of our memory is that it is comprised of raw text rather\\ndistributed representations, which makes the memory both (i) human-readable, lending a form of\\ninterpretability to our model, and (ii) human-writable, enabling us to dynamically update the model’s\\nmemory by editing the document index. This approach has also been used in knowledge-intensive\\ndialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF\\nrather than end-to-end learnt retrieval [9].\\nRetrieve-and-Edit approaches Our method shares some similarities with retrieve-and-edit style\\napproaches, where a similar training input-output pair is retrieved for a given input, and then edited\\nto provide a ﬁnal output. These approaches have proved successful in a number of domains including\\nMachine Translation [ 18, 22] and Semantic Parsing [21]. Our approach does have several differences,\\nincluding less of emphasis on lightly editing a retrieved item, but on aggregating content from several\\npieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents\\nrather than related training pairs. This said, RAG techniques may work well in these settings, and\\ncould represent promising future work.\\n6 Discussion\\nIn this work, we presented hybrid generation models with access to parametric and non-parametric\\nmemory. We showed that our RAG models obtain state of the art results on open-domain QA. We\\nfound that people prefer RAG’s generation over purely parametric BART, ﬁnding RAG more factual\\nand speciﬁc. We conducted an thorough investigation of the learned retrieval component, validating\\nits effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model\\nwithout requiring any retraining. In future work, it may be fruitful to investigate if the two components\\ncan be jointly pre-trained from scratch, either with a denoising objective similar to BART or some\\nanother objective. Our work opens up new research directions on how parametric and non-parametric\\nmemories interact and how to most effectively combine them, showing promise in being applied to a\\nwide variety of NLP tasks.\\n9'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 9, 'page_label': '10'}, page_content='Broader Impact\\nThis work offers several positive societal beneﬁts over previous work: the fact that it is more\\nstrongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate” less\\nwith generations that are more factual, and offers more control and interpretability. RAG could be\\nemployed in a wide variety of scenarios with direct beneﬁt to society, for example by endowing it\\nwith a medical index and asking it open-domain questions on that topic, or by helping people be more\\neffective at their jobs.\\nWith these advantages also come potential downsides: Wikipedia, or any potential external knowledge\\nsource, will probably never be entirely factual and completely devoid of bias. Since RAG can be\\nemployed as a language model, similar concerns as for GPT-2 [50] are valid here, although arguably\\nto a lesser extent, including that it might be used to generate abuse, faked or misleading content in\\nthe news or on social media; to impersonate others; or to automate the production of spam/phishing\\ncontent [54]. Advanced language models may also lead to the automation of various jobs in the\\ncoming decades [16]. In order to mitigate these risks, AI systems could be employed to ﬁght against\\nmisleading content and automated spam/phishing.\\nAcknowledgments\\nThe authors would like to thank the reviewers for their thoughtful and constructive feedback on this\\npaper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors\\nwould also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP\\nthanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD\\nprogram.\\nReferences\\n[1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan\\nMajumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina\\nStoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine\\nReading COmprehension Dataset. arXiv:1611.09268 [cs], November 2016. URL http:\\n//arxiv.org/abs/1611.09268. arXiv: 1611.09268.\\n[2] Petr Baudiš and Jan Šediv`y. Modeling of the question answering task in the yodaqa system. In\\nInternational Conference of the Cross-Language Evaluation Forum for European Languages,\\npages 222–228. Springer, 2015. URL https://link.springer.com/chapter/10.1007%\\n2F978-3-319-24027-5_20 .\\n[3] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase\\nfrom Question-Answer Pairs. In Proceedings of the 2013 Conference on Empirical Methods\\nin Natural Language Processing, pages 1533–1544, Seattle, Washington, USA, October 2013.\\nAssociation for Computational Linguistics. URL http://www.aclweb.org/anthology/\\nD13-1160.\\n[4] Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencod-\\ning&autoregressive language model for context-conditioned generation. ArXiv, abs/2004.07159,\\n2020. URL https://arxiv.org/abs/2004.07159.\\n[5] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer\\nOpen-Domain Questions. In Proceedings of the 55th Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers), pages 1870–1879, Vancouver, Canada,\\nJuly 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL\\nhttps://www.aclweb.org/anthology/P17-1171.\\n[6] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and\\nJonathan Berant. Coarse-to-ﬁne question answering for long documents. In Proceedings of the\\n55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\\npages 209–220, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi:\\n10.18653/v1/P17-1020. URL https://www.aclweb.org/anthology/P17-1020.\\n10'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 10, 'page_label': '11'}, page_content='[7] Christopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Compre-\\nhension. arXiv:1710.10723 [cs], October 2017. URL http://arxiv.org/abs/1710.10723.\\narXiv: 1710.10723.\\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of\\nDeep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Con-\\nference of the North American Chapter of the Association for Computational Linguistics: Human\\nLanguage Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis,\\nMinnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.\\nURL https://www.aclweb.org/anthology/N19-1423.\\n[9] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wiz-\\nard of wikipedia: Knowledge-powered conversational agents. In International Conference on\\nLearning Representations, 2019. URL https://openreview.net/forum?id=r1l73iRqKm.\\n[10] Matthew Dunn, Levent Sagun, Mike Higgins, V . Ugur Guney, V olkan Cirik, and Kyunghyun\\nCho. SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine.\\narXiv:1704.05179 [cs], April 2017. URL http://arxiv.org/abs/1704.05179. arXiv:\\n1704.05179.\\n[11] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceed-\\nings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:\\nLong Papers), pages 889–898, Melbourne, Australia, July 2018. Association for Computational\\nLinguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anthology/\\nP18-1082.\\n[12] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:\\nLong form question answering. In Proceedings of the 57th Annual Meeting of the Association\\nfor Computational Linguistics, pages 3558–3567, Florence, Italy, July 2019. Association for\\nComputational Linguistics. doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/\\nanthology/P19-1346.\\n[13] Angela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers\\nwith KNN-based composite memory, 2020. URL https://openreview.net/forum?id=\\nH1gx1CNKPH.\\n[14] Thibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski.\\nEntities as experts: Sparse memory access with entity supervision. ArXiv, abs/2004.07202,\\n2020. URL https://arxiv.org/abs/2004.07202.\\n[15] Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen\\ntau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In AAAI\\nConference on Artiﬁcial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/\\nAAAI/AAAI18/paper/view/16710.\\n[16] Katja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, and Owain Evans. When will AI\\nexceed human performance? evidence from AI experts. CoRR, abs/1705.08807, 2017. URL\\nhttp://arxiv.org/abs/1705.08807.\\n[17] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural\\nmachine translation. In AAAI Conference on Artiﬁcial Intelligence , 2018. URL https:\\n//www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282.\\n[18] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural\\nmachine translation. In 32nd AAAI Conference on Artiﬁcial Intelligence, AAAI 2018 , 32nd\\nAAAI Conference on Artiﬁcial Intelligence, AAAI 2018, pages 5133–5140. AAAI press, 2018.\\n32nd AAAI Conference on Artiﬁcial Intelligence, AAAI 2018 ; Conference date: 02-02-2018\\nThrough 07-02-2018.\\n[19] Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by\\nediting prototypes. Transactions of the Association for Computational Linguistics, 6:437–450,\\n2018. doi: 10.1162/tacl_a_00030. URL https://www.aclweb.org/anthology/Q18-1031.\\n11'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 11, 'page_label': '12'}, page_content='[20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM:\\nRetrieval-augmented language model pre-training. ArXiv, abs/2002.08909, 2020. URL https:\\n//arxiv.org/abs/2002.08909.\\n[21] Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. A\\nretrieve-and-edit framework for predicting structured outputs. In S. Bengio,\\nH. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, ed-\\nitors, Advances in Neural Information Processing Systems 31 , pages 10052–\\n10062. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/\\n8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.\\npdf.\\n[22] Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. Simple and effective retrieve-\\nedit-rerank text generation. In Proceedings of the 58th Annual Meeting of the Association for\\nComputational Linguistics, pages 2532–2538, Online, July 2020. Association for Computa-\\ntional Linguistics. doi: 10.18653/v1/2020.acl-main.228. URL https://www.aclweb.org/\\nanthology/2020.acl-main.228.\\n[23] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. arXiv\\npreprint arXiv:1702.08734, 2017. URL https://arxiv.org/abs/1702.08734.\\n[24] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A Large Scale\\nDistantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the\\n55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\\npages 1601–1611, Vancouver, Canada, July 2017. Association for Computational Linguistics.\\ndoi: 10.18653/v1/P17-1147. URL https://www.aclweb.org/anthology/P17-1147.\\n[25] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-\\naugmented recurrent nets. In Proceedings of the 28th International Conference on\\nNeural Information Processing Systems - Volume 1 , NIPS’15, page 190–198, Cam-\\nbridge, MA, USA, 2015. MIT Press. URL https://papers.nips.cc/paper/\\n5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets .\\n[26] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and\\nWen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint\\narXiv:2004.04906, 2020. URL https://arxiv.org/abs/2004.04906.\\n[27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generaliza-\\ntion through memorization: Nearest neighbor language models. In International Conference on\\nLearning Representations, 2020. URL https://openreview.net/forum?id=HklBjCEKvH.\\n[28] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua\\nBengio and Yann LeCun, editors, 3rd International Conference on Learning Representations,\\nICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL\\nhttp://arxiv.org/abs/1412.6980.\\n[29] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh,\\nChris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Ken-\\nton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob\\nUszkoreit, Quoc Le, and Slav Petrov. Natural Questions: a Benchmark for Ques-\\ntion Answering Research. Transactions of the Association of Computational Lin-\\nguistics, 2019. URL https://tomkwiat.users.x20web.corp.google.com/papers/\\nnatural-questions/main-1455-kwiatkowski.pdf .\\n[30] Guillaume Lample, Alexandre Sablayrolles, Marc’ Aurelio Ranzato, Ludovic Denoyer, and\\nHerve Jegou. Large memory layers with product keys. In H. Wallach, H. Larochelle,\\nA. Beygelzimer, F. d’ Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural In-\\nformation Processing Systems 32, pages 8548–8559. Curran Associates, Inc., 2019. URL http:\\n//papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf .\\n[31] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised\\nopen domain question answering. In Proceedings of the 57th Annual Meeting of the Association\\n12'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 12, 'page_label': '13'}, page_content='for Computational Linguistics, pages 6086–6096, Florence, Italy, July 2019. Association for\\nComputational Linguistics. doi: 10.18653/v1/P19-1612. URL https://www.aclweb.org/\\nanthology/P19-1612.\\n[32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,\\nOmer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence\\npre-training for natural language generation, translation, and comprehension. arXiv preprint\\narXiv:1910.13461, 2019. URL https://arxiv.org/abs/1910.13461.\\n[33] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting\\nobjective function for neural conversation models. In Proceedings of the 2016 Conference of the\\nNorth American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies, pages 110–119, San Diego, California, June 2016. Association for Computational\\nLinguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anthology/\\nN16-1014.\\n[34] Margaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation\\nwith optimized questions and multi-turn comparisons. ArXiv, abs/1909.03087, 2019. URL\\nhttps://arxiv.org/abs/1909.03087.\\n[35] Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. Robust neural machine\\ntranslation with joint textual and phonetic embedding. In Proceedings of the 57th Annual\\nMeeting of the Association for Computational Linguistics, pages 3044–3049, Florence, Italy,\\nJuly 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL\\nhttps://www.aclweb.org/anthology/P19-1291.\\n[36] Peter J. Liu*, Mohammad Saleh*, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser,\\nand Noam Shazeer. Generating wikipedia by summarizing long sequences. In International\\nConference on Learning Representations, 2018. URL https://openreview.net/forum?\\nid=Hyg0vbWC-.\\n[37] Yury A. Malkov and D. A. Yashunin. Efﬁcient and robust approximate nearest neighbor search\\nusing hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis and\\nMachine Intelligence, 42:824–836, 2016. URL https://arxiv.org/abs/1603.09320.\\n[38] Gary Marcus. The next decade in ai: four steps towards robust artiﬁcial intelligence. arXiv\\npreprint arXiv:2002.06177, 2020. URL https://arxiv.org/abs/2002.06177.\\n[39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rocktäschel, Vassilis\\nPlachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies affect the\\nveriﬁability of generated text. arXiv preprint arXiv:1911.03587 , 2019. URL https:\\n//arxiv.org/abs/1911.03587.\\n[40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,\\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed\\nprecision training. In ICLR, 2018. URL https://openreview.net/forum?id=r1gs9JgRZ.\\n[41] Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploit-\\ning background knowledge for building conversation systems. In Proceedings of the 2018\\nConference on Empirical Methods in Natural Language Processing, pages 2322–2332, Brus-\\nsels, Belgium, October-November 2018. Association for Computational Linguistics. doi:\\n10.18653/v1/D18-1255. URL https://www.aclweb.org/anthology/D18-1255.\\n[42] Preksha Nema and Mitesh M. Khapra. Towards a better metric for evaluating question generation\\nsystems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language\\nProcessing, pages 3950–3959, Brussels, Belgium, October-November 2018. Association for\\nComputational Linguistics. doi: 10.18653/v1/D18-1429. URL https://www.aclweb.org/\\nanthology/D18-1429.\\n[43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder,\\nand Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In\\nTarek Richard Besold, Antoine Bordes, Artur S. d’Avila Garcez, and Greg Wayne, editors,\\nProceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic\\n13'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 13, 'page_label': '14'}, page_content='approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing\\nSystems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop\\nProceedings. CEUR-WS.org, 2016. URL http://ceur-ws.org/Vol-1773/CoCoNIPS_\\n2016_paper9.pdf.\\n[44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint\\narXiv:1901.04085, 2019. URL https://arxiv.org/abs/1901.04085.\\n[45] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,\\nand Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings\\nof the 2019 Conference of the North American Chapter of the Association for Computational\\nLinguistics (Demonstrations), pages 48–53, Minneapolis, Minnesota, June 2019. Association\\nfor Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://www.aclweb.\\norg/anthology/N19-4009.\\n[46] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun\\nCho. Finding generalizable evidence by learning to convince q&a models. In Proceedings\\nof the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th\\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages\\n2402–2411, Hong Kong, China, November 2019. Association for Computational Linguistics.\\ndoi: 10.18653/v1/D19-1244. URL https://www.aclweb.org/anthology/D19-1244.\\n[47] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,\\nand Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019\\nConference on Empirical Methods in Natural Language Processing and the 9th International\\nJoint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463–2473, Hong\\nKong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/\\nD19-1250. URL https://www.aclweb.org/anthology/D19-1250.\\n[48] Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H.\\nMiller, and Sebastian Riedel. How context affects language models’ factual predictions. In\\nAutomated Knowledge Base Construction, 2020. URL https://openreview.net/forum?\\nid=025X0zPfn.\\n[49] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Im-\\nproving Language Understanding by Generative Pre-Training, 2018. URL\\nhttps://s3-us-west-2.amazonaws.com/openai-assets/research-covers/\\nlanguage-unsupervised/language_understanding_paper.pdf.\\n[50] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\\nSutskever. Language models are unsupervised multitask learners, 2019. URL\\nhttps://d4mucfpksywv.cloudfront.net/better-language-models/language_\\nmodels_are_unsupervised_multitask_learners.pdf.\\n[51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed\\ntext-to-text transformer. arXiv e-prints, 2019. URL https://arxiv.org/abs/1910.10683.\\n[52] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into\\nthe parameters of a language model? arXiv e-prints, 2020. URL https://arxiv.org/abs/\\n2002.08910.\\n[53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and\\nbeyond. Found. Trends Inf. Retr., 3(4):333–389, April 2009. ISSN 1554-0669. doi: 10.1561/\\n1500000019. URL https://doi.org/10.1561/1500000019.\\n[54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-V oss, Jeff Wu, Alec\\nRadford, and Jian-Bing Wang. Release strategies and the social impacts of language models.\\nArXiv, abs/1908.09203, 2019.\\n[55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory net-\\nworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,Advances\\nin Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015.\\nURL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf .\\n14'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 14, 'page_label': '15'}, page_content='[56] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a\\nlarge-scale dataset for fact extraction and VERiﬁcation. In Proceedings of the 2018 Conference\\nof the North American Chapter of the Association for Computational Linguistics: Human\\nLanguage Technologies, Volume 1 (Long Papers), pages 809–819, New Orleans, Louisiana,\\nJune 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL\\nhttps://www.aclweb.org/anthology/N18-1074.\\n[57] James H. Thorne and Andreas Vlachos. Avoiding catastrophic forgetting in mitigating model\\nbiases in sentence-pair classiﬁcation with elastic weight consolidation. ArXiv, abs/2004.14366,\\n2020. URL https://arxiv.org/abs/2004.14366.\\n[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁ ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V . Luxburg,\\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,Advances in Neural\\nInformation Processing Systems 30, pages 5998–6008. Curran Associates, Inc., 2017. URL\\nhttp://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf .\\n[59] Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David\\nCrandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes.\\nAAAI Conference on Artiﬁcial Intelligence, 2018. URL https://www.aaai.org/ocs/index.\\nphp/AAAI/AAAI18/paper/view/17329.\\n[60] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.\\nGLUE: A multi-task benchmark and analysis platform for natural language understanding.\\nIn Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting\\nNeural Networks for NLP, pages 353–355, Brussels, Belgium, November 2018. Association for\\nComputational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/\\nanthology/W18-5446.\\n[61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix\\nHill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for General-\\nPurpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer,\\nF. d\\\\textquotesingle Alché-Buc, E. Fox, and R. Garnett, editors,Advances in Neural Information\\nProcessing Systems 32, pages 3261–3275. Curran Associates, Inc., 2019. URL https://\\narxiv.org/abs/1905.00537.\\n[62] Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang,\\nGerry Tesauro, Bowen Zhou, and Jing Jiang. R3: Reinforced ranker-reader for open-domain\\nquestion answering. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of\\nthe Thirty-Second AAAI Conference on Artiﬁcial Intelligence, (AAAI-18), the 30th innovative\\nApplications of Artiﬁcial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational\\nAdvances in Artiﬁcial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7,\\n2018, pages 5981–5988. AAAI Press, 2018. URL https://www.aaai.org/ocs/index.\\nphp/AAAI/AAAI18/paper/view/16712.\\n[63] Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang,\\nTim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-\\nranking in open-domain question answering. In ICLR, 2018. URL https://openreview.\\nnet/forum?id=rJl3yM-Ab.\\n[64] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio\\nand Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR\\n2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015. URL\\nhttp://arxiv.org/abs/1410.3916.\\n[65] Jason Weston, Emily Dinan, and Alexander Miller. Retrieve and reﬁne: Improved sequence\\ngeneration models for dialogue. In Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd\\nInternational Workshop on Search-Oriented Conversational AI, pages 87–92, Brussels, Belgium,\\nOctober 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5713. URL\\nhttps://www.aclweb.org/anthology/W18-5713.\\n15'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 15, 'page_label': '16'}, page_content='[66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\\nMoi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,\\nPatrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain\\nGugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface’s transformers:\\nState-of-the-art natural language processing. ArXiv, abs/1910.03771, 2019.\\n[67] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi-\\nsupervised question answering. In Proceedings of the 2019 Conference on Empirical Meth-\\nods in Natural Language Processing and the 9th International Joint Conference on Natural\\nLanguage Processing (EMNLP-IJCNLP) , pages 2495–2509, Hong Kong, China, Novem-\\nber 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL\\nhttps://www.aclweb.org/anthology/D19-1253.\\n[68] Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and\\nJian Yin. Reasoning over semantic-level graph for fact checking. ArXiv, abs/1909.03745, 2019.\\nURL https://arxiv.org/abs/1909.03745.\\n16'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 16, 'page_label': '17'}, page_content='Appendices for Retrieval-Augmented Generation for\\nKnowledge-Intensive NLP Tasks\\nA Implementation Details\\nFor Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models.\\nFor RAG-Sequence models, we report test results using 50 retrieved documents, and we use the\\nThorough Decoding approach since answers are generally short. We use greedy decoding for QA as\\nwe did not ﬁnd beam search improved results. For Open-MSMarco and Jeopardy question generation,\\nwe report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence,\\nand we also train a BART-large model as a baseline. We use a beam size of four, and use the Fast\\nDecoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance.\\nB Human Evaluation\\nFigure 4: Annotation interface for human evaluation of factuality. A pop-out for detailed instructions\\nand a worked example appear when clicking \"view tool guide\".\\nFigure 4 shows the user interface for human evaluation. To avoid any biases for screen position,\\nwhich model corresponded to sentence A and sentence B was randomly selected for each example.\\nAnnotators were encouraged to research the topic using the internet, and were given detailed instruc-\\ntions and worked examples in a full instructions tab. We included some gold sentences in order to\\nassess the accuracy of the annotators. Two annotators did not perform well on these examples and\\ntheir annotations were removed from the results.\\nC Training setup Details\\nWe train all RAG models and BART baselines using Fairseq [45].2 We train with mixed precision\\nﬂoating point arithmetic [40], distributing training across 8, 32GB NVIDIA V100 GPUs, though\\ntraining and inference can be run on one GPU. We ﬁnd that doing Maximum Inner Product Search\\nwith FAISS is sufﬁciently fast on CPU, so we store document index vectors on CPU, requiring∼100\\nGB of CPU memory for all of Wikipedia. After submission, We have ported our code to HuggingFace\\nTransformers [66]3, which achieves equivalent performance to the previous version but is a cleaner\\nand easier to use implementation. This version is also open-sourced. We also compress the document\\nindex using FAISS’s compression tools, reducing the CPU memory requirement to 36GB. Scripts to\\nrun experiments with RAG can be found athttps://github.com/huggingface/transformers/\\nblob/master/examples/rag/README.md and an interactive demo of a RAG model can be found\\nat https://huggingface.co/rag/\\n2https://github.com/pytorch/fairseq\\n3https://github.com/huggingface/transformers\\n17'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 17, 'page_label': '18'}, page_content='D Further Details on Open-Domain QA\\nFor open-domain QA, multiple answer annotations are often available for a given question. These\\nanswer annotations are exploited by extractive models during training as typically all the answer\\nannotations are used to ﬁnd matches within documents when preparing training data. For RAG, we\\nalso make use of multiple annotation examples for Natural Questions and WebQuestions by training\\nthe model with each (q,a) pair separately, leading to a small increase in accuracy. For TriviaQA,\\nthere are often many valid answers to a given question, some of which are not suitable training targets,\\nsuch as emoji or spelling variants. For TriviaQA, we ﬁlter out answer candidates if they do not occur\\nin top 1000 documents for the query.\\nCuratedTrec preprocessing The answers for CuratedTrec are given in the form of regular expres-\\nsions, which has been suggested as a reason why it is unsuitable for answer-generation models [20].\\nTo overcome this, we use a pre-processing step where we ﬁrst retrieve the top 1000 documents for\\neach query, and use the answer that most frequently matches the regex pattern as the supervision\\ntarget. If no matches are found, we resort to a simple heuristic: generate all possible permutations for\\neach regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace.\\nTriviaQA Evaluation setups The open-domain QA community customarily uses public develop-\\nment datasets as test datasets, as test data for QA datasets is often restricted and dedicated to reading\\ncompehension purposes. We report our results using the datasets splits used in DPR [26], which are\\nconsistent with common practice in Open-domain QA. For TriviaQA, this test dataset is the public\\nTriviaQA Web Development split. Roberts et al.[52] used the TriviaQA ofﬁcial Wikipedia test set\\ninstead. Févry et al. [14] follow this convention in order to compare with Roberts et al. [52] (See\\nappendix of [14]). We report results on both test sets to enable fair comparison to both approaches.\\nWe ﬁnd that our performance is much higher using the ofﬁcial Wiki test set, rather than the more\\nconventional open-domain test set, which we attribute to the ofﬁcial Wiki test set questions being\\nsimpler to answer from Wikipedia.\\nE Further Details on FEVER\\nFor FEVER classiﬁcation, we follow the practice from [ 32], and ﬁrst re-generate the claim, and\\nthen classify using the representation of the ﬁnal hidden state, before ﬁnally marginalizing across\\ndocuments to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The\\nﬁrst is to classify the claim as either \"Supported\", \"Refuted\" or \"Not Enough Info\", which is the task\\nwe explore in the main paper. FEVER’s other sub-task involves extracting sentences from Wikipedia\\nas evidence supporting the classiﬁcation prediction. As FEVER uses a different Wikipedia dump to\\nus, directly tackling this task is not straightforward. We hope to address this in future work.\\nF Null Document Probabilities\\nWe experimented with adding \"Null document\" mechanism to RAG, similar to REALM [20] in order\\nto model cases where no useful information could be retrieved for a given input. Here, ifkdocuments\\nwere retrieved, we would additionally \"retrieve\" an empty document and predict a logit for the null\\ndocument, before marginalizing over k+ 1predictions. We explored modelling this null document\\nlogit by learning (i) a document embedding for the null document, (ii) a static learnt bias term, or\\n(iii) a neural network to predict the logit. We did not ﬁnd that these improved performance, so in\\nthe interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents\\ncannot always be retrieved, we observe that the model learns to always retrieve a particular set of\\ndocuments for questions that are less likely to beneﬁt from retrieval, suggesting that null document\\nmechanisms may not be necessary for RAG.\\nG Parameters\\nOur RAG models contain the trainable parameters for the BERT-base query and document encoder of\\nDPR, with 110M parameters each (although we do not train the document encoder ourselves) and\\n406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable\\n18'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 18, 'page_label': '19'}, page_content='Table 7: Number of instances in the datasets used. *A hidden subset of this data is used for evaluation\\nTask Train Development Test\\nNatural Questions 79169 8758 3611\\nTriviaQA 78786 8838 11314\\nWebQuestions 3418 362 2033\\nCuratedTrec 635 134 635\\nJeopardy Question Generation 97392 13714 26849\\nMS-MARCO 153726 12468 101093*\\nFEVER-3-way 145450 10000 10000\\nFEVER-2-way 96966 6666 6666\\nparameters. The best performing \"closed-book\" (parametric only) open-domain QA model is T5-11B\\nwith 11 Billion trainable parameters. The T5 model with the closest number of parameters to our\\nmodels is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [52],\\nsubstantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/non-\\nparametric models require far fewer trainable parameters for strong open-domain QA performance.\\nThe non-parametric memory index does not consist of trainable parameters, but does consists of 21M\\n728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit ﬂoating\\npoint precision to manage memory and disk footprints.\\nH Retrieval Collapse\\nIn preliminary experiments, we observed that for some tasks such as story generation [ 11], the\\nretrieval component would “collapse” and learn to retrieve the same documents regardless of the\\ninput. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents,\\nand the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit\\nrequirement for factual knowledge in some tasks, or the longer target sequences, which could result\\nin less informative gradients for the retriever. Perez et al.[46] also found spurious retrieval results\\nwhen optimizing a retrieval component in order to improve performance on downstream tasks.\\nI Number of instances per dataset\\nThe number of training, development and test datapoints in each of our datasets is shown in Table 7.\\n19'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1'}, page_content='MuRAG: Multimodal Retrieval-Augmented Generator\\nfor Open Question Answering over Images and Text\\nWenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, William W. Cohen\\nGoogle Research\\n{wenhuchen,hexiang,patverga,wcohen}@google.com\\nAbstract\\nWhile language Models store a massive\\namount of world knowledge implicitly in their\\nparameters, even very large models often fail\\nto encode information about rare entities and\\nevents, while incurring huge computational\\ncosts. Recently, retrieval-augmented models,\\nsuch as REALM, RAG, and RETRO, have\\nincorporated world knowledge into language\\ngeneration by leveraging an external non-\\nparametric index and have demonstrated im-\\npressive performance with constrained model\\nsizes. However, these methods are restricted\\nto retrieving only textual knowledge, neglect-\\ning the ubiquitous amount of knowledge in\\nother modalities like images – much of which\\ncontains information not covered by any text.\\nTo address this limitation, we propose the\\nﬁrst Multimodal Retrieval-Augmented Trans-\\nformer (MuRAG), which accesses an external\\nnon-parametric multimodal memory to aug-\\nment language generation. MuRAG is pre-\\ntrained with a mixture of large-scale image-\\ntext and text-only corpora using a joint con-\\ntrastive and generative loss. We perform ex-\\nperiments on two different datasets that re-\\nquire retrieving and reasoning over both im-\\nages and text to answer a given query: We-\\nbQA, and MultimodalQA. Our results show\\nthat MuRAG achieves state-of-the-art accu-\\nracy, outperforming existing models by 10-\\n20% absolute on both datasets and under both\\ndistractor and full-wiki settings.\\n1 Introduction\\nPre-trained language models like GPT-3 (Brown\\net al., 2020), PaLM (Chowdhery et al., 2022), etc\\nhave been shown to capture a massive amount\\nof world knowledge implicitly in their parame-\\nters. However, using such large models incurs an\\nextremely high computation cost. As an alterna-\\ntive to a singular monolithic transformer, retrieval-\\naugmented architectures like KNN-LM (Khandel-\\nwal et al., 2019), REALM (Guu et al., 2020),\\nFigure 1: Visual information-seeking queries: These\\nqueries are unanswerable with text-only retrieval and\\nrequire retrieving and reasoning over images.\\nRAG (Lewis et al., 2020), FiD (Izacard and Grave,\\n2021), and RETRO (Borgeaud et al., 2021) have\\nbeen proposed to decouple world knowledge from\\nthe model’s parameters. More speciﬁcally, these\\nmodels are trained to access an external mem-\\nory to enhance the model’s predictions. Such\\nretrieval-augmented architectures have multiple\\nbeneﬁcial properties including: decreased model\\nsize (Borgeaud et al., 2021), better attribution/-\\nexplanation for model predictions (Lewis et al.,\\n2020), and adaptability to new information with-\\nout retraining (Verga et al., 2021). However, pre-\\nvious retrieval-augmented models are limited to\\nmemories that contain only text or structured data\\nand hence cannot make use of the massive amount\\nof multimodal knowledge available on the web—\\nmuch of which contains information only available\\nin non-text modalities.\\nFigure 1, shows several information-seeking\\nqueries that require retrieving and reasoning over\\nvisual knowledge. Here, a user ﬁrst poses a ques-\\ntion such as “What can be found on the White\\nHouse balconies at Christmas”. The system then\\nretrieves relevant items from its memory, for exam-\\narXiv:2210.02928v2  [cs.CL]  20 Oct 2022'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 1, 'page_label': '2'}, page_content='ple, the ﬁrst image of Figure 1 with the caption\\n“White House during Christmas”, which it uses to\\nproduce the answer “wreaths and garlands”. Ex-\\nisting text retrieval-augmented models would strug-\\ngle with such queries because, in many cases, they\\nwould simply not have access to the answer as some\\nknowledge does not exist in text form. That, cou-\\npled with the abundance of multimodal knowledge\\nthat exists, leads to the conclusion that retrieval-\\naugmented models should ultimately be developed\\nto retrieve and reason over multiple modalities.\\nFigure 2: Model Overview: retrieval-and-predict pro-\\ncess of MuRAG on downstream datasets.\\nIn this paper, we are speciﬁcally interested in\\nendowing pre-trained language models with a non-\\nparametric multimodal memory containing images,\\ntext, or image-text pairs. To accomplish this, we\\nﬁrst combine pre-trained T5 (Raffel et al., 2020)\\nand ViT (Dosovitskiy et al., 2020) models to build\\na backbone encoder (Figure 3), which encodes\\nimage-text pairs, image-only, and text-only inputs\\ninto a multimodal representation. MuRAG uses the\\nbackbone encoder to embed items into an external\\nmemory as well as queries to retrieve multimodal\\nknowledge from that memory. These retrievals\\nthen augment a language model to generate more\\nvisually-grounded outputs.\\nWe pre-train MuRAG with a mixture of\\nimage-text and text-only datasets including\\nLAION (Schuhmann et al., 2021), Conceptual-\\nCaption (Sharma et al., 2018), VQA (An-\\ntol et al., 2015) and Probably-Asked-Questions\\n(PAQ) (Lewis et al., 2021). More speciﬁcally, we\\nreformulate these datasets in a retrieve-and-predict\\nformat. Here, the model’s input is an image along\\nwith a text prompt. The model then retrieves from\\na memory containing captions and passages, which\\nit uses to generate a target token sequence. The\\nmodel is trained with both a contrastive and a gen-\\nerative loss; this teaches the model to discriminate\\nrelevant from irrelevant memory entries, and guides\\nthe model to leverage the multimodal knowledge\\ninto generation.\\nUnlike the pre-training stage, during ﬁne-\\ntuning Figure 2 the model’s input is a question,\\nand the memory contains a collection of captioned\\nimages and text snippets. We ﬁne-tune MuRAG\\non the downstream datasets with a contrastive and\\ngenerative loss similar to pre-training. To avoid ex-\\ncessive computation cost, we develop a two-stage\\ntraining pipeline to ﬁrst train with small in-batch\\nmemory, and then with a statically encoded and\\nindexed large global memory.\\nOur experiments show that MuRAG achieves\\nstate-of-the-art performance on two different open-\\nmultimodal-QA datasets, both of which require\\nretrieving images and text from a large corpus to\\nanswer factoid questions: WebQA (Chang et al.,\\n2022) and MultimodalQA (Talmor et al., 2021). On\\nboth datasets, we outperform sophisticated base-\\nlines (Li et al., 2020; Radford et al., 2021; Zhang\\net al., 2021) by 10-20% accuracy under both dis-\\ntractor (from 40+ candidates) and full-wiki settings\\n(from 1M candidates). We also perform a compre-\\nhensive study to ablate different components of the\\npre-training to see their contributions. These em-\\npirical results demonstrate the effectiveness of our\\nproposed models to integrate multimodal knowl-\\nedge into pre-trained generation models and pave\\nthe way to uniﬁed retrieval-augmented frameworks.\\n2 Related Work\\nRetrieval Augmented Models Retrieval aug-\\nmented models are hybrid models containing\\nboth parameterized sequence models and a non-\\nparametric memory, infusing world knowledge into\\nexisting language models. Among them, KNN-\\nLM (Khandelwal et al., 2019) was ﬁrst proposed\\nto retrieve instances from a text training corpus to\\nhelp language modeling. Later, RETRO (Borgeaud\\net al., 2021) was proposed to scale up the text cor-\\npus to trillions of tokens, enabling the model to\\nachieve similar perplexity to GPT-3 (Brown et al.,\\n2020) with 25x fewer model parameters. Another\\nfamily of models, such as REALM (Guu et al.,\\n2020), RAG (Lewis et al., 2020), and FiD (Izacard\\nand Grave, 2021), integrate Wikipedia passages as\\na datastore to beneﬁt downstream knowledge in-\\ntensive tasks (e.g. Question Answering). REALM\\nis an encoder-only model trained with masked lan-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 2, 'page_label': '3'}, page_content='guage modeling, while RAG and FiD adopt an\\nencoder-decoder model with a generative language\\nmodeling objective. Compared to them, MuRAG\\nis the ﬁrst retrieval-augmented model that is ca-\\npable of using knowledge presented in multiple\\nmodalities (i.e. visual and textual knowledge data),\\nwhereas all prior methods are restricted to using\\ntext-only knowledge.\\nMultimodal Transformers Multimodal trans-\\nformers have demonstrated strong performances\\nin learning cross-modal representation that are gen-\\nerally beneﬁcial on downstream vision and lan-\\nguage tasks, such as image-text retrieval (Karpa-\\nthy and Fei-Fei, 2015), image captioning (Chen\\net al., 2015), and VQA (Antol et al., 2015). These\\nmethods typically learn a joint transformer model\\non top of unimodal visual and textual backbones,\\nvia fusing deep features from each modality. The\\nearly version of multimodal transformers (Lu et al.,\\n2019; Chen et al., 2020; Li et al., 2020) usually\\nlearns a Transformer on pre-extracted unimodal\\nfeatures for contextualization, which makes it im-\\npossible to adjust those unimodal features to the\\ntarget tasks. Recently, SimVLM (Wang et al., 2022)\\nand COCA (Yu et al., 2022) proposed end-to-end\\ntraining for both deep multimodal transformers and\\nunimodal featurization networks and demonstrated\\nstrong performance in both multimodal and uni-\\nmodal downstream tasks. The multimodal memory\\nencoder of MuRAG is broadly similar to SimVLM\\nand CoCa, but has a different focus to encode and\\nretrieve multimodal knowledge ( i.e. images and\\ntexts) to augment language generation models.\\nMultimodal Question Answering The problem\\nof multimodal question answering has been ex-\\ntensively studied. VQA was the ﬁrst proposed to\\nanswer questions from visual-only inputs. Later,\\nOK-VQA (Marino et al., 2019) enlarged VQA’s\\nscope to annotate questions requiring both image\\nand implicit textual/common-sense knowledge to\\nanswer. More recently, MuMuQA (Reddy et al.,\\n2021), ManyModelQA (Hannan et al., 2020) and\\nMIMOQA (Singh et al., 2021) provide questions\\nwhich require reasoning over images and explicitly\\nprovided text snippets. However, these datasets\\nare restricted to dealing with given text and images\\nwithout requiring any retrieval from the web: they\\nare analogous to machine-reading approaches to\\nQA from text like SQuAD, rather than open-book\\nQA. To study the more realistic open multimodal\\nQA task, WebQA (Chang et al., 2022) and Multi-\\nmodalQA (Talmor et al., 2021) have been proposed\\nto evaluate answers to open queries which require\\nretrieving and reasoning over a large-scale web\\nmultimodal corpus. Our model uses these datasets\\nto study open-world multimodal question answer-\\ning, obtaining state-of-the-art results.\\n3 Model\\n3.1 Backbone Encoder\\nFigure 3: Backbone encoder: ViT encodes image\\npatches into a sequence of vectors eI, while word em-\\nbedding converts text tokens into another sequence of\\nvectors eT. These vectors are concatenated to form\\nfθ(e) and fed to a decoder for text generation.\\nMuRAG is built on top of a simpler model we\\ncall a “backbone” model, which is pre-trained to\\nencode image-text pairs such that they are suitable\\nfor both answer generation and retrieval. The back-\\nbone model’s encoder is used as a component of\\nthe MuRAG model. The backbone model is built\\nwith a pre-trained visual Transformer (Dosovitskiy\\net al., 2020) and a T5 text Transformer (Raffel et al.,\\n2020), and consists of a multimodal encoderfθ and\\ndecoder gθ. The encoder takes as input a sequence\\nof image-text pairs, where either the image or the\\ntext component can be empty to accommodate text-\\nonly and image-only cases.\\nAs depicted in Figure 3, the encoder can take a\\nsequence of images and text. For image input, we\\nﬁrst split each into 16x16 patches and feed them\\nto a ViT (Dosovitskiy et al., 2020) transformer to\\ngenerate a sequence of visual embedding denoted\\nas eI ∈RLi×D, where Li is the length of the im-\\nage tokens. For text input, we use word embedding\\nto produce another sequence of textual embedding\\neT ∈RLt×D. For k images and n text inputs, we\\nconcatenate all their embeddings in the input or-\\nder as e = [e1\\nI; e1\\nT; ··· ; ek\\nI; en\\nT] ∈R(kLt+nLi)×D,\\nwhich is fed to another bi-directional transformer\\nfθ initialized from T5. We enable cross-attention'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4'}, page_content='between the two modalities to produce a fused rep-\\nresentation, denoted as fθ(e) ∈ R(kLt+nLi)×D.\\nWe add a [CLS] token to obtain a pooled repre-\\nsentation fθ(e)[CLS] ∈RD for dense retrieval.\\n3.2 MuRAG\\nWe build MuRAG (shown in Figure 4) on top of\\nthe backbone model. During the retriever stage,\\nMuRAG takes a query q of any modality as in-\\nput and retrieves from a memory Mof image-text\\npairs. Speciﬁcally, we apply the backbone encoder\\nfθ to encode a query q, and use maximum inner\\nproduct search (MIPS (Guo et al., 2020)) over all of\\nthe memory candidates m ∈M to ﬁnd the Top-K\\nnearest neighbors TopK(M|q) = [ m1, ··· , mk].\\nFormally, we deﬁne TopK(M|q) as follows:\\nTopK(M|q) = TopK\\nm∈M\\nfθ(q)[CLS] ·fθ(m)[CLS]\\nDuring the reader stage, the retrievals (the raw im-\\nage patches) are combined with the query q as\\nan augmented input [m1, ··· , mk, q], which is fed\\nto the backbone encoder fθ to produce retrieval-\\naugmented encoding. The decoder model gθ uses\\nattention over this representation to generate tex-\\ntual outputs y = y1, ··· , yn token by token.\\np(yi|yi−1) = gθ(yi|fθ(TopK(M|q); q); y1:i−1)\\nwhere y is decoded from a given vocabulary V.\\n3.3 Pre-training\\nThe pre-training implementation is depicted in the\\nupper portion of Figure 4, where the input query\\nis an image xI plus a text prompt xp. The exter-\\nnal memory Mcontains textual-only entries mT.\\nThe Top-K retrievalsmT\\n1 , ··· , mT\\nk are leveraged to\\ngenerate the textual output. To avoid the excessive\\ncomputation cost of backpropagation over the mas-\\nsive external memory, we adopt an in-batch mem-\\nory MB, dynamically constructed from the input\\nexamples in a batch. The small in-batch memory\\nenables MuRAG to continuously update the mem-\\nory encoder efﬁciently similar to TOME (de Jong\\net al., 2022) and QAMAT (Chen et al., 2022).\\nDataset The pre-training corpus consists of\\nLAION (Schuhmann et al., 2021), Conceptual-\\nCaption-12M+3M (CC) (Sharma et al., 2018;\\nChangpinyo et al., 2021), VQA (Antol et al., 2015)\\nand PAQ (Lewis et al., 2021) Table 1. LAION is\\na publicly-released image-text dataset containing\\ncrawled image-text pairs ﬁltered by CLIP (Rad-\\nford et al., 2021). We apply rules to ﬁlter LAION\\nfrom 400M to 200M by removing text with HTTP-\\nURLs or image width/height beyond 1000 pixels.\\nCC contains 15M (image, anonymized alt-text)\\npairs crawled from the web but ﬁltered more ex-\\ntensively to maintain high alignment quality. VQA\\ncontains annotated QA pairs aligned to MSCOCO\\nimages. We further add captions to each image\\nfrom MSCOCO-Captioning (Lin et al., 2014) to\\ncreate (Image, Caption, QA) triples. PAQ is a text-\\nonly dataset containing 65M machine-generated\\nQA pairs along with their source Wikipedia pas-\\nsage.\\nDataset #Size Format Source\\nCC 15M (Image, Caption) Crawled\\nLAION 200M (Image, Alt-Text) Crawled\\nPAQ 65M (Passage, QA) Generated\\nVQA 400K (Image, Caption, QA) Annotated\\nTable 1: Pre-training Dataset Statistics\\nFor LAION and CC, we use the input image as\\nxI, and ‘generate caption:’ as the text promptxp.\\nFor VQA, we use the input image as xI and the\\nquestion as the prompt xp. For PAQ, we use an\\nempty array as the input image and the question\\nas the prompt. The in-batch memory MB is con-\\nstructed by stacking the captions associated with\\nthe input images in LAION/CC/VQA and the pas-\\nsages associated with the questions in PAQ. Each\\ntextual memory entry is denoted as mT. The de-\\ncoder is optimized to generate either a caption or\\nan answer, depending on the source dataset. Since\\nthe four dataset sizes are highly unbalanced, we\\nuse ﬁxed mixture sampling ratios to balance their\\npresence during pre-training.\\nWe train the model with a joint loss L = Lgen +\\nLcon as follows:\\nLcon = −log exp(fθ(xI, xp) ·fθ(mT))∑\\nm∈MB\\nexp(fθ(xI, xp) ·fθ(mT))\\nLgen = −log gθ(y|fθ(Mp; xI; xp))\\nMp =\\n{\\nTopK(MB|xI, xp) If (xI, xp) ∈PAQ/VQA\\nØ If (xI, xp) ∈LAION/CC\\nwhere Mp is the retrieved augmentation: if the\\ninput query is from PAQ/VQA, we use the retrieved\\nmemory entries, otherwise, we use null. The reason\\nfor setting it to null for LAION/CC is to avoid a\\ntrivial solution when the generation target (caption)\\nalso exactly appears in the memory.\\nThe contrastive loss Lcon is minimized to dis-\\ncriminate between the positive query-memory pairs'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 4, 'page_label': '5'}, page_content='Figure 4: Model Architecture: the model accesses an external memory to obtain multimodal knowledge contained\\nin images or text snippets, which is used to augment the generation. The upper part deﬁnes the pre-training\\nimplementation, while the lower part deﬁnes ﬁne-tuning implementation.\\nand all other query-memory pairs from the mem-\\nory. The pairwise matching score is computed as\\nthe dot product between query fθ(xI; xp)[CLS] and\\ncandidates fθ(mT)[CLS]. This objective enables\\nthe model to retrieve the most relevant knowledge\\nfrom the memory. The generative loss Lgen is min-\\nimized to generate target tokens y conditioned on\\nthe retrieval-augmented representation. This ob-\\njective enables the model to combine information\\nacross different modalities for text generation.\\n3.4 Fine-tuning\\nWe ﬁnetune MuRAG to align with the expected\\ninputs of the downstream datasets which require an-\\nswering text questions by retrieving image-caption\\npairs or text snippets from the external knowledge\\ndatastore. As depicted in the lower part of Figure 4,\\nthe input query for the downstream task is a text\\nquestion xq, and the memory Mcontaining (im-\\nage, text) pairs (mI, mT).1 The Top-K retrievals\\n{(mI\\n1, mT\\n1 ), ··· , (mI\\nk, mT\\nk)}are leveraged to gen-\\nerate the answer a. To minimize the computation\\ncost, we develop a two-stage pipeline to optimize\\nwith an in-batch memory and then resume with\\nﬁxed retrieval from global memory.\\nIn-Batch Training In this stage, we aim to mini-\\nmize the joint loss functionL = Lcon+Lgen based\\n1We set the image to a zero array if the memory entry is a\\ntext snippet.\\non the in-batch memory MB as follows:\\nLcon = −log exp(fθ(xq) ·fθ(mI; mT))∑\\nm∈MB\\nexp(fθ(xq) ·fθ(mI; mT))\\nLgen = −log gθ(y|fθ(TopK(MB|xq); xq))\\nThe in-batch memory MB is constructed in the\\nfollowing way: the k-th example in the dataset is\\nrepresented as (xq,k, yk, {mI\\ni, mI\\ni}k, {¯mI\\nj, ¯mT\\nj }k),\\nwhere m represents the positive (image, text)\\nsource, and ¯m represents the hard negative\\n(image, text) source provided by the dataset 2.\\nFor a batch with B examples, we assemble\\nall the associated positive and negative knowl-\\nedge source as our in-batch memory MB =\\n{{mI\\ni, mI\\ni}1, {¯mI\\nj, ¯mT\\nj }1, ··· , {¯mI\\nj, ¯mT\\nj }B}.\\nFixed-Retrieval Training After in-batch train-\\ning, we encode all available cross-modal pairs, and\\nindex these encodings for fast MIPS retrieval. We\\nthen apply the trained retriever to search over the\\nfull multimodal corpus Mto obtain the global top-\\nK retrievals TopK(M|xq) and continue to opti-\\nmize Lgen. During this training phase, the stored\\nencodings are not updated. During inference time,\\nwe use ﬁxed encodings to generate the answers.\\n2These hard negatives are mined through Bing Search API\\nand Wikipedia page, refer to (Chang et al., 2022) for details.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 5, 'page_label': '6'}, page_content='4 Experiments\\n4.1 Implementation Details\\nThe backbone model uses T5-base (Raffel et al.,\\n2020) and a ViT-large model (Dosovitskiy et al.,\\n2020) as described in Table 2. We adopt the\\nsentence-piece model from T5 with a vocabulary\\nsize of 32128. The ViT model was pre-trained\\non the JFT dataset. We resize every image into\\n224x224 pixels and split them into a sequence of\\n16x16 patches. The output of ViT is a sequence\\nof 1024-dimension vectors, which are projected\\nto 768-dimension for consistency with T5 model.\\nMuRAG reuses the model as retriever and reader,\\nthus the full model size is 527M parameters.\\nModel #Enc #Dec Hidden Heads Params\\nViT-large 24 0 1024 16 307M\\nT5-base 12 12 768 12 220M\\nTable 2: The model size and conﬁgurations, with\\n#Enc/#Dec denoting encoder/decoder layers.\\nOur model is implemented in JAX (Bradbury\\net al., 2018), based on the T5X codebase (Roberts\\net al., 2022). During pre-training, we ﬁrst train the\\nmodel on LAION for 1M steps, and then continue\\ntraining on CC/PAQ/VQA with 1:1:1 sample ratio\\nfor another 200K steps. We optimize the model\\nwith Adafactor (Shazeer and Stern, 2018). For both\\nstages, we adopt a constant learning rate of 5e-4\\nand a batch size of 4096. The models are trained\\non 64 Cloud v4 TPUs (Jouppi et al., 2020).\\nWe then ﬁne-tune MuRAG on WebQA and Mul-\\ntimodalQA with a constant learning rate of 3e-4\\nfor 20K steps. The checkpoint with the highest\\nvalidation score is run on the test set. We use a\\nbatch size of 64 and set TopK=4 for both in-batch\\ntraining and ﬁxed-retrieval training. We noticed\\nthat increasing Top-K further does not yield further\\nimprovement. We use a beam size of 2 to search\\nfor the best hypothesis for both datasets (increasing\\nit further doesn’t yield better performance).\\n4.2 Datasets\\nFor evaluation, we choose two multimodal QA\\ndatasets: WebQA (Chang et al., 2022) and Mul-\\ntimodalQA (Talmor et al., 2021) and demonstrate\\ntheir statistics in Table 3.\\nWebQA This dataset contains multi-hop, multi-\\nmodal question-answer pairs where all questions\\nare knowledge-seeking queries. The queries re-\\nquire 1-2 images or 1-2 text snippets to answer.\\nDataset Train Dev Test\\nImage/Text Image/Text Image/Text\\nWebQA 18K/17K 2.5K/2.4K 3.4K/4K\\nMultimodalQA 2.1K/7.4K 230/721 -\\nTable 3: Overall Statistics of downstream dataset.\\nEach query in WebQA is associated with a set of\\nvisual/text distractors (hard negatives). The an-\\nswers in WebQA are normally complete sentences\\nto better assess the model’s generation capabil-\\nity. Two evaluation setups are used, namely dis-\\ntractor and full-wiki. Under the distractor setup,\\nthe model needs to retrieve from these hard neg-\\natives + positives to answer the question. Under\\nthe full-wiki setup, the model needs to search over\\n1.1M text and visual sources from Wikipedia to an-\\nswer the question. For evaluation, WebQA uses\\nBARTScore (Yuan et al., 2021) to measure the\\nﬂuency between the generation and the reference,\\nand keyword accuracy score to measure the cor-\\nrectness/truthfulness of the generation. These two\\nscores are multiplied to calculate the overall score.\\nMultimodalQA-Subset This dataset contains\\nhuman-annotated multimodal questions over differ-\\nent modalities including tables, text, and images.\\nWikipedia tables are used as anchors to connect dif-\\nferent modalities. The authors ﬁrst use the template\\nto generate questions and then ask crowd-workers\\nto ﬁlter and paraphrase the generated questions.\\nSince tables are outside the scope of our paper, we\\nfocus on the subset of queries requiring only text\\nand image information. Speciﬁcally, we choose the\\nquestions with types of ‘TextQ’ and ‘ImageQ’ to\\nconstruct the subset. The query requires 1 image\\nor 1 text snippet to answer. Each query in Multi-\\nmodalQA is also associated with visual and text dis-\\ntractors (hard negatives). Similarly, two evaluation\\nsetups are used as before. Under a full-wiki setup,\\nMultimodalQA uses a database containing 500K\\ntext and visual sources. The evaluation scores are\\nbased on Exact Match and F1.\\n4.3 Baselines\\nFor WebQA and MultimodalQA, we mainly\\ncompare different variants of pre-trained vision-\\nlanguage models.\\nVLP In WebQA, VLP-like models (Zhou et al.,\\n2020) like Oscar (Li et al., 2020) and VinvL (Zhang\\net al., 2021) are used as the standard baselines.\\nThese models were pre-trained on Conceptual'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 6, 'page_label': '7'}, page_content='3M (Sharma et al., 2018) with a masked language\\nobjective. During ﬁne-tuning, the VLP model takes\\na set of token inputs <[CLS], si, [SEP], Q, [SEP]>\\nﬁrst to select the most plausible source si, and then\\nfeed si in the form of <[CLS], S, Q, A, [SEP]>\\nto autoregressively decode answer A with masked\\nlanguage model prediction.\\nAutoRouting In MultimodalQA, this method\\nﬁrst applies a question type classiﬁer to detect the\\nmodality of the question (either a passage or an\\nimage), and then routes the question to its sub-\\nmodel. The method uses RoBERTa-large (Roberts\\net al., 2022) for text-questions and VilBERT (Lu\\net al., 2019) with features extracted from Faster-\\nRCNN (Ren et al., 2015) for image questions.\\nCLIP (K) CLIP (Radford et al., 2021) is used for\\nfull-wiki retrieval. Speciﬁcally, the baselines sys-\\ntems adopt CLIP to encode queries and all the im-\\nage/text candidates separately into vectors and then\\nrun approximated nearest neighbor searches to ﬁnd\\na set of K potential candidates. After the coarse-\\nlevel retrieval without cross-attention, it adopts a\\nreranker to further narrow down to the 1-2 candi-\\ndates to feed as input S to the QA model.\\n4.4 Experimental Results\\nWe demonstrate WebQA’s results in Table 4. All\\nresults reported are the medium score from three\\nruns with different random seeds, and the variance\\nof the Overall score is within 0.2%. We can observe\\nthat MuRAG can signiﬁcantly outperform VLP\\nwith different backends including Oscar, ResNet,\\nand VinVL. In retrieval performance, our model\\noutperforms VLP by 15% in the full-wiki setting.\\nFor Fluency, our model outperforms VLP by 12%\\nunder the distractor setting and 14% under the full-\\nwiki setting. For Accuracy, our model manages\\nto achieve 16% under the distractor setting and\\neven 20% the under the full-wiki setting. These\\nimprovements reﬂect the high ﬂuency and accuracy\\nof MuRAG’s generation, and the improvement is\\nmore pronounced for full wiki.\\nWe show the MultimodalQA results in Table 5.\\nWe can see that MuRAG is also able to vastly\\noutperform the routing-based multimodality QA\\nmodel. For text questions, our model improves\\nover AutoRouting by 10+% EM under both set-\\ntings. For image questions, the gap becomes more\\nsigniﬁcant, with 20+% improvement under both\\nsettings. Similarly, we ﬁnd that our model is more\\ncapable of handling full-wiki corpus.\\nEvaluation Distractor\\nMetrics Retr FL Accuracy Overall\\nQuestion-Only - 34.9 22.2 13.4\\nVLP (Oscar) 68.9 42.6 36.7 22.6\\nVLP + ResNeXt 69.0 43.0 37.0 23.0\\nVLP + VinVL 70.9 44.2 38.9 24.1\\nMuRAG 74.6 55.7 54.6 36.1\\nEvaluation Full-Wiki\\nCLIP (2) + VLP 11.9 34.2 24.1 14.6\\nCLIP (20) + VLP 24.0 36.1 27.2 16.1\\nMuRAG 39.7 50.7 47.8 31.5\\nTable 4: WebQA ofﬁcial test-set results indicated\\non leaderboard 3 as of May 2022. Retr denotes\\nthe retrieval-F1 score. FL refers to ﬂuency metric\\nBARTSCcore, and Accuracy refers to keyword match-\\ning F1 score, they are combined as Overall.\\nEvaluation Distractor\\nMetrics Text Image All\\nEM F1 EM F1 EM\\nQuestion-Only 15.4 18.4 11.0 15.6 13.8\\nAutoRouting 49.5 56.9 37.8 37.8 46.6\\nMuRAG 60.8 67.5 58.2 58.2 60.2\\nEvaluation Full-Wiki\\nMetrics Text Image All\\nEM F1 EM F1 EM\\nCLIP (10) +\\nAutoRouting 35.6 40.2 32.5 32.5 34.7\\nMuRAG 49.7 56.1 56.5 56.5 51.4\\nTable 5: Multimodal dev-set results on the subset.\\n4.5 Ablation Study\\nHere we ablate the properties of MuRAG to better\\nunderstand our experimental results.\\nPre-training Corpus In order to study the contri-\\nbutions of different pre-training corpora, we investi-\\ngated several pre-training corpus combinations. We\\nreport their ﬁne-tuned results on WebQA test set\\nin Table 6. As can be seen, without any pre-training,\\nour model only achieves an overall score of 23.5,\\nwhich lags behind the baseline models. After pre-\\ntraining on different singular datasets, MuRAG is\\nable to achieve better performance than the base-\\nlines. Among the individual datasets, LAION is\\nshown to yield the highest score, and adding CC,\\nPAQ, and VQA to the pre-training corpus set one\\nby one produces steady improvements.\\nTwo-Stage Fine-tuning In order to study the ne-\\ncessity of the two-stage ﬁne-tuning, we perform an\\nablation study to see the impact of the two stages.\\nWe display our results in Table 7. (Only In-Batch)'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 7, 'page_label': '8'}, page_content='Pre-train Dataset FL Accuracy Overall\\nNone 42.5 36.1 23.5\\nCC 46.4 41.3 25.6\\nLAION 47.8 44.8 28.3\\nVQA 47.0 44.4 27.4\\nPAQ 46.8 42.8 27.0\\nLAION+CC 49.5 47.4 30.7\\nLAION+CC+PAQ 53.7 51.8 34.4\\nLAION+CC+PAQ+VQA 55.7 54.6 36.1\\nTable 6: Ablation Study for different pre-training cor-\\npus, score under distractor setting.\\nModel WebQA Multimodal\\nMuRAG (Only In-Batch) 29.4 49.6\\nMuRAG (Only Fixed-Retrieval) 25.8 40.7\\nMuRAG (Two Stage) 31.5 51.4\\nTable 7: Ablation Study for different ﬁne-tuning stages\\nto see their contributions. WebQA uses the overall\\nscore, and MultimodalQA refers to EM-all score.\\nEvaluation Model Correct Wrong\\nDistractor MuRAG (Text) 80% 20%\\nMuRAG (Image) 64% 36%\\nFull-Wiki MuRAG (Text) 72% 28%\\nMuRAG (Image) 54% 46%\\nTable 8: The human evaluation results on WebQA\\ndataset separately for image/text queries.\\nrefers to the model trained only with in-batch mem-\\nory are directly used to generate outputs by access-\\ning the global memory. Without further tuning,\\nthe performance will drop by roughly 2% on both\\ndatasets. (Only Fixed-Retrieval) refers to using the\\npre-trained retriever directly to obtain Top-K and\\nthen optimize the generative loss. As can be seen,\\nthe performance drop is more severe in this case\\nfor both datasets. This is understandable due the\\nmisalignment between pre-training retrieval is (im-\\nage + text->text) while the ﬁne-tuning retrieval is\\n(text -> image+text). Thus, it is necessary to adapt\\nthe MuRAG’s pre-trained retriever to different use\\ncases depending on the downstream datasets.\\n4.6 Human Analysis\\nIn order to better understand the model’s perfor-\\nmance, we manually study 200 model outputs and\\nclassify them into three categories and show our\\nmanual analysis results in Table 8. As can be seen,\\nimage queries are much harder than text queries.\\nMuRAG only achieves 64% accuracy for the dis-\\ntractor setting and 54% accuracy for the full-wiki\\nsetting, falling signiﬁcantly behind text accuracy.\\nWe further categorize the image-query errors\\nFigure 5: Upper left: correct prediction, Upper Right:\\nerror due to miscounting, Lower: error due to misrecog-\\nnition (multiple image reasoning). Q refers to the ques-\\ntion, P refers to prediction and R refers to the reference.\\nmanually into the categories of Table 9. Counting\\nis the most difﬁcult question type, and constitutes\\n52% of the total errors, while object recognition\\nerrors rank second, constituting 29% of errors. In\\ncontrast, identifying color, shape, and gender is\\ncomparatively easier, with fairly low error rates.\\nWe demonstrate some correct and typical error\\ncases in Figure 5 including miscounting and mis-\\nrecognizing objects. We observe that these errors\\nare mostly due to several reasons: 1) the question\\nis related to infrequent objects, thus making recog-\\nnition errors, 2) the image scene is highly complex\\nwith a large number of objects, thus grounding to a\\nspeciﬁc region is difﬁcult, 3) the questions require\\noptical character recognition ability from images.\\nHence, the bottleneck of MuRAG is still in the\\nvisual understanding module.\\nCategory Count Object Color Shape Gender\\nRatio 52% 29.4% 5.8% 5.8% 5.8%\\nTable 9: Error categorization and their ratios on sam-\\npled WebQA-dev image queries.\\n5 Examples\\nWe list more examples in Figure 6 and Figure 7.\\nAs can be seen, in the ﬁrst example, the model is'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 8, 'page_label': '9'}, page_content='grounded on the oracle image-text pair to make the\\ncorrect prediction. However, in the second exam-\\nple, though the model retrieves the wrong image-\\ntext pair, it is able to make the correct prediction of\\n‘the angel is holding a dead body’. We conjecture\\nthat the model utilizes textual clues to make the pre-\\ndiction rather than grounding on the image itself.\\nSuch shortcut learning is concerning and needs to\\nbe addressed through better learning algorithms.\\nFigure 6: Examples: we demonstrate model retrieval\\nvs. groundtruth and model answer vs. reference.\\n6 Conclusion\\nIn this paper, we build the ﬁrst visually-grounded\\nlanguage generator capable of retrieving multi-\\nmodal knowledge from a large-scale corpus. Our\\nexperiments show the promise of this approach, as\\nit outperforms existing baselines by a large margin.\\nAt the same time, the performance on knowledge-\\nseeking queries that require reasoning over images\\nis still signiﬁcantly lower than the performance on\\nqueries requiring only text. This indicates that there\\nis still ample room for further improvements and\\nwe hope our study can motivate more research on\\nbetter multimodal retrieval-augmented models.\\nLimitations\\nThe current approach has several limitations: 1)\\nsince we do not mine hard negatives during pre-\\ntraining, negatives come from other examples\\nwithin the same batch. This requires that we set the\\nbatch size sufﬁciently large enough to collect hard-\\nenough negatives. This results in the pre-training\\nFigure 7: Examples: we demonstrate model retrieval\\nvs. groundtruth, and model answer vs. reference.\\nrequiring a large number of computation resources\\nto reach competitive retrieval abilities. 2) our pre-\\ntraining corpus’s format (image -> text) is differ-\\nent from ﬁne-tuning (text -> image+text). This\\nmisalignment limits the model’s performance. Fu-\\nture work should consider how to design a better-\\naligned pre-training objective to achieve better per-\\nformance. 3) Current visual representation in the\\nreader stage is relatively expensive, i.e. 16x16=196\\ntokens per image, which poses great challenges for\\nthe transformer encoder to scale up to large Top-K\\nvalues due to the quadratic attention complexity.\\nEthical Statement\\nOur work uses the LAION dataset, a widely-used\\nand publicly available large-scale visual-language\\ncorpus crawled from the web. The authors have\\nconducted automatic ﬁltering to greatly reduce\\nharmful content. However, it is not possible to\\nfully remove all of the potential risks from the data\\ngiven its tremendous size. Being trained on this\\ndataset, we anticipate our model to contain some\\nbiases (racial, gender, etc.). During our manual\\ninspection, we saw some such biases, for example,\\n5% of errors are caused by misrecognition of gen-\\nder. However, there are other many other forms of\\nbiases that we cannot fully enumerate or observe'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10'}, page_content='explicitly.\\nReferences\\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\\ngaret Mitchell, Dhruv Batra, C Lawrence Zitnick,\\nand Devi Parikh. 2015. Vqa: Visual question an-\\nswering. In Proceedings of the IEEE international\\nconference on computer vision, pages 2425–2433.\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\\nmann, Trevor Cai, Eliza Rutherford, Katie Millican,\\nGeorge van den Driessche, Jean-Baptiste Lespiau,\\nBogdan Damoc, Aidan Clark, et al. 2021. Improv-\\ning language models by retrieving from trillions of\\ntokens. arXiv preprint arXiv:2112.04426.\\nJames Bradbury, Roy Frostig, Peter Hawkins,\\nMatthew James Johnson, Chris Leary, Dougal\\nMaclaurin, George Necula, Adam Paszke, Jake\\nVanderPlas, Skye Wanderman-Milne, and Qiao\\nZhang. 2018. JAX: composable transformations of\\nPython+NumPy programs.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot\\nlearners. Advances in neural information processing\\nsystems, 33:1877–1901.\\nYingshan Chang, Mridu Narang, Hisami Suzuki, Gui-\\nhong Cao, Jianfeng Gao, and Yonatan Bisk. 2022.\\nWebqa: Multihop and multimodal qa. The Confer-\\nence on Computer Vision and Pattern Recognition.\\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and\\nRadu Soricut. 2021. Conceptual 12m: Pushing web-\\nscale image-text pre-training to recognize long-tail\\nvisual concepts. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recog-\\nnition, pages 3558–3568.\\nWenhu Chen, Pat Verga, Michiel de Jong, John Wi-\\neting, and William Cohen. 2022. Augmenting\\npre-trained language models with qa-memory for\\nopen-domain question answering. arXiv preprint\\narXiv:2204.04581.\\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-\\nishna Vedantam, Saurabh Gupta, Piotr Dollár, and\\nC Lawrence Zitnick. 2015. Microsoft coco captions:\\nData collection and evaluation server.arXiv preprint\\narXiv:1504.00325.\\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed\\nEl Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\\nJingjing Liu. 2020. Uniter: Universal image-text\\nrepresentation learning. In European conference on\\ncomputer vision, pages 104–120. Springer.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\\nPaul Barham, Hyung Won Chung, Charles Sutton,\\nSebastian Gehrmann, et al. 2022. Palm: Scaling\\nlanguage modeling with pathways. arXiv preprint\\narXiv:2204.02311.\\nMichiel de Jong, Yury Zemlyanskiy, Nicholas FitzGer-\\nald, Fei Sha, and William Cohen. 2022. Mention\\nmemory: incorporating textual knowledge into trans-\\nformers through entity mention attention. ICLR.\\nAlexey Dosovitskiy, Lucas Beyer, Alexander\\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\\nThomas Unterthiner, Mostafa Dehghani, Matthias\\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2020.\\nAn image is worth 16x16 words: Transformers\\nfor image recognition at scale. In International\\nConference on Learning Representations.\\nRuiqi Guo, Philip Sun, Erik Lindgren, Quan Geng,\\nDavid Simcha, Felix Chern, and Sanjiv Kumar. 2020.\\nAccelerating large-scale inference with anisotropic\\nvector quantization. In International Conference on\\nMachine Learning, pages 3887–3896. PMLR.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-\\nsupat, and Mingwei Chang. 2020. Retrieval aug-\\nmented language model pre-training. In Proceed-\\nings of the 37th International Conference on Ma-\\nchine Learning, volume 119 of Proceedings of Ma-\\nchine Learning Research, pages 3929–3938. PMLR.\\nDarryl Hannan, Akshay Jain, and Mohit Bansal. 2020.\\nManymodalqa: Modality disambiguation and qa\\nover diverse inputs. In Proceedings of the AAAI Con-\\nference on Artiﬁcial Intelligence , volume 34, pages\\n7879–7886.\\nGautier Izacard and Édouard Grave. 2021. Leveraging\\npassage retrieval with generative models for open\\ndomain question answering. In Proceedings of the\\n16th Conference of the European Chapter of the As-\\nsociation for Computational Linguistics: Main Vol-\\nume, pages 874–880.\\nNorman P Jouppi, Doe Hyun Yoon, George Kurian,\\nSheng Li, Nishant Patil, James Laudon, Cliff Young,\\nand David Patterson. 2020. A domain-speciﬁc\\nsupercomputer for training deep neural networks.\\nCommunications of the ACM, 63(7):67–78.\\nAndrej Karpathy and Li Fei-Fei. 2015. Deep visual-\\nsemantic alignments for generating image descrip-\\ntions. In Proceedings of the IEEE conference\\non computer vision and pattern recognition , pages\\n3128–3137.\\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\\nZettlemoyer, and Mike Lewis. 2019. Generalization\\nthrough memorization: Nearest neighbor language\\nmodels. In International Conference on Learning\\nRepresentations.\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\\ntäschel, et al. 2020. Retrieval-augmented generation\\nfor knowledge-intensive nlp tasks. Advances in Neu-\\nral Information Processing Systems, 33:9459–9474.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11'}, page_content='Patrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale\\nMinervini, Heinrich Küttler, Aleksandra Piktus, Pon-\\ntus Stenetorp, and Sebastian Riedel. 2021. Paq: 65\\nmillion probably-asked questions and what you can\\ndo with them. Transactions of the Association for\\nComputational Linguistics, 9:1098–1115.\\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xi-\\naowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu,\\nLi Dong, Furu Wei, et al. 2020. Oscar: Object-\\nsemantics aligned pre-training for vision-language\\ntasks. In European Conference on Computer Vision,\\npages 121–137. Springer.\\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\\nand C Lawrence Zitnick. 2014. Microsoft coco:\\nCommon objects in context. In European confer-\\nence on computer vision, pages 740–755. Springer.\\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\\nLee. 2019. Vilbert: Pretraining task-agnostic visi-\\nolinguistic representations for vision-and-language\\ntasks. Advances in neural information processing\\nsystems, 32.\\nKenneth Marino, Mohammad Rastegari, Ali Farhadi,\\nand Roozbeh Mottaghi. 2019. Ok-vqa: A visual\\nquestion answering benchmark requiring external\\nknowledge. In Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recogni-\\ntion, pages 3195–3204.\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish\\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark,\\net al. 2021. Learning transferable visual models\\nfrom natural language supervision. In International\\nConference on Machine Learning, pages 8748–8763.\\nPMLR.\\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\\nine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\\nthe limits of transfer learning with a uniﬁed text-to-\\ntext transformer. Journal of Machine Learning Re-\\nsearch, 21(140):1–67.\\nRevanth Gangi Reddy, Xilin Rui, Manling Li, Xudong\\nLin, Haoyang Wen, Jaemin Cho, Lifu Huang, Mo-\\nhit Bansal, Avirup Sil, Shih-Fu Chang, et al. 2021.\\nMumuqa: Multimedia multi-hop news question an-\\nswering via cross-media knowledge extraction and\\ngrounding. arXiv preprint arXiv:2112.10728.\\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian\\nSun. 2015. Faster r-cnn: Towards real-time object\\ndetection with region proposal networks. Advances\\nin neural information processing systems, 28.\\nAdam Roberts, Hyung Won Chung, Anselm Lev-\\nskaya, Gaurav Mishra, James Bradbury, Daniel An-\\ndor, Sharan Narang, Brian Lester, Colin Gaffney,\\nAfroz Mohiuddin, et al. 2022. Scaling up mod-\\nels and data with t5x and seqio. arXiv preprint\\narXiv:2203.17189.\\nChristoph Schuhmann, Richard Vencu, Romain Beau-\\nmont, Robert Kaczmarczyk, Clayton Mullis, Aarush\\nKatta, Theo Coombes, Jenia Jitsev, and Aran Komat-\\nsuzaki. 2021. Laion-400m: Open dataset of clip-\\nﬁltered 400 million image-text pairs. arXiv preprint\\narXiv:2111.02114.\\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\\nRadu Soricut. 2018. Conceptual captions: A\\ncleaned, hypernymed, image alt-text dataset for au-\\ntomatic image captioning. In Proceedings of the\\n56th Annual Meeting of the Association for Compu-\\ntational Linguistics (Volume 1: Long Papers), pages\\n2556–2565.\\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\\nAdaptive learning rates with sublinear memory cost.\\nIn International Conference on Machine Learning ,\\npages 4596–4604. PMLR.\\nHrituraj Singh, Anshul Nasery, Denil Mehta, Aish-\\nwarya Agarwal, Jatin Lamba, and Balaji Vasan Srini-\\nvasan. 2021. Mimoqa: Multimodal input multi-\\nmodal output question answering. In Proceedings\\nof the 2021 Conference of the North American Chap-\\nter of the Association for Computational Linguistics:\\nHuman Language Technologies, pages 5317–5332.\\nAlon Talmor, Ori Yoran, Amnon Catav, Dan Lahav,\\nYizhong Wang, Akari Asai, Gabriel Ilharco, Han-\\nnaneh Hajishirzi, and Jonathan Berant. 2021. Multi-\\nmodalqa: complex question answering over text, ta-\\nbles and images. In ICLR.\\nPat Verga, Haitian Sun, Livio Baldini Soares, and\\nWilliam Weston Cohen. 2021. Adaptable and inter-\\npretable neural memory over symbolic knowledge.\\nIn Proceedings of NAACL-HLT, pages 3678–3691.\\nZirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yu-\\nlia Tsvetkov, and Yuan Cao. 2022. Simvlm: Simple\\nvisual language model pretraining with weak super-\\nvision. ICLR.\\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Ye-\\nung, Mojtaba Seyedhosseini, and Yonghui Wu. 2022.\\nCoca: Contrastive captioners are image-text founda-\\ntion models. arXiv preprint arXiv:2205.01917.\\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\\nBartscore: Evaluating generated text as text gener-\\nation. Advances in Neural Information Processing\\nSystems, 34.\\nPengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei\\nYang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jian-\\nfeng Gao. 2021. Vinvl: Revisiting visual representa-\\ntions in vision-language models. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and\\nPattern Recognition, pages 5579–5588.\\nLuowei Zhou, Hamid Palangi, Lei Zhang, Houdong\\nHu, Jason Corso, and Jianfeng Gao. 2020. Uni-\\nﬁed vision-language pre-training for image caption-\\ning and vqa. In Proceedings of the AAAI Conference\\non Artiﬁcial Intelligence , volume 34, pages 13041–\\n13049.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 11, 'page_label': '12'}, page_content='A Pre-training\\nDuring Pre-trainnig, we found that directly train-\\ning with a mixture of all four datasets will lead to\\ninstability. We experimented with different vari-\\nants and found that a scheduled pre-training can\\nlead to a stable solution. We propose to ﬁrst pre-\\ntrain the model on the largest LAION dataset for\\n1M steps, and then continue training on the other\\nthree datasets with a ﬁxed sample ratio. We plot\\nthe ﬁrst stage of LAION training in Figure 8. We\\nmonitor the generation quality (LAION image ->\\ntext captioning), and the retrieval quality (image ->\\n4096 in-batch caption retrieval). As can be seen,\\nthe LAION pre-training converges after 1M steps,\\nwhere we ﬁrst warm up and then decrease the learn-\\ning rate using a scheduler.\\nFigure 8: LAION Pre-training, validation accuracy,\\ngeneration Cider score and retrieval recall score from\\nthe in-batch memory.\\nWe further the pre-training on a mixture of the\\nother three datasets. We plot their inference eval-\\nuation scores in Figure 9. We can see that the\\nmodel is able to achieve very strong performance\\non these datasets, i.e. higher than 1.2 CiDEr\\non CC12M+3M validation set. The model also\\nachieves strong performance on text-only reading\\ncomprehension on PAQ (similar to NQ), i.e. higher\\nthan 55% EM score. On the VQA dataset, the\\nmodel is able to achieve higher than 72% VQA ac-\\ncuracy on the validation set. These results demon-\\nstrate the efﬁciency and multi-tasking capabilities\\nof the pre-trained model. The overall retrieval\\naccuracy from the multimodal memory consist-\\ning of captions, and passages are plotted in Fig-\\nure 10, where the model is able to achieve 85%\\nRECALL@1 from a 4K memory.\\nB Model Conﬁguration\\nWe demonstrate the ViT conﬁguration as follows:\\n\" v i t _ c o n f i g \" : {\\n\" model \" : \" ViT \" ,\\n\" p a t c h e s \" : {\\n\" s i z e \" : [ 1 6 , 16]\\n} ,\\n\" h i d d e n _ s i z e \" : 1024 ,\\n\" i m a g e _ s i z e \" : [ 2 2 4 , 2 2 4 ] ,\\n\" num_heads \" : 16 ,\\n\" n u m _ l a y e r s \" : 24 ,\\n\" mlp_dim \" : 4096 ,\\n\" r e t u r n _ p o o l e d _ o u t p u t \" : f a l s e ,\\n\" d r o p o u t _ r a t e \" : 0 . 1\\n} ,\\nWe demonstrate the T5-EncDec conﬁguration as\\nfollows:\\n\" m o d e l _ c o n f i g \" : {\\n\" v o c a b _ s i z e \" : 32128 ,\\n\" h i d d e n _ s i z e \" : 768 ,\\n\" i n t e r m e d i a t e _ d i m \" : 2048 ,\\n\" n u m _ a t t e n t i o n _ h e a d s \" : 12 ,\\n\" memory_key_dim \" : 768 ,\\n\" e n c o d e r _ l a y e r s \" : 12 ,\\n\" d e c o d e r _ l a y e r s \" : 12 ,\\n\" d r o p o u t _ r a t e \" : 0 . 1 ,\\n\" m a x _ d i s t a n c e \" : 128 ,\\n\" num_buckets \" : 32 ,\\n\" s c a l e \" : 1 . 0 ,\\n\" r e t r i e v a l _ w e i g h t \" : 0 . 5 ,\\n}'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 12, 'page_label': '13'}, page_content='Figure 9: Mixture Pre-training, CiDEr, EM, and VQA\\naccuracy for CC, PAQ, and VQA datasets.\\nFigure 10: Mixture Pre-training retrieval accuracy over\\nCC, PAQ, and VQA datasets.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1'}, page_content='Ragas: Automated Evaluation of Retrieval Augmented Generation\\nShahul Es†, Jithin James†, Luis Espinosa-Anke∗♢, Steven Schockaert∗\\n†Exploding Gradients\\n∗CardiffNLP, Cardiff University, United Kingdom\\n♢AMPLYFI, United Kingdom\\nshahules786@gmail.com,jamesjithin97@gmail.com\\n{espinosa-ankel,schockaerts1}@cardiff.ac.uk\\nAbstract\\nWe introduce Ragas (Retrieval Augmented\\nGeneration Assessment), a framework for\\nreference-free evaluation of Retrieval Aug-\\nmented Generation (RAG) pipelines. RAG\\nsystems are composed of a retrieval and an\\nLLM based generation module, and provide\\nLLMs with knowledge from a reference textual\\ndatabase, which enables them to act as a natu-\\nral language layer between a user and textual\\ndatabases, reducing the risk of hallucinations.\\nEvaluating RAG architectures is, however, chal-\\nlenging because there are several dimensions to\\nconsider: the ability of the retrieval system to\\nidentify relevant and focused context passages,\\nthe ability of the LLM to exploit such passages\\nin a faithful way, or the quality of the genera-\\ntion itself. With Ragas, we put forward a suite\\nof metrics which can be used to evaluate these\\ndifferent dimensions without having to rely on\\nground truth human annotations. We posit that\\nsuch a framework can crucially contribute to\\nfaster evaluation cycles of RAG architectures,\\nwhich is especially important given the fast\\nadoption of LLMs.\\n1 Introduction\\nLanguage Models (LMs) capture a vast amount\\nof knowledge about the world, which allows them\\nto answer questions without accessing any exter-\\nnal sources. This idea of LMs as repositories of\\nknowledge emerged shortly after the introduction\\nof BERT (Devlin et al., 2019) and became more\\nfirmly established with the introduction of ever\\nlarger LMs (Roberts et al., 2020). While the most\\nrecent Large Language Models (LLMs) capture\\nenough knowledge to rival human performance\\nacross a wide variety of question answering bench-\\nmarks (Bubeck et al., 2023), the idea of using\\nLLMs as knowledge bases still has two fundamen-\\ntal limitations. First, LLMs are not able to answer\\nquestions about events that have happened after\\nthey were trained. Second, even the largest models\\nstruggle to memorise knowledge that is only rarely\\nmentioned in the training corpus (Kandpal et al.,\\n2022; Mallen et al., 2023). The standard solution\\nto these issues is to rely on Retrieval Augmented\\nGeneration (RAG) (Lee et al., 2019; Lewis et al.,\\n2020; Guu et al., 2020). Answering a question\\nthen essentially involves retrieving relevant pas-\\nsages from a corpus and feeding these passages,\\nalong with the original question, to the LM. While\\ninitial approaches relied on specialised LMs for\\nretrieval-augmented language modelling (Khandel-\\nwal et al., 2020; Borgeaud et al., 2022), recent work\\nhas suggested that simply adding retrieved docu-\\nments to the input of a standard LM can also work\\nwell (Khattab et al., 2022; Ram et al., 2023; Shi\\net al., 2023), thus making it possible to use retrieval-\\naugmented strategies in combination with LLMs\\nthat are only available through APIs.\\nWhile the usefulness of retrieval-augmented\\nstrategies is clear, their implementation requires\\na significant amount of tuning, as the overall per-\\nformance will be affected by the retrieval model,\\nthe considered corpus, the LM, or the prompt for-\\nmulation, among others. Automated evaluation of\\nretrieval-augmented systems is thus paramount. In\\npractice, RAG systems are often evaluated in terms\\nof the language modelling task itself, i.e. by mea-\\nsuring perplexity on some reference corpus. How-\\never, such evaluations are not always predictive\\nof downstream performance (Wang et al., 2023c).\\nMoreover, this evaluation strategy relies on the LM\\nprobabilities, which are not accessible for some\\nclosed models (e.g. ChatGPT and GPT-4). Ques-\\ntion answering is another common evaluation task,\\nbut usually only datasets with short extractive an-\\nswers are considered, which may not be represen-\\ntative of how the system will be used.\\nTo address these issues, in this paper we present\\nRagas1, a framework for the automated assessment\\n1Ragas is available at https://github.com/\\nexplodinggradients/ragas.\\narXiv:2309.15217v2  [cs.CL]  28 Apr 2025'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2'}, page_content='of retrieval augmented generation systems. We fo-\\ncus on settings where reference answers may not be\\navailable, and where we want to estimate different\\nproxies for correctness, in addition to the useful-\\nness of the retrieved passages. The Ragas frame-\\nwork provides an integration with both llama-index\\nand Langchain, the most widely used frameworks\\nfor building RAG solutions, thus enabling devel-\\nopers to easily integrate Ragas into their standard\\nworkflow.\\n2 Related Work\\nEstimating faithfulness using LLMsThe prob-\\nlem of detecting hallucinations in LLM generated\\nresponses has been extensively studied (Ji et al.,\\n2023). Several authors have suggested the idea\\nof predicting factuality using a few-shot prompt-\\ning strategy (Zhang et al., 2023). Recent analy-\\nses, however, suggest that existing models struggle\\nwith detecting hallucination when using standard\\nprompting strategies (Li et al., 2023; Azaria and\\nMitchell, 2023). Other approaches rely on linking\\nthe generated responses to facts from an external\\nknowledge base (Min et al., 2023), but this is not\\nalways possible.\\nYet another strategy is to inspect the probabili-\\nties assigned to individual tokens, where we would\\nexpect the model to be less confident in halluci-\\nnated answers than in factual ones. For instance,\\nBARTScore (Yuan et al., 2021) estimates factuality\\nby looking at the conditional probability of the gen-\\nerated text given the input. Kadavath et al. (2022)\\nuse a variation of this idea. Starting from the ob-\\nservation that LLMs provide well-calibrated proba-\\nbilities when answering multiple-choice questions,\\nthey essentially convert the problem of validating\\nmodel generated answers into a multiple-choice\\nquestion which asks whether the answer is true or\\nfalse. Rather than looking at the output probabil-\\nities, Azaria and Mitchell (2023) propose to train\\na supervised classifier on the weights from one of\\nthe hidden layers of the LLM, to predict whether a\\ngiven statement is true or not. While the approach\\nperforms well, the need to access the hidden states\\nof the model makes it unsuitable for systems that\\naccess LLMs through an API.\\nFor models that do not provide access to token\\nprobabilities, such as ChatGPT and GPT-4, differ-\\nent methods are needed. SelfCheckGPT (Manakul\\net al., 2023) addresses this problem by instead sam-\\npling multiple answers. Their core idea is that\\nfactual answers are more stable: when an answer is\\nfactual, we can expect that different samples will\\ntend to be semantically similar, whereas this is less\\nlikely to be the case for hallucinated answers.\\nAutomated evaluation of text generation systems\\nLLMs have also been leveraged to automatically\\nevaluate other aspects of generated text fragments,\\nbeyond factuality. For instance, GPTScore (Fu\\net al., 2023) uses a prompt that specifies the consid-\\nered aspect (e.g. fluency) and then scores passages\\nbased on the average probability of the generated\\ntokens, according to a given autoregressive LM.\\nThis idea of using prompts was previously also\\nconsidered by Yuan et al. (2021), although they\\nused a smaller fine-tuned LM (i.e. BART) and did\\nnot observe a clear benefit from using prompts. An-\\nother approach directly asks ChatGPT to evaluate\\na particular aspect of the given answer by provid-\\ning a score between 0 and 100, or by providing a\\nrating on a 5-star scale (Wang et al., 2023a). Re-\\nmarkably, strong results can be obtained in this\\nway, although it comes with the limitation of being\\nsensitive to the design of the prompt. Rather than\\nscoring individual answers, some authors have also\\nfocused on using an LLM to select the best answer\\namong a number of candidates (Wang et al., 2023b),\\ntypically to compare the performance of different\\nLLMs. However, care is needed with this approach,\\nas the order in which the answers is presented can\\ninfluence the result (Wang et al., 2023b).\\nIn terms of how ground truth answers or, more\\ngenerally, generations, have been typically used\\nin the literature, most approaches have relied on\\nthe availability of one or more reference answers.\\nFor instance, BERTScore (Zhang et al., 2020)\\nand MoverScore (Zhao et al., 2019) use contex-\\ntualised embeddings, produced by a pre-trained\\nBERT model, to compare the similarity between\\nthe generated answer and the reference answers.\\nBARTScore (Yuan et al., 2021) similarly uses refer-\\nence answers to compute aspects such as precision\\n(estimated as the probability of generating the gen-\\nerated answer given the reference) and recall (esti-\\nmated as the probability of generating the reference\\ngiven the generated answer).\\n3 Evaluation Strategies\\nWe consider a standard RAG setting, where given a\\nquestion q, the system first retrieves some context\\nc(q) and then uses the retrieved context to generate\\nan answer as(q). When building a RAG system,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3'}, page_content='we usually do not have access to human-annotated\\ndatasets or reference answers. We therefore fo-\\ncus on metrics that are fully self-contained and\\nreference-free. We focus in particular three quality\\naspects, which we argue are of central importance.\\nFirst, Faithfulness refers to the idea that the an-\\nswer should be grounded in the given context. This\\nis important to avoid hallucinations, and to ensure\\nthat the retrieved context can act as a justification\\nfor the generated answer. Indeed, RAG systems are\\noften used in applications where the factual con-\\nsistency of the generated text w.r.t. the grounded\\nsources is highly important, e.g. in domains such as\\nlaw, where information is constantly evolving. Sec-\\nond, Answer Relevancerefers to the idea that the\\ngenerated answer should address the actual ques-\\ntion that was provided. Finally,Context Relevance\\nrefers to the idea that the retrieved context should\\nbe focused, containing as little irrelevant informa-\\ntion as possible. This is important given the cost\\nassociated with feeding long context passages to\\nLLMs. Moreover, when context passages are too\\nlong, LLMs are often less effective in exploiting\\nthat context, especially for information that is pro-\\nvided in the middle of the context passage (Liu\\net al., 2023).\\nWe now explain how these three quality aspects\\ncan be measured in a fully automated way, by\\nprompting an LLM. In our implementation and\\nexperiments, all prompts are evaluated using the\\ngpt-3.5-turbo-16k model, which is available\\nthrough the OpenAI API2.\\nFaithfulness We say that the answer as(q) is\\nfaithful to the context c(q) if the claims that are\\nmade in the answer can be inferred from the con-\\ntext. To estimate faithfulness, we first use an LLM\\nto extract a set of statements, S(as(q)). The aim\\nof this step is to decompose longer sentences into\\nshorter and more focused assertions. We use the\\nfollowing prompt for this step3:\\nGiven a question and answer, create one\\nor more statements from each sentence\\nin the given answer.\\nquestion: [question]\\nanswer: [answer]\\nwhere [question] and [answer] refer to the\\ngiven question and answer. For each statement si\\n2https://platform.openai.com\\n3To help clarify the task, we include a demonstration as\\npart of the prompt. This demonstration is not explicitly shown\\nin the listing of the prompts throughout this paper.\\nin S, the LLM determines ifsi can be inferred from\\nc(q) using a verification function v(si, c(q)). This\\nverification step is carried out using the following\\nprompt:\\nConsider the given context and following\\nstatements, then determine whether they\\nare supported by the information present\\nin the context. Provide a brief explana-\\ntion for each statement before arriving\\nat the verdict (Yes/No). Provide a final\\nverdict for each statement in order at the\\nend in the given format. Do not deviate\\nfrom the specified format.\\nstatement: [statement 1]\\n...\\nstatement: [statement n]\\nThe final faithfulness score, F, is then computed\\nas F = |V |\\n|S| , where |V | is the number of statements\\nthat were supported according to the LLM and |S|\\nis the total number of statements.\\nAnswer relevance We say that the answer as(q)\\nis relevant if it directly addresses the question in\\nan appropriate way. In particular, our assessment\\nof answer relevance does not take into account fac-\\ntuality, but penalises cases where the answer is\\nincomplete or where it contains redundant informa-\\ntion. To estimate answer relevance, for the given\\nanswer as(q), we prompt the LLM to generate n\\npotential questions qi based on as(q), as follows:\\nGenerate a question for the given answer.\\nanswer: [answer]\\nWe then obtain embeddings for all questions us-\\ning the text-embedding-ada-002 model, avail-\\nable from the OpenAI API. For each qi, we cal-\\nculate the similarity sim(q, qi) with the original\\nquestion q, as the cosine between the correspond-\\ning embeddings. The answer relevance score, AR,\\nfor question q is then computed as:\\nAR = 1\\nn\\nnX\\ni=1\\nsim(q, qi) (1)\\nThis metric evaluates how closely the generated\\nanswer aligns with the initial question or instruc-\\ntion.\\nContext relevance The context c(q) is consid-\\nered relevant to the extent that it exclusively con-\\ntains information that is needed to answer the ques-\\ntion. In particular, this metric aims to penalise the'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4'}, page_content='inclusion of redundant information. To estimate\\ncontext relevance, given a question q and its con-\\ntext c(q), the LLM extracts a subset of sentences,\\nSext, from c(q) that are crucial to answer q, using\\nthe following prompt:\\nPlease extract relevant sentences from\\nthe provided context that can potentially\\nhelp answer the following question. If no\\nrelevant sentences are found, or if you\\nbelieve the question cannot be answered\\nfrom the given context, return the phrase\\n\"Insufficient Information\". While extract-\\ning candidate sentences you’re not al-\\nlowed to make any changes to sentences\\nfrom given context.\\nThe context relevance score is then computed as:\\nCR = number of extracted sentences\\ntotal number of sentences in c(q) (2)\\n4 The WikiEval Dataset\\nTo evaluate the proposed framework, we ideally\\nneed examples of question-context-answer triples\\nwhich are annotated with human judgments. We\\ncan then verify to what extent our metrics agree\\nwith human assessments of faithfulness, answer\\nrelevance and context relevance. Since we are not\\naware of any publicly available datasets that could\\nbe used for this purpose, we created a new dataset,\\nwhich we refer to as WikiEval4. To construct the\\ndataset, we first selected 50 Wikipedia pages cov-\\nering events that have happened since the start of\\n20225. In selecting these pages, we prioritised\\nthose with recent edits. For each of the 50 pages,\\nwe then asked ChatGPT to suggest a question that\\ncan be answered based on the introductory section\\nof the page, using the following prompt:\\nYour task is to formulate a question from\\ngiven context satisfying the rules given\\nbelow:\\n1. The question should be fully answered\\nfrom the given context.\\n2. The question should be framed from\\na part that contains non-trivial informa-\\ntion.\\n3. The answer should not contain any\\n4https://huggingface.co/datasets/\\nexplodinggradients/WikiEval\\n5That is, beyond the reported training cutoff of the model\\nwe used in our experiments.\\nlinks.\\n4. The question should be of moderate\\ndifficulty.\\n5. The question must be reasonable and\\nmust be understood and responded to by\\nhumans.\\n6. Do not use phrases that ’provided con-\\ntext’, etc in the question\\ncontext:\\nWe also used ChatGPT to answer the generated\\nquestion, when given the corresponding introduc-\\ntory section as context, using the following prompt:\\nAnswer the question using the informa-\\ntion from the given context.\\nquestion: [question]\\ncontext: [context]\\nAll questions were annotated along the three con-\\nsidered quality dimensions by two annotators. Both\\nannotators were fluent in English and were given\\nclear instructions about the meaning of the three\\nconsidered quality dimensions. For faithfulness\\nand context relevance, the two annotators agreed in\\naround 95% of cases. For answer relevance, they\\nagreed in around 90% of the cases. Disagreements\\nwere resolved after a discussion between the anno-\\ntators.\\nFaithfulness To obtain human judgements about\\nfaithfulness, we first used ChatGPT to answer the\\nquestion without access to any additional context.\\nWe then asked the annotators to judge which of the\\ntwo answers was the most faithful (i.e. the standard\\none or the one generated without context), given\\nthe question and corresponding Wikipedia page.\\nAnswer relevance We first used ChatGPT to\\nobtain candidate answers with lower answer rel-\\nevance, using the following prompt:\\nAnswer the given question in an incom-\\nplete manner.\\nquestion: [question]\\nWe then asked human annotators to compare this\\nanswer, and indicate which of the two answers had\\nthe highest answer relevance.\\nContext relevance To measure this aspect, we\\nfirst added additional sentences to the context by\\nscraping back-links to the corresponding Wikipedia\\npage. In this way, we were able to add information\\nto the context that was related but less relevant for'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5'}, page_content='Faith. Ans. Rel. Cont. Rel.\\nRagas 0.95 0.78 0.70\\nGPT Score 0.72 0.52 0.63\\nGPT Ranking 0.54 0.40 0.52\\nTable 1: Agreement with human annotators in pairwise\\ncomparisons of faithfulness, answer relevance and con-\\ntext relevance, using the WikEval dataset (accuracy).\\nanswering the question. For the few pages with-\\nout any back-links, we instead used ChatGPT to\\ncomplete the given context.\\n5 Experiments\\nTable 1 analyses the agreement between the met-\\nrics proposed in Section 3 and the human assess-\\nments from the proposed WikiEval dataset. Each\\nWikiEval instance requires the model to compare\\ntwo answers or two context fragments. We count\\nhow often the answer/context preferred by the\\nmodel (i.e. with highest estimated faithfulness, an-\\nswer relevance, or context relevance) coincides\\nwith the answer/context preferred by the human\\nannotators. We report the results in terms of ac-\\ncuracy (i.e. the fraction of instances on which the\\nmodel agrees with the annotators).\\nTo put the results in context, we compare our\\nproposed metrics (shown as Ragas in Table 1) with\\ntwo baseline methods. For the first method, shown\\nas GPT Score, we ask ChatGPT to assign a score\\nbetween 0 and 10 for the three quality dimensions.\\nTo this end, we use a prompt that describes the\\nmeaning of the quality metric and then asks to\\nscore the given answer/context in line with that\\ndefinition. For instance, for evaluating faithfulness,\\nwe used the following prompt:\\nFaithfulness measures the information\\nconsistency of the answer against the\\ngiven context. Any claims that are made\\nin the answer that cannot be deduced\\nfrom context should be penalized.\\nGiven an answer and context, assign a\\nscore for faithfulness in the range 0-10.\\ncontext: [context]\\nanswer: [answer]\\nTies, where the same score is assigned by the LLM\\nto both answer candidates, were broken randomly.\\nThe second baseline, shown as GPT Ranking, in-\\nstead asks ChatGPT to select the preferred answer/-\\ncontext. In this case, the prompt again includes\\na definition of the considered quality metric. For\\ninstance, for evaluating answer relevance, we used\\nthe following prompt:\\nAnswer Relevancy measures the degree\\nto which a response directly addresses\\nand is appropriate for a given question.\\nIt penalizes the present of redundant in-\\nformation or incomplete answers given a\\nquestion. Given an question and answer,\\nrank each answer based on Answer Rele-\\nvancy.\\nquestion: [question]\\nanswer 1: [answer 1]\\nanswer 2: [answer 2]\\nThe results in Table 1 show that our proposed\\nmetrics are much closer aligned with the human\\njudgements than the predictions from the two base-\\nlines. For faithfulness, the Ragas prediction are in\\ngeneral highly accurate. For answer relevance, the\\nagreement is lower, but this is largely due to the\\nfact that the differences between the two candidate\\nanswers are often very subtle. We found context\\nrelevance to be the hardest quality dimension to\\nevaluate. In particular, we observed that ChatGPT\\noften struggles with the task of selecting the sen-\\ntences from the context that are crucial, especially\\nfor longer contexts.\\n6 Conclusions\\nWe have highlighted the need for automated\\nreference-free evaluation of RAG systems. In par-\\nticular, we have argued the need for an evaluation\\nframework that can assess faithfulness (i.e. is the\\nanswer grounded in the retrieved context), answer\\nrelevance (i.e. does the answer address the ques-\\ntion) and context relevance (i.e. is the retrieved\\ncontext sufficiently focused). To support the devel-\\nopment of such a framework, we have introduced\\nWikiEval, a dataset which human judgements of\\nthese three different aspects. Finally, we have also\\ndescribed Ragas, our implementation of the three\\nconsidered quality aspects. This framework is easy\\nto use and can provide deverlopers of RAG sys-\\ntems with valuable insights, even in the absence\\nof any ground truth. Our evaluation on WikiEval\\nhas shown that the predictions from Ragas are\\nclosely aligned with human predictions, especially\\nfor faithfulness and answer relevance.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6'}, page_content='References\\nAmos Azaria and Tom M. Mitchell. 2023. The inter-\\nnal state of an LLM knows when its lying. CoRR,\\nabs/2304.13734.\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\\nTrevor Cai, Eliza Rutherford, Katie Millican, George\\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia\\nGuy, Jacob Menick, Roman Ring, Tom Hennigan,\\nSaffron Huang, Loren Maggiore, Chris Jones, Albin\\nCassirer, Andy Brock, Michela Paganini, Geoffrey\\nIrving, Oriol Vinyals, Simon Osindero, Karen Si-\\nmonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.\\n2022. Improving language models by retrieving from\\ntrillions of tokens. In International Conference on\\nMachine Learning, ICML 2022, 17-23 July 2022, Bal-\\ntimore, Maryland, USA, volume 162 of Proceedings\\nof Machine Learning Research , pages 2206–2240.\\nPMLR.\\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\\nberg, et al. 2023. Sparks of artificial general intelli-\\ngence: Early experiments with gpt-4. arXiv preprint\\narXiv:2303.12712.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In Proceedings of the 2019 Conference of\\nthe North American Chapter of the Association for\\nComputational Linguistics: Human Language Tech-\\nnologies, Volume 1 (Long and Short Papers), pages\\n4171–4186, Minneapolis, Minnesota. Association for\\nComputational Linguistics.\\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\\nLiu. 2023. Gptscore: Evaluate as you desire. CoRR,\\nabs/2302.04166.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\\npat, and Mingwei Chang. 2020. Retrieval augmented\\nlanguage model pre-training. In International confer-\\nence on machine learning, pages 3929–3938. PMLR.\\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\\nMadotto, and Pascale Fung. 2023. Survey of halluci-\\nnation in natural language generation. ACM Comput-\\ning Surveys, 55(12):1–38.\\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\\nHenighan, Dawn Drain, Ethan Perez, Nicholas\\nSchiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli\\nTran-Johnson, Scott Johnston, Sheer El Showk, Andy\\nJones, Nelson Elhage, Tristan Hume, Anna Chen,\\nYuntao Bai, Sam Bowman, Stanislav Fort, Deep\\nGanguli, Danny Hernandez, Josh Jacobson, Jack-\\nson Kernion, Shauna Kravec, Liane Lovitt, Ka-\\nmal Ndousse, Catherine Olsson, Sam Ringer, Dario\\nAmodei, Tom Brown, Jack Clark, Nicholas Joseph,\\nBen Mann, Sam McCandlish, Chris Olah, and Jared\\nKaplan. 2022. Language models (mostly) know what\\nthey know. CoRR, abs/2207.05221.\\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric\\nWallace, and Colin Raffel. 2022. Large language\\nmodels struggle to learn long-tail knowledge. CoRR,\\nabs/2211.08411.\\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\\nZettlemoyer, and Mike Lewis. 2020. Generalization\\nthrough memorization: Nearest neighbor language\\nmodels. In 8th International Conference on Learning\\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia,\\nApril 26-30, 2020. OpenReview.net.\\nOmar Khattab, Keshav Santhanam, Xiang Lisa Li,\\nDavid Hall, Percy Liang, Christopher Potts, and\\nMatei Zaharia. 2022. Demonstrate-search-predict:\\nComposing retrieval and language models for\\nknowledge-intensive NLP. CoRR, abs/2212.14024.\\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\\n2019. Latent retrieval for weakly supervised open do-\\nmain question answering. In Proceedings of the 57th\\nAnnual Meeting of the Association for Computational\\nLinguistics, pages 6086–6096.\\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,\\nTim Rocktäschel, Sebastian Riedel, and Douwe\\nKiela. 2020. Retrieval-augmented generation for\\nknowledge-intensive NLP tasks. In Advances in Neu-\\nral Information Processing Systems 33: Annual Con-\\nference on Neural Information Processing Systems\\n2020, NeurIPS 2020, December 6-12, 2020, virtual.\\nJunyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun\\nNie, and Ji-Rong Wen. 2023. Halueval: A large-\\nscale hallucination evaluation benchmark for large\\nlanguage models. CoRR, abs/2305.11747.\\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-\\njape, Michele Bevilacqua, Fabio Petroni, and Percy\\nLiang. 2023. Lost in the middle: How language\\nmodels use long contexts.\\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\\nDaniel Khashabi, and Hannaneh Hajishirzi. 2023.\\nWhen not to trust language models: Investigating\\neffectiveness of parametric and non-parametric mem-\\nories. In Proceedings of the 61st Annual Meeting of\\nthe Association for Computational Linguistics (Vol-\\nume 1: Long Papers) , pages 9802–9822, Toronto,\\nCanada. Association for Computational Linguistics.\\nPotsawee Manakul, Adian Liusie, and Mark J. F. Gales.\\n2023. Selfcheckgpt: Zero-resource black-box hal-\\nlucination detection for generative large language\\nmodels. CoRR, abs/2303.08896.\\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike\\nLewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,\\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2023.\\nFactscore: Fine-grained atomic evaluation of fac-\\ntual precision in long form text generation. CoRR,\\nabs/2305.14251.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7'}, page_content='Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\\nShoham. 2023. In-context retrieval-augmented lan-\\nguage models. CoRR, abs/2302.00083.\\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\\nHow much knowledge can you pack into the param-\\neters of a language model? In Proceedings of the\\n2020 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP), pages 5418–5426,\\nOnline. Association for Computational Linguistics.\\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon\\nSeo, Rich James, Mike Lewis, Luke Zettlemoyer, and\\nWen-tau Yih. 2023. REPLUG: retrieval-augmented\\nblack-box language models. CoRR, abs/2301.12652.\\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxi-\\nang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie\\nZhou. 2023a. Is chatgpt a good NLG evaluator? A\\npreliminary study. CoRR, abs/2303.04048.\\nPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai\\nLin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui.\\n2023b. Large language models are not fair evaluators.\\nCoRR, abs/2305.17926.\\nShufan Wang, Yixiao Song, Andrew Drozdov, Aparna\\nGarimella, Varun Manjunatha, and Mohit Iyyer.\\n2023c. KNN-LM does not improve open-ended text\\ngeneration. CoRR, abs/2305.14625.\\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\\nBartscore: Evaluating generated text as text genera-\\ntion. In Advances in Neural Information Processing\\nSystems 34: Annual Conference on Neural Informa-\\ntion Processing Systems 2021, NeurIPS 2021, De-\\ncember 6-14, 2021, virtual, pages 27263–27277.\\nTianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei\\nFang, Luc Gaitskell, Thomas Hartvigsen, Xixin Wu,\\nDanny Fox, Helen Meng, and James R. Glass. 2023.\\nInterpretable unified language checking. CoRR,\\nabs/2304.03728.\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\\nWeinberger, and Yoav Artzi. 2020. Bertscore: Evalu-\\nating text generation with BERT. In8th International\\nConference on Learning Representations, ICLR 2020,\\nAddis Ababa, Ethiopia, April 26-30, 2020. OpenRe-\\nview.net.\\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-\\ntian M. Meyer, and Steffen Eger. 2019. MoverScore:\\nText generation evaluating with contextualized em-\\nbeddings and earth mover distance. In Proceedings\\nof the 2019 Conference on Empirical Methods in\\nNatural Language Processing and the 9th Interna-\\ntional Joint Conference on Natural Language Pro-\\ncessing (EMNLP-IJCNLP), pages 563–578, Hong\\nKong, China. Association for Computational Lin-\\nguistics.\\nA Examples from WikiEval\\nTables 2, 3 and 4 show examples from the WikiEval\\ndataset, focusing in particular on answers with high\\nand low faithfulness (Table 2), high and low answer\\nrelevance (Table 3), and high and low context rele-\\nvance (Table 4).'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8'}, page_content='Question Context Answer\\nWho directed the film Op-\\npenheimer and who stars\\nas J. Robert Oppenheimer\\nin the film?\\nOppenheimer is a 2023 biographical thriller film written\\nand directed by Christopher Nolan. Based on the 2005\\nbiography American Prometheus by Kai Bird and Mar-\\ntin J. Sherwin, the film chronicles the life of J. Robert\\nOppenheimer, a theoretical physicist who was pivotal in\\ndeveloping the first nuclear weapons as part of the Man-\\nhattan Project, and thereby ushering in the Atomic Age.\\nCillian Murphy stars as Oppenheimer, with Emily Blunt\\nas Oppenheimer’s wife Katherine \"Kitty\" Oppenheimer.\\nHigh Faithfulness : Christopher\\nNolan directed the film Oppen-\\nheimer. Cillian Murphy stars as J.\\nRobert Oppenheimer in the film.\\nLow Faithfulness : James\\nCameron directed the film Op-\\npenheimer. Tom Cruise stars as J.\\nRobert Oppenheimer in the film.\\nTable 2: Example from WikiEval, showing answers with high and low faithfulness.\\nQuestion Answer\\nWhen is the scheduled\\nlaunch date and time for\\nthe PSLV-C56 mission,\\nand where will it be\\nlaunched from?\\nHigh answer relevance: The PSLV-C56 mission is scheduled to be launched on Sunday, 30\\nJuly 2023 at 06:30 IST / 01:00 UTC. It will be launched from the Satish Dhawan Space\\nCentre, Sriharikota, Andhra Pradesh, India.\\nLow answer relevance: The scheduled launch date and time for the PSLV-C56 mission have\\nnot been provided.The PSLV-C56 mission is an important space mission for India. It aims to\\nlaunch a satellite into orbit to study weather patterns.\\nTable 3: Example from WikiEval, showing answers with high and low answer relevance.\\nQuestion Context\\nWhen was the Chimnabai\\nClock Tower completed,\\nand who was it named af-\\nter?\\nHigh context relevance: The Chimnabai Clock Tower, also known as the Raopura Tower, is\\na clock tower situated in the Raopura area of Vadodara, Gujarat, India. It was completed\\nin 1896 and named in memory of Chimnabai I (1864–1885), a queen and the first wife of\\nSayajirao Gaekwad III of Baroda State.\\nLow context relevance: The Chimnabai Clock Tower, also known as the Raopura Tower, is\\na clock tower situated in the Raopura area of Vadodara, Gujarat, India. It was completed\\nin 1896 and named in memory of Chimnabai I (1864–1885), a queen and the first wife of\\nSayajirao Gaekwad III of Baroda State. It was built in Indo-Saracenic architecture style.\\nHistory. Chimnabai Clock Tower was built in 1896. The tower was named after Chimnabai\\nI (1864–1885), a queen and the first wife of Sayajirao Gaekwad III of Baroda State. It was\\ninaugurated by Mir Kamaluddin Hussainkhan, the last Nawab of Baroda. During the rule of\\nGaekwad, it was a stoppage for horse drawn trams. The clock tower was erected at the cost\\nof 25,000 (equivalent to 9.2 million or USD 120,000 in 2023).\\nTable 4: Example from WikiEval, showing answers with high and low context relevance.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 0, 'page_label': '1'}, page_content='Active Retrieval Augmented Generation\\nZhengbao Jiang1∗ Frank F. Xu1∗ Luyu Gao1∗ Zhiqing Sun1∗ Qian Liu2\\nJane Dwivedi-Yu3 Yiming Yang1 Jamie Callan1 Graham Neubig1\\n1Language Technologies Institute, Carnegie Mellon University\\n2Sea AI Lab 3FAIR, Meta\\n{zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu\\nAbstract\\nDespite the remarkable ability of large lan-\\nguage models (LMs) to comprehend and gen-\\nerate language, they have a tendency to hal-\\nlucinate and create factually inaccurate out-\\nput. Augmenting LMs by retrieving informa-\\ntion from external knowledge resources is one\\npromising solution. Most existing retrieval aug-\\nmented LMs employ a retrieve-and-generate\\nsetup that only retrieves information once based\\non the input. This is limiting, however, in\\nmore general scenarios involving generation\\nof long texts, where continually gathering in-\\nformation throughout generation is essential. In\\nthis work, we provide a generalized view of ac-\\ntive retrieval augmented generation, methods\\nthat actively decide when and what to retrieve\\nacross the course of the generation. We propose\\nForward-Looking Active REtrieval augmented\\ngeneration (FLARE), a generic method which\\niteratively uses a prediction of the upcoming\\nsentence to anticipate future content, which is\\nthen utilized as a query to retrieve relevant doc-\\numents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along\\nwith baselines comprehensively over 4 long-\\nform knowledge-intensive generation tasks/-\\ndatasets. FLARE achieves superior or compet-\\nitive performance on all tasks, demonstrating\\nthe effectiveness of our method.1\\n1 Introduction\\nGenerative language models (LMs) (Brown et al.,\\n2020; Ouyang et al., 2022; OpenAI, 2023; Chowd-\\nhery et al., 2022; Zhang et al., 2022; Touvron et al.,\\n2023; Zhao et al., 2023) have become a founda-\\ntional component in natural language processing\\n(NLP) systems with their remarkable abilities. Al-\\nthough LMs have memorized some world knowl-\\nedge during training (Petroni et al., 2019; Roberts\\net al., 2020; Jiang et al., 2020), they still tend to\\n∗Lead contributors.\\n1Code and datasets are available athttps://github.com/\\njzbjyb/FLARE.\\nhallucinate and create imaginary content (Maynez\\net al., 2020; Zhou et al., 2021). Augmenting LMs\\nwith retrieval components that look up relevant in-\\nformation from external knowledge resources is a\\npromising direction to address hallucination (Khan-\\ndelwal et al., 2020; Izacard et al., 2022).\\nRetrieval augmented LMs commonly use a\\nretrieve-and-generate setup where they retrieve doc-\\numents based on the user’s input, and then generate\\na complete answer conditioning on the retrieved\\ndocuments (Chen et al., 2017; Guu et al., 2020;\\nLewis et al., 2020; Izacard and Grave, 2021; Sachan\\net al., 2021; Lee et al., 2021; Jiang et al., 2022;\\nIzacard et al., 2022; Nakano et al., 2021; Qian\\net al., 2023; Lazaridou et al., 2022; Shi et al., 2023).\\nThese single-time retrieval augmented LMs outper-\\nform purely parametric LMs, particularly for short-\\nform knowledge-intensive generation tasks such\\nas factoid question answering (QA) (Kwiatkowski\\net al., 2019; Joshi et al., 2017), where the informa-\\ntion needs are clear in the user’s input, and it is\\nsufficient to retrieve relevant knowledge once solely\\nbased on the input.\\nIncreasingly powerful large LMs have also\\ndemonstrated abilities in more complex tasks that\\ninvolve generating long-form output, such as long-\\nform QA (Fan et al., 2019; Stelmakh et al., 2022),\\nopen-domain summarization (Cohen et al., 2021;\\nHayashi et al., 2021; Giorgi et al., 2022), and\\n(chain-of-thought; CoT) reasoning (Wei et al.,\\n2022; Ho et al., 2020; Geva et al., 2021; Hendrycks\\net al., 2020). In contrast to short-form generation,\\nlong-form generation presents complex informa-\\ntion needs that are not always evident from the in-\\nput alone. Similar to how humans gradually gather\\ninformation as we create content such as papers,\\nessays, or books, long-form generation with LMs\\nwould require gathering multiple pieces of knowl-\\nedge throughout the generation process . For ex-\\nample, to generate a summary about a particular\\ntopic, the initial retrieval based on the topic name\\narXiv:2305.06983v2  [cs.CL]  22 Oct 2023'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 1, 'page_label': '2'}, page_content='Generate a summary about Joe Biden.\\nSearch results:   !![1]: …[2]: …\\nJoe Biden (born November 20, 1942) is the 46th president of the United States.Joe Biden (born November 20, 1942) is the 46th president of the United States.\\nHe graduated from the University of Delaware in 1965 with a Bachelor of Arts in history and political science.\\nJoe Biden attended the University of Pennsylvania, where he earned a law degree.\\nRetriever\\nInputStep 1\\nSearch results:   !\"![1]: …[2]: …\\n\"####\\n\"#$\\n#$\\n$\\nStep 2\\nJoe Biden announced his candidacy for the 2020 presidential election on April 25, 2019.\\nJoe Biden announced his candidacy for the 2020 presidential election on August 18, 2019.\"#%\\n#%\\nStep 3\\nSearch results:   !\"\"[1]: …[2]: …Retrieveddocuments\\nLM\\nGeneration\\n$\\n%$\\n%%\\nFigure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user\\ninput x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray\\nitalic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the\\nsystem retrieves relevant documents and regenerates the sentence.\\n(e.g., Joe Biden) may not cover all aspects and de-\\ntails. It is crucial to retrieve extra information as\\nneeded during generation, such as when generat-\\ning a certain aspect (e.g., Joe Biden’s education\\nhistory) or a specific detail (e.g., the date of Joe\\nBiden’s presidential campaign announcement).\\nSeveral attempts have been made to retrieve mul-\\ntiple times throughout generation. These attempts\\ninclude methods that passively use the past context\\nto retrieve additional information at a fixed interval\\n(Khandelwal et al., 2020; Borgeaud et al., 2022;\\nRam et al., 2023; Trivedi et al., 2022) which might\\nnot accurately reflect what LMs intend to gener-\\nate in the future or retrieve at inappropriate points.\\nSome works in multihop QA decompose the full\\nquestion into sub-questions, each of which is used\\nto retrieve extra information (Press et al., 2022; Yao\\net al., 2022; Khot et al., 2022; Khattab et al., 2022).\\nWe ask the following question: can we create a\\nsimple and generic retrieval augmented LM thatac-\\ntively decides when and what to retrievethroughout\\nthe generation process, and are applicable to a va-\\nriety of long-form generation tasks? We provide a\\ngeneralized view of active retrieval augmented gen-\\neration. Our hypothesis regarding when to retrieve\\nis that LMs should retrieve information only when\\nthey lack the required knowledge to avoid unneces-\\nsary or inappropriate retrieval that occurs in passive\\nretrieval augmented LMs (Khandelwal et al., 2020;\\nBorgeaud et al., 2022; Ram et al., 2023; Trivedi\\net al., 2022). Given the observation that large LMs\\ntend to be well-calibrated and low probability/con-\\nfidence often indicates a lack of knowledge (Ka-\\ndavath et al., 2022), we adopt an active retrieval\\nstrategy that only retrieves when LMs generate low-\\nprobability tokens. When deciding what to retrieve,\\nit is important to consider what LMs intend to gen-\\nerate in the future, as the goal of active retrieval is to\\nbenefit future generations. Therefore, we propose\\nanticipating the future by generating a temporary\\nnext sentence, using it as a query to retrieve rel-\\nevant documents, and then regenerating the next\\nsentence conditioning on the retrieved documents.\\nCombining the two aspects, we propose Forward-\\nLooking Active REtrieval augmented generation\\n(FLARE), as illustrated in Figure 1. FLARE iter-\\natively generates a temporary next sentence, use\\nit as the query to retrieve relevant documents if it\\ncontains low-probability tokens and regenerate the\\nnext sentence until reaches the end.\\nFLARE is applicable to any existing LMs at\\ninference time without additional training. Con-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 2, 'page_label': '3'}, page_content='sidering the impressive performance achieved by\\nGPT-3.5 (Ouyang et al., 2022) on a variety of\\ntasks, we examine the effectiveness of our meth-\\nods on text-davinci-003. We evaluate FLARE\\non 4 diverse tasks/datasets involving generating\\nlong outputs, including multihop QA (2WikiMul-\\ntihopQA), commonsense reasoning (StrategyQA),\\nlong-form QA (ASQA), and open-domain summa-\\nrization (WikiAsp) (Ho et al., 2020; Geva et al.,\\n2021; Stelmakh et al., 2022; Hayashi et al., 2021).\\nOver all tasks, FLARE achieves superior or com-\\npetitive performance compared to single-time and\\nmulti-time retrieval baselines, demonstrating the\\neffectiveness and generalizability of our method.\\n2 Retrieval Augmented Generation\\nWe formally define single-time retrieval augmented\\ngeneration and propose the framework of active\\nretrieval augmented generation.\\n2.1 Notations and Definitions\\nGiven a user input x and a document corpus D =\\n{di}|D|\\ni=1 (such as all Wikipedia articles), the goal of\\nretrieval augmented LMs is to generate the answer\\ny = [s1, s2, ...,sm] = [w1, w2, ..., wn] containing\\nm sentences or n tokens leveraging information\\nretrieved from the corpus.\\nIn retrieval augmented LM, the LM typically\\npairs with a retriever that can retrieve a list of\\ndocuments Dq = ret(q) for a query q; the LM\\nconditions on both the user input x and retrieved\\ndocuments Dq to generate the answer. Since we\\nfocus on examining various methods of determin-\\ning when and what to retrieve, we follow exist-\\ning methods (Ram et al., 2023; Trivedi et al.,\\n2022) to prepend the retrieved documents before\\nthe user input to aid future generation for both\\nbaselines and our method for fair comparisons:\\ny = LM([Dq, x]), where [·, ·] is concatenation fol-\\nlowing the specified order.\\n2.2 Single-time Retrieval Augmented\\nGeneration\\nThe most common choice is to directly use the user\\ninput as the query for retrieval and generate the\\ncomplete answer at once y = LM([Dx, x]).\\n2.3 Active Retrieval Augmented Generation\\nTo aid long-form generation with retrieval, we pro-\\npose active retrieval augmented generation. It is a\\ngeneric framework that actively decides when and\\nwhat to retrieve through the generation process,\\nresulting in the interleaving of retrieval and genera-\\ntion. Formally, at step t(t ≥ 1), the retrieval query\\nqt is formulated based on both the user input x and\\npreviously generated output y<t = [y0, ...,yt−1]:\\nqt = qry(x, y<t),\\nwhere qry(·) is the query formulation function. At\\nthe beginning (t = 1), the previous generation is\\nempty (y<1 = ∅), and the user input is used as the\\ninitial query (q1 = x). Given retrieved documents\\nDqt, LMs continually generate the answer until the\\nnext retrieval is triggered or reaches the end:\\nyt = LM([Dqt, x, y<t]),\\nwhere yt represents the generated tokens at the cur-\\nrent step t, and the input to LMs is the concatena-\\ntion of the retrieved documents Dqt, the user input\\nx, and the previous generation y<t. We discard\\npreviously retrieved documents ∪t′<tDqt′ and only\\nuse the retrieved documents from the current step\\nto condition the next generation to prevent reaching\\nthe input length limit of LMs.\\n3 FLARE: Forward-Looking Active\\nREtrieval Augmented Generation\\nOur intuition is that (1) LMs should only retrieve\\ninformation when they do not have the necessary\\nknowledge to avoid unnecessary or inappropriate\\nretrieval, and (2) the retrieval queries should reflect\\nthe intents of future generations. We propose two\\nforward-looking active retrieval augmented gener-\\nation (FLARE) methods to implement the active\\nretrieval augmented generation framework. The\\nfirst method prompts the LM to generate retrieval\\nqueries when necessary while generating the an-\\nswer using retrieval-encouraging instructions, de-\\nnoted as FLAREinstruct. The second method directly\\nuses the LM’s generation as search queries, denoted\\nas FLAREdirect, which iteratively generates the next\\nsentence to gain insight into the future topic, and\\nif uncertain tokens are present, retrieves relevant\\ndocuments to regenerate the next sentence.\\n3.1 FLARE with Retrieval Instructions\\nInspired by Toolformer (Schick et al., 2023), a\\nstraightforward way of expressing information\\nneeds for retrieval is to generate “[Search(query)]”\\nwhen additional information is needed (Schick\\net al., 2023), e.g., “The colors on the flag of\\nGhana have the following meanings. Red is for\\n[Search(Ghana flag red meaning)] the blood of mar-\\ntyrs, ...” When working with GPT-3.5 models that'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 3, 'page_label': '4'}, page_content='Search results:   !![1]: …[2]: …\\nJoe Biden attended\\nSearch results:   !\"![1]: …[2]: …Search results:   !\"\"[1]: …[2]: …\\n[Search(Joe Biden University)]\\n[Search(Joe Biden degree)]the University of Pennsylvania, where he earned\\na law degree.\\nGenerate a summary about Joe Biden.Input$\\n&$\\n&#%$\\n&%%%Generation\\nRetriever\\n$\\n%$\\n%%\\nFigure 2: An illustration of forward-looking active re-\\ntrieval augmented generation with retrieval instructions\\n(FLAREinstruct). It iteratively generates search queries\\n(shown in gray italic) to retrieve relevant information to\\naid future generations.\\noffer only API access, we elicit such behavior by\\nfew-shot prompting (Brown et al., 2020).\\nSpecifically, for a downstream task, we place\\nthe search-related instruction and exemplars at the\\nbeginning as skill 1, followed by the instruction and\\nexemplars of the downstream task as skill 2. Given\\na test case, we ask LMs to combine skills 1 and 2 to\\ngenerate search queries while performing the task.\\nThe structure of the prompt is shown in Prompt 3.1,\\nand full details can be found in Prompt D.3.\\nPrompt 3.1: retrieval instructions\\nSkill 1. An instruction to guide LMs to generate search\\nqueries.\\nSeveral search-related exemplars.\\nSkill 2. An instruction to guide LMs to perform a\\nspecific downstream task (e.g., multihop QA).\\nSeveral task-related exemplars.\\nAn instruction to guide LMs to combine skills 1\\nand 2 for the test case.\\nThe input of the test case.\\nAs shown in Figure 2, when the LM generates\\n“[Search(query)]” (shown in gray italic), we stop\\nthe generation and use the query terms to retrieve\\nrelevant documents, which are prepended before\\nthe user input to aid future generation until the\\nnext search query is generated or reaches the end.\\nAdditional implementation details are included in\\nAppendix A.\\n3.2 Direct FLARE\\nSince we cannot fine-tune black-box LMs, we\\nfound queries generated by FLAREinstruct through\\nretrieval instructions might not be reliable. There-\\nfore, we propose a more direct way of forward-\\nlooking active retrieval that uses the next sentence\\nto decide when and what to retrieve.\\n3.2.1 Confidence-based Active Retrieval\\nAs shown in Figure 1, at step t, we first generate a\\ntemporary next sentence ˆst = LM([x, y<t]) with-\\nout conditioning on retrieved documents. Then we\\ndecide whether to trigger retrieval and formulate\\nqueries based on ˆst. If the LM is confident aboutˆst,\\nwe accept it without retrieving additional informa-\\ntion; if not, we use ˆst to formulate search queries\\nqt to retrieve relevant documents, and then regen-\\nerate the next sentence st. The reason we utilize\\nsentences as the basis of our iteration is due to their\\nsignificance as semantic units that are neither too\\nshort nor too lengthy like phrases and paragraphs.\\nHowever, our approach can also utilize phrases or\\nparagraphs as the basis.\\nSince LMs tend to be well-calibrated that low\\nprobability/confidence often indicates a lack of\\nknowledge (Jiang et al., 2021; Kadavath et al.,\\n2022; Varshney et al., 2022), we actively trigger\\nretrieval if any token of ˆst has a probability lower\\nthan a threshold θ ∈ [0, 1]. θ = 0means retrieval\\nis never triggered, while θ = 1triggers retrieval\\nevery sentence.\\nyt =\\n(\\nˆst if all tokens of ˆst have probs ≥ θ\\nst = LM([Dqt, x, y<t]) otherwise\\nwhere the query qt is formulated based on ˆst.\\n3.2.2 Confidence-based Query Formulation\\nOne way to perform retrieval is to directly use the\\nnext sentence ˆst as the query qt. This shares a sim-\\nilar spirit with methods that use generated hypo-\\nthetical titles or paragraphs from LMs as retrieval\\nqueries or evidences (Gao et al., 2022; Sun et al.,\\n2022; Yu et al., 2022; Mao et al., 2021). We gen-\\neralize such techniques to long-form generation\\nwhere active information access is essential.\\nWe found retrieving with the next sentence\\nachieves significantly better results than with the\\nprevious context, as shown later in subsection 6.2.\\nHowever, it has a risk of perpetuating errors con-\\ntained in it. For example, if the LM produces the\\nsentence “Joe Biden attended the University of\\nPennsylvania” instead of the correct fact that he\\nattended the University of Delaware, using this er-\\nroneous sentence as a query might retrieve mislead-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 4, 'page_label': '5'}, page_content='Joe Biden attended the University of Pennsylvania, where he earned a law degree.\\nAsk a question to which the answer is “the University of Pennsylvania”Ask a question to which the answer is “a law degree”\\nWhat university did Joe Biden attend?What degree did Joe Biden earn?\\nimplicit query by maskingexplicit query by question generationJoe Biden attended  , where he earned  .\\nLM such as ChatGPT\\nFigure 3: Implicit and explicit query formulation. To-\\nkens with low probabilities are marked with underlines.\\ning information. We propose two simple methods\\nto overcome this issue as illustrated in Figure 3.\\nMasked sentences as implicit queries. The first\\nmethod masks out low-confidence tokens inˆst with\\nprobabilities below a threshold β ∈ [0, 1], where a\\nhigher β results in more aggressive masking. This\\nremoves potential distractions from the sentence to\\nimprove retrieval accuracy.\\nGenerated questions as explicit queries. An-\\nother method is to generate explicit questions that\\ntarget the low-confident span in ˆst. For example, if\\nthe LM is uncertain about “the University of Penn-\\nsylvania”, a question like “Which university did\\nJoe Biden attend?” can help retrieve relevant in-\\nformation. Self-ask (Press et al., 2022) achieved\\nthis by manually inserting follow-up questions\\ninto downstream task exemplars as shown later\\nin Prompt D.2, which requires task-specific annota-\\ntion efforts. Instead, we developed a universal ap-\\nproach that generates questions for low-confidence\\nspans without additional annotation. Specifically,\\nWe first extract all spans from ˆst with probabilities\\nbelow β. For each extracted span z, we prompt\\ngpt-3.5-turbo to generate a question qt,z that\\ncan be answered with the span:\\nPrompt 3.2: zero-shot question generation\\nUser input x.\\nGenerated output so far y≤t.\\nGiven the above passage, ask a question to which\\nthe answer is the term/entity/phrase “z”.\\nWe retrieve using each generated question and\\ninterleave the returned documents into a single\\nranking list to aid future generations. In summary,\\nqueries qt are formulated based on ˆst as follows:\\nqt =\\n(\\n∅ if all tokens of ˆst have probs ≥ θ\\nmask(ˆst) or qgen(ˆst) otherwise\\n3.3 Implementation Details\\nBase LM We validate our method on one of the\\nmost advanced GPT-3.5 LMs text-davinci-003\\nby iteratively querying their API.2\\nDocument corpus and retrievers. Since we fo-\\ncus on the integration of retrieval and generation,\\nwe use off-the-shelf retrievers that take queries\\nas inputs and return a list of relevant documents.\\nFor datasets that mainly rely on knowledge from\\nWikipedia, we use the Wikipedia dump from\\nKarpukhin et al. (2020) and employ BM25 (Robert-\\nson and Zaragoza, 2009) as the retriever. For\\ndatasets that rely on knowledge from the open web,\\nwe use the Bing search engine as our retriever.3\\nRetrieved document formatting. Multiple re-\\ntrieved documents are linearized according to their\\nranking and then added to the beginning of the user\\ninput using Prompt D.1.\\nOther implementation details such as sentence to-\\nkenization and efficiency are included Appendix A.\\n4 Multi-time Retrieval Baselines\\nExisting passive multi-time retrieval augmented\\nLMs can also be formulated using our framework\\n(subsection 2.3). In this section, we formally in-\\ntroduce three baseline categories based on when\\nand what to retrieve. These baselines are not exact\\nreproductions of the corresponding paper because\\nmany design choices differ which makes direct\\ncomparisons impossible. We implemented them\\nusing the same settings, with the only variation\\nbeing when and what to retrieve.\\nPrevious-window approaches trigger retrieval\\nevery l tokens, where l represents the window size.\\nGenerated tokens from the previous window are\\nused as the query:\\nqt = yt−1 (t ≥ 2),\\nyt = [w(t−1)l+1, ..., wtl].\\nSome existing methods in this category are RETRO\\n(Borgeaud et al., 2022), IC-RALM (Ram et al.,\\n2https://api.openai.com/v1/completions April 23.\\n3https://www.microsoft.com/en-us/bing/apis/\\nbing-web-search-api'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 5, 'page_label': '6'}, page_content='2023), which retrieve every few tokens, and KNN-\\nLM (Khandelwal et al., 2020), which retrieves ev-\\nery token.4 We follow Ram et al. (2023) to use a\\nwindow size of l = 16.\\nPrevious-sentence approaches trigger retrieval\\nevery sentence and use the previous sentence as the\\nquery, and IRCoT (Trivedi et al., 2022) belongs to\\nthis category:\\nqt = yt−1 (t ≥ 2),\\nyt = st.\\nQuestion decomposition approaches manually\\nannotated task-specific exemplars to guide LMs\\nto generate decomposed sub-questions while pro-\\nducing outputs. For example, self-ask (Press et al.,\\n2022), a method in this category, manually inserts\\nsub-questions in exemplars using Prompt D.2. For\\nthe test case, retrieval is triggered dynamically\\nwhenever the model generates a sub-question.\\nThe aforementioned approaches can retrieve ad-\\nditional information while generating. However,\\nthey have notable drawbacks: (1) Using previously\\ngenerated tokens as queries might not reflect what\\nLMs intend to generate in the future. (2) Retriev-\\ning information at a fixed interval can be inefficient\\nbecause it might occur at inappropriate points. (3)\\nQuestion decomposition approaches require task-\\nspecific prompt engineering, which restricts their\\ngeneralizability in new tasks.\\n5 Experimental Setup\\nWe evaluate the effectiveness of FLARE on 4 di-\\nverse knowledge-intensive tasks using few-shot in-\\ncontext learning (Radford et al., 2019; Brown et al.,\\n2020; Liu et al., 2023). We follow previous works\\n(Trivedi et al., 2022) to sub-sample at most 500\\nexamples from each dataset due to the cost of run-\\nning experiments. Datasets, metrics, and settings\\nare summarized in Table 7 of Appendix B. The\\nhyperparameters of FLARE are selected based on\\nthe development set and listed in Table 9. FLARE\\nrefers to FLAREdirect if not specifically stated.\\nMultihop QA The goal of multihop QA is to\\nanswer complex questions through information re-\\ntrieval and reasoning. We use 2WikiMultihopQA\\n(Ho et al., 2020) which contains 2-hop complex\\n4Since KNN-LM uses the contextualized representation\\ncorresponding to the current decoding position to retrieve rel-\\nevant information which encodes all previous tokens. Strictly\\nspeaking, qt should be y<t.\\nquestions sourced from Wikipedia articles that re-\\nquire composition, comparison, or inference, e.g.,\\n“Why did the founder of Versus die?” We follow\\nWang et al. (2022) to generate both the chain-of-\\nthought and the final answer. Experimental setting\\ndetails are included in Appendix B.\\nWe use regular expressions to extract the final\\nanswer from the output and compare it with the ref-\\nerence answer using exact match (EM), and token-\\nlevel F1, precision, and recall.\\nCommonsense reasoning Commonsense reason-\\ning requires world and commonsense knowledge\\nto generate answers. We use StrategyQA (Geva\\net al., 2021) which is a collection of crowdsourced\\nyes/no questions, e.g., “Would a pear sink in wa-\\nter?” We follow Wei et al. (2022) to generate both\\nthe chain-of-thought and the final yes/no answer.\\nDetails are included in Appendix B.\\nWe extract the final answer and match it against\\nthe gold answer using exact match.\\nLong-form QA Long-form QA aims to generate\\ncomprehensive answers to questions seeking com-\\nplex information (Fan et al., 2019; Stelmakh et al.,\\n2022). We use ASQA (Stelmakh et al., 2022) as our\\ntestbed where inputs are ambiguous questions with\\nmultiple interpretations, and outputs should cover\\nall of them. For example, “Where do the Philadel-\\nphia Eagles play their home games?” could be\\nasking about the city, sports complex, or stadium.\\nWe found in many cases it is challenging even for\\nhumans to identify which aspect of the question\\nis ambiguous. Therefore, we created another set-\\nting (ASQA-hint) where we provide a brief hint\\nto guide LMs to stay on track when generating an-\\nswers. The hint for the above case is “This question\\nis ambiguous in terms of which specific location or\\nvenue is being referred to.” Experimental setting\\ndetails are included in Appendix B.\\nWe use metrics from Stelmakh et al. (2022), in-\\ncluding EM, RoBERTa-based QA score (Disambig-\\nF1), ROUGE (Lin, 2004), and an overall score com-\\nbining Disambig-F1 and ROUGE (DR).\\nOpen-domain summarization The goal of open-\\ndomain summarization is to generate a comprehen-\\nsive summary about a topic by gathering informa-\\ntion from open web (Giorgi et al., 2022). We use\\nWikiAsp (Hayashi et al., 2021) which aims to gen-\\nerate aspect-based summaries about entities from\\n20 domains in Wikipedia, e.g., “Generate a sum-\\nmary about Echo School (Oregon) including the'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 6, 'page_label': '7'}, page_content='0.0\\n20.0\\n40.0\\n60.0\\n80.0\\n2WikiMultihopQA StrategyQA ASQA ASQA-hint WikiAsp\\nNo ret. Single-time ret. Previous-window ret. Forward-Looking Active REtrieval augmented generation (FLARE)\\nFigure 4: Comparision between FLARE and baselines across all tasks/datasets. We report the primary metric for\\neach dataset: EM for 2WikiMultihopQA, StrategyQA, and ASQA, and UniEval for WikiAsp.\\nfollowing aspects: academics, history.” Experimen-\\ntal setting details are included in Appendix B.\\nMetrics include ROUGE, named entity-based F1,\\nand UniEval (Zhong et al., 2022) which measures\\nfactual consistency.\\n6 Experimental Results\\nWe first report overall results across 4 tasks/datasets\\nand compare the performance of FLARE with all\\nthe baselines introduced in section 4. We then\\nrun ablation experiments to study the efficacy of\\nvarious design choices of our method.\\n6.1 Comparison with Baselines\\nOverall results. The overall performance of\\nFLARE and baseline across all tasks/datasets are\\nreported in Figure 4. FLARE outperforms all base-\\nline on all tasks/datasets, indicating that FLARE\\nis a generic method that can effectively retrieve\\nadditional information throughout the generation.\\nAmong various tasks, multihop QA shows the\\nmost significant improvement. This is largely due\\nto the task’s clear definition and specific objective\\nof producing the final answer through a 2-hop rea-\\nsoning process, which makes it easier for LMs to\\ngenerate on-topic output. In contrast, ASQA and\\nWikiAsp are more open-ended, which increases the\\ndifficulty of both generation and evaluation. The\\nimprovement on ASQA-hint is larger than that of\\nASQA because identifying ambiguous aspects is\\nchallenging even for humans in many cases, and\\nproviding a generic hint helps LMs to stay on topic.\\nThorough comparisons with baselines. The per-\\nformance of all baselines on 2WikiMultihopQA\\nare reported in Table 1. FLARE outperforms all\\nbaselines by a large margin, which confirms that\\nforward-looking active retrieval is highly effective.\\nMost multi-time retrieval augmented approaches\\noutperform single-time retrieval but with different\\nMethods EM F 1 Prec. Rec.\\nNo retrieval 28.2 36.8 36.5 38.6\\nSingle-time retrieval 39.4 48.8 48.6 51.5\\nMulti-time retrieval\\nPrevious-window 43.2 52.3 51.7 54.5\\nPrevious-sentence 39.0 49.2 48.9 51.8\\nQuestion decomposition 47.8 56.4 56.1 58.6\\nFLAREinstruct (ours) 42.4 49.8 49.1 52.5\\nFLAREdirect (ours) 51.0 59.7 59.1 62.6\\nTable 1: FLARE and baselines on 2WikiMultihopQA.\\nPrevious-window (Borgeaud et al., 2022; Ram et al.,\\n2023), previous-sentence (Trivedi et al., 2022), and ques-\\ntion decomposition (Press et al., 2022; Yao et al., 2022)\\nmethods are reimplemented for fair comparisons.\\nmargins. The improvement of retrieving using the\\nprevious sentence is relatively small which we hy-\\npothesize is mainly because the previous sentence\\noften describes entities or relations different from\\nthose in the next sentence in 2WikiMultihopQA.\\nWhile the previous-window approach might use\\nthe first half of a sentence to retrieve information\\npotentially helpful for generating the second half.\\nAmong all baselines, the question decomposition\\napproach (Press et al., 2022) achieves the best per-\\nformance. which is not surprising since the in-\\ncontext exemplars manually annotated with decom-\\nposed sub-questions (Prompt D.2) guide LMs to\\ngenerate sub-questions that align with the topic/in-\\ntent of future generations. FLARE outperforms\\nthis baseline, indicating that manual exemplar an-\\nnotation is not necessary for effective future-aware\\nretrieval. The gap between FLAREinstruct and ques-\\ntion decomposition is large, indicating that teaching\\nLMs to generate search queries using task-generic\\nretrieval instructions and exemplars is challenging.\\nWe report all metrics for the other datasets in\\nTable 2. FLARE outperforms baselines with re-\\nspect to all metrics. Retrieval using the previ-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 7, 'page_label': '8'}, page_content='Datasets StrategyQA ASQA ASQA-hint WikiAsp\\nMetrics EM EM D-F 1 R-L DR EM D-F 1 R-L DR UniEval E-F1 R-L\\nNo retrieval 72.9 33.8 24.2 33.3 28.4 40.1 32.5 36.4 34.4 47.1 14.1 26.4\\nSingle-time retrieval 68.6 40.0 27.1 34.0 30.4 43.2 34.8 37.4 36.0 52.4 17.4 26.9\\nMulti-time retrieval\\nPrevious-window 71.2 39.9 27.0 34.3 30.4 43.7 35.7 37.5 36.6 51.8 18.1 27.3\\nPrevious-sentence 71.0 39.9 27.9 34.3 30.9 44.7 35.9 37.5 36.7 52.6 17.8 27.2\\nFLARE (ours) 77.3 41.3 28.2 34.3 31.1 46.2 36.7 37.7 37.2 53.4 18.9 27.6\\nTable 2: Comparison between FLARE and baselines on StrategyQA, ASQA, ASQA-hint, and WikiAsp. D-F1 is\\nDisambig-F1, R-L is ROUGE-L, and E-F1 is named entity-based F1.\\n2WikiMultihopQA ASQA-hint\\nEM F 1 Prec. Rec. EM D-F 1 R-L DR\\nPrevious 39.0 49.2 48.9 51.8 42.5 34.1 36.9 35.5\\nNext 48.8 57.6 57.1 60.5 45.9 35.7 37.5 36.6\\nTable 3: A head-to-head comparison between using the\\nprevious sentence and the next sentence for retrieval.\\n#Tokens EM F 1 Prec. Rec.\\n16 43.2 52.3 51.7 54.5\\n32 43.6 52.4 52.0 55.0\\n48 40.0 49.3 49.0 52.0\\nAll 39.0 48.5 48.2 51.1\\nTable 4: Previous-window approaches using different\\nnumbers of tokens as queries.\\nous window underperforms single-time retrieval\\non ASQA, which we hypothesize is because the\\nprevious window does not accurately reflect future\\nintent. Since we focus on evaluating factuality, met-\\nrics with an emphasis on factual content (such as\\nEM, Disambig-F1, UniEval) are more reliable than\\nmetrics computed over all tokens (ROUGE-L).\\n6.2 Ablation Study\\nImportance of forward-looking retrieval. We\\nfirst validate that forward-looking retrieval is more\\neffective than past-context-based retrieval. We run\\nablation experiments on 2WikiMultihopQA and\\nASQA-hint comparing retrieval using the previ-\\nous versus the next sentence. Specifically, both\\nmethods retrieve every sentence and directly use\\nthe complete previous/next sentence as queries. As\\nshown in Table 3, using the next sentence to retrieve\\nis clearly better than using the previous sentence,\\nconfirming our hypothesis.\\nWe also run previous-window approaches using\\ndifferent numbers of past tokens as queries. As\\nshown in Table 4, using too many tokens (> 32) in\\n%steps/sentences with retrieval\\n0.0\\n20.0\\n40.0\\n60.0\\n80.0\\n0.0 25.0 50.0 75.0 100.0\\n2WikiMultihopQA StrategyQA\\nFigure 5: Performance (EM) of FLARE with respect\\nto the percentage of steps/sentences with retrieval on\\n2WikiMultihopQA and StrategyQA.\\nthe past hurts the performance, further confirming\\nour hypothesis that previous context might not be\\nrelevant to intent of future generations.\\nImportance of active retrieval. Next, we inves-\\ntigate how active retrieval threshold θ affects per-\\nformance. To alter our method from not retrieving\\nto retrieving every sentence, we adjust the confi-\\ndence threshold θ that determines when to trigger\\nretrieval from 0 to 1. We then calculate the pro-\\nportion of steps/sentences where retrieval is acti-\\nvated, and present the performance based on it. As\\nshown in Figure 5, on 2WikiMultihopQA, the per-\\nformance plateaus when the retrieval percentage\\nexceeds 60%, indicating that retrieval when LMs\\nare confident is not necessary. On StrategyQA, the\\nperformance drops when the retrieval percentage\\nexceeds 50%, indicating that unnecessary retrieval\\ncan introduce noise and impede the original gen-\\neration process. We found triggering retrieval for\\n40%-80% of sentences usually leads to a good per-\\nformance across tasks/datasets.\\nEffectiveness of different query formulation\\nmethods We study implicit query formation by\\nmasking and explicit query formulation through\\nquestion generation. In Table 5, we compare the\\nperformance of FLARE with different masking'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 8, 'page_label': '9'}, page_content='β EM F 1 Prec. Rec.\\n0.0 0.488 0.576 0.571 0.605\\n0.2 0.498 0.588 0.582 0.616\\n0.4 0.510 0.597 0.591 0.627\\n0.6 0.506 0.593 0.586 0.622\\nTable 5: Performance of FLARE with respect to the\\nmasking threshold β on 2WikiMultihopQA.\\nASQA-hint WikiAsp\\nEM D-F 1 R-L DR UniEval E-F 1 R-L\\nImplicit 45.7 36.9 37.7 37.3 53.4 18.8 27.7\\nExplicit 46.2 36.7 37.7 37.2 53.4 18.9 27.6\\nTable 6: A comparison between implicit and explicit\\nquery formulation methods in FLARE.\\nthresholds β. Retrieving directly with the complete\\nsentence (β = 0) is worse than masking tokens\\nwith low probabilities, confirming our hypothesis\\nthat low-confidence erroneous tokens can distract\\nretrievers. We compare implicit and explicit query\\nformulation methods in Table 6. Performances of\\nboth methods are similar, indicating that both meth-\\nods can effectively reflect information needs.\\n7 Related Work\\nWe refer to subsection 2.2 and section 4 for ex-\\ntensively discussion on single-time and multi-time\\nretrieval augmented LMs, which is the most rele-\\nvant area to this paper.\\nIterative and adaptive retrieval Iterative re-\\ntrieval and refinement has been studied in both\\ntext and code generation tasks (Peng et al., 2023;\\nZhang et al., 2023; Zemlyanskiy et al., 2022; Yu\\net al., 2023). FLARE differs from these methods in\\nthe granularity of generation and retrieval strategies.\\nAdaptive retrieval has been studied in single-time\\nretrieval scenarios based on either question pop-\\nularity or generation probabilities (Mallen et al.,\\n2022; Li et al., 2023), while we focus on long-form\\ngeneration requiring active information access.\\nBrowser-enhanced LMs WebGPT (Nakano\\net al., 2021) and WebCPM (Qin et al., 2023) train\\nLMs to interact with browser to enhance factuality\\nusing reinforcement learning or supervised train-\\ning where multiple queries can be triggered before\\ngeneration. FLARE is built on text-based retrievers\\nbut can be combined with a browser to potentially\\nimprove retrieval quality.\\n8 Conclusion\\nTo aid long-form generation with retrieval aug-\\nmentation, we propose an active retrieval aug-\\nmented generation framework that decides when\\nand what to retrieve during generation. We imple-\\nment this framework with forward-looking active\\nretrieval that iteratively uses the upcoming sentence\\nto retrieve relevant information if it contains low-\\nconfidence tokens and regenerates the next sen-\\ntence. Experimental results on 4 tasks/datasets\\ndemonstrate the effectiveness of our methods. Fu-\\nture directions include better strategies for active\\nretrieval and developing efficient LM architectures\\nfor active information integration.\\n9 Limitations\\nWe also conduct experiments on Wizard of\\nWikipedia (Dinan et al., 2019) and ELI5 (Fan et al.,\\n2019), and found that FLARE did not provide sig-\\nnificant gains. Wizard of Wikipedia is a knowledge-\\nintensive dialogue generation dataset where the out-\\nput is relatively short (∼20 tokens on average) so\\nretrieving multiple disparate pieces of information\\nmight not be necessary. ELI5 (Fan et al., 2019)\\nis a long-form QA dataset requiring in-depth an-\\nswers to open-ended questions. Due to issues men-\\ntioned in Krishna et al. (2021) such as difficulties\\nof grounding generation in retrieval and evalua-\\ntion, both single-time retrieval and FLARE did not\\nprovide significant gains over not using retrieval.\\nFrom an engineering perspective, interleaving gen-\\neration and retrieval with a naive implementation\\nincreases both overheads and the cost of generation.\\nLMs need to be activated multiple times (once for\\neach retrieval) and a caching-free implementation\\nalso requires recomputing the previous activation\\neach time after retrieval. This issue can be poten-\\ntially alleviated with special architectural designs\\nthat encode the retrieved documents Dqt and the\\ninput/generation (x/y<t) independently.\\nAcknowledgements\\nThis work was supported in part by a grant from\\nthe Singapore Defence Science and Technology\\nAgency and the IBM PhD Fellowship. We thank\\nChunting Zhou, Amanda Bertsch, Uri Alon, Hi-\\nroaki Hayashi, Harsh Trivedi, Patrick Lewis, Timo\\nSchick, Kaixin Ma, Shuyan Zhou, and Songwei Ge\\nfor their insightful discussions and help with the\\nexperiments.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 9, 'page_label': '10'}, page_content='References\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\\nTrevor Cai, Eliza Rutherford, Katie Millican, George\\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia\\nGuy, Jacob Menick, Roman Ring, Tom Hennigan,\\nSaffron Huang, Loren Maggiore, Chris Jones, Albin\\nCassirer, Andy Brock, Michela Paganini, Geoffrey\\nIrving, Oriol Vinyals, Simon Osindero, Karen Si-\\nmonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.\\n2022. Improving language models by retrieving from\\ntrillions of tokens. In International Conference on\\nMachine Learning, ICML 2022, 17-23 July 2022, Bal-\\ntimore, Maryland, USA, volume 162 of Proceedings\\nof Machine Learning Research , pages 2206–2240.\\nPMLR.\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\\nGretchen Krueger, Tom Henighan, Rewon Child,\\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\\nClemens Winter, Christopher Hesse, Mark Chen, Eric\\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\\nJack Clark, Christopher Berner, Sam McCandlish,\\nAlec Radford, Ilya Sutskever, and Dario Amodei.\\n2020. Language models are few-shot learners. In Ad-\\nvances in Neural Information Processing Systems 33:\\nAnnual Conference on Neural Information Process-\\ning Systems 2020, NeurIPS 2020, December 6-12,\\n2020, virtual.\\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\\nBordes. 2017. Reading wikipedia to answer open-\\ndomain questions. In Proceedings of the 55th Annual\\nMeeting of the Association for Computational Lin-\\nguistics, ACL 2017, Vancouver, Canada, July 30 -\\nAugust 4, Volume 1: Long Papers, pages 1870–1879.\\nAssociation for Computational Linguistics.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\\nPaul Barham, Hyung Won Chung, Charles Sutton,\\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\\nHutchinson, Reiner Pope, James Bradbury, Jacob\\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\\nRewon Child, Oleksandr Polozov, Katherine Lee,\\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\\nand Noah Fiedel. 2022. Palm: Scaling language mod-\\neling with pathways. CoRR, abs/2204.02311.\\nNachshon Cohen, Oren Kalinsky, Yftah Ziser, and\\nAlessandro Moschitti. 2021. Wikisum: Coherent\\nsummarization dataset for efficient human-evaluation.\\nIn Proceedings of the 59th Annual Meeting of the As-\\nsociation for Computational Linguistics and the 11th\\nInternational Joint Conference on Natural Language\\nProcessing, ACL/IJCNLP 2021, (Volume 2: Short\\nPapers), Virtual Event, August 1-6, 2021, pages 212–\\n219. Association for Computational Linguistics.\\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\\nFan, Michael Auli, and Jason Weston. 2019. Wizard\\nof wikipedia: Knowledge-powered conversational\\nagents. In 7th International Conference on Learning\\nRepresentations, ICLR 2019, New Orleans, LA, USA,\\nMay 6-9, 2019. OpenReview.net.\\nAngela Fan, Yacine Jernite, Ethan Perez, David Grang-\\nier, Jason Weston, and Michael Auli. 2019. ELI5:\\nlong form question answering. In Proceedings of\\nthe 57th Conference of the Association for Compu-\\ntational Linguistics, ACL 2019, Florence, Italy, July\\n28- August 2, 2019, Volume 1: Long Papers, pages\\n3558–3567. Association for Computational Linguis-\\ntics.\\nLuyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan.\\n2022. Precise zero-shot dense retrieval without rele-\\nvance labels. CoRR, abs/2212.10496.\\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\\nDan Roth, and Jonathan Berant. 2021. Did aristotle\\nuse a laptop? a question answering benchmark with\\nimplicit reasoning strategies. Transactions of the\\nAssociation for Computational Linguistics , 9:346–\\n361.\\nJohn M. Giorgi, Luca Soldaini, Bo Wang, Gary D.\\nBader, Kyle Lo, Lucy Lu Wang, and Arman Co-\\nhan. 2022. Exploring the challenges of open\\ndomain multi-document summarization. CoRR,\\nabs/2212.10526.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\\npat, and Ming-Wei Chang. 2020. REALM: retrieval-\\naugmented language model pre-training. CoRR,\\nabs/2002.08909.\\nHiroaki Hayashi, Prashant Budania, Peng Wang, Chris\\nAckerson, Raj Neervannan, and Graham Neubig.\\n2021. Wikiasp: A dataset for multi-domain aspect-\\nbased summarization. Trans. Assoc. Comput. Lin-\\nguistics, 9:211–225.\\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\\n2020. Measuring massive multitask language under-\\nstanding. CoRR, abs/2009.03300.\\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,\\nand Akiko Aizawa. 2020. Constructing A multi-hop\\nQA dataset for comprehensive evaluation of reason-\\ning steps. In Proceedings of the 28th International\\nConference on Computational Linguistics, COLING\\n2020, Barcelona, Spain (Online), December 8-13,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 10, 'page_label': '11'}, page_content='2020, pages 6609–6625. International Committee on\\nComputational Linguistics.\\nGautier Izacard and Edouard Grave. 2021. Leveraging\\npassage retrieval with generative models for open do-\\nmain question answering. In Proceedings of the 16th\\nConference of the European Chapter of the Associ-\\nation for Computational Linguistics: Main Volume,\\nEACL 2021, Online, April 19 - 23, 2021, pages 874–\\n880. Association for Computational Linguistics.\\nGautier Izacard, Patrick S. H. Lewis, Maria Lomeli,\\nLucas Hosseini, Fabio Petroni, Timo Schick, Jane\\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\\nEdouard Grave. 2022. Few-shot learning with\\nretrieval augmented language models. CoRR,\\nabs/2208.03299.\\nZhengbao Jiang, Jun Araki, Haibo Ding, and Graham\\nNeubig. 2021. How can we know When language\\nmodels know? on the calibration of language mod-\\nels for question answering. Trans. Assoc. Comput.\\nLinguistics, 9:962–977.\\nZhengbao Jiang, Luyu Gao, Jun Araki, Haibo Ding,\\nZhiruo Wang, Jamie Callan, and Graham Neubig.\\n2022. Retrieval as attention: End-to-end learning\\nof retrieval and reading within a single transformer.\\nCoRR, abs/2212.02027.\\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\\nNeubig. 2020. How can we know what language\\nmodels know. Trans. Assoc. Comput. Linguistics ,\\n8:423–438.\\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In Proceedings of the 55th Annual Meeting of\\nthe Association for Computational Linguistics, ACL\\n2017, Vancouver, Canada, July 30 - August 4, Volume\\n1: Long Papers, pages 1601–1611. Association for\\nComputational Linguistics.\\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\\nHenighan, Dawn Drain, Ethan Perez, Nicholas\\nSchiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli\\nTran-Johnson, Scott Johnston, Sheer El Showk, Andy\\nJones, Nelson Elhage, Tristan Hume, Anna Chen,\\nYuntao Bai, Sam Bowman, Stanislav Fort, Deep\\nGanguli, Danny Hernandez, Josh Jacobson, Jack-\\nson Kernion, Shauna Kravec, Liane Lovitt, Ka-\\nmal Ndousse, Catherine Olsson, Sam Ringer, Dario\\nAmodei, Tom Brown, Jack Clark, Nicholas Joseph,\\nBen Mann, Sam McCandlish, Chris Olah, and Jared\\nKaplan. 2022. Language models (mostly) know what\\nthey know. CoRR, abs/2207.05221.\\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\\nS. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,\\nand Wen-tau Yih. 2020. Dense passage retrieval for\\nopen-domain question answering. In Proceedings of\\nthe 2020 Conference on Empirical Methods in Nat-\\nural Language Processing, EMNLP 2020, Online,\\nNovember 16-20, 2020, pages 6769–6781. Associa-\\ntion for Computational Linguistics.\\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\\nZettlemoyer, and Mike Lewis. 2020. Generalization\\nthrough memorization: Nearest neighbor language\\nmodels. In 8th International Conference on Learning\\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia,\\nApril 26-30, 2020. OpenReview.net.\\nOmar Khattab, Keshav Santhanam, Xiang Lisa Li,\\nDavid Hall, Percy Liang, Christopher Potts, and\\nMatei Zaharia. 2022. Demonstrate-search-predict:\\nComposing retrieval and language models for\\nknowledge-intensive NLP. CoRR, abs/2212.14024.\\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu,\\nKyle Richardson, Peter Clark, and Ashish Sabharwal.\\n2022. Decomposed prompting: A modular approach\\nfor solving complex tasks. CoRR, abs/2210.02406.\\nKalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021.\\nHurdles to progress in long-form question answering.\\nIn North American Association for Computational\\nLinguistics.\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\\nfield, Michael Collins, Ankur P. Parikh, Chris Alberti,\\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\\nton Lee, Kristina Toutanova, Llion Jones, Matthew\\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\\nral questions: a benchmark for question answering\\nresearch. Trans. Assoc. Comput. Linguistics, 7:452–\\n466.\\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech\\nStokowiec, and Nikolai Grigorev. 2022. Internet-\\naugmented language models through few-shot\\nprompting for open-domain question answering.\\nCoRR, abs/2203.05115.\\nHaejun Lee, Akhil Kedia, Jongwon Lee, Ashwin Paran-\\njape, Christopher D. Manning, and Kyoung-Gu Woo.\\n2021. You only need one model for open-domain\\nquestion answering. CoRR, abs/2112.07381.\\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,\\nTim Rocktäschel, Sebastian Riedel, and Douwe\\nKiela. 2020. Retrieval-augmented generation for\\nknowledge-intensive NLP tasks. In Advances in Neu-\\nral Information Processing Systems 33: Annual Con-\\nference on Neural Information Processing Systems\\n2020, NeurIPS 2020, December 6-12, 2020, virtual.\\nJunyi Li, Tianyi Tang, Wayne Xin Zhao, Jingyuan Wang,\\nJian-Yun Nie, and Ji-Rong Wen. 2023. The web can\\nbe your oyster for improving large language models.\\nCoRR, abs/2305.10998.\\nChin-Yew Lin. 2004. ROUGE: A package for auto-\\nmatic evaluation of summaries. In Text Summariza-\\ntion Branches Out, pages 74–81, Barcelona, Spain.\\nAssociation for Computational Linguistics.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 11, 'page_label': '12'}, page_content='Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\\ntrain, prompt, and predict: A systematic survey of\\nprompting methods in natural language processing.\\nACM Comput. Surv., 55(9):195:1–195:35.\\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\\nHannaneh Hajishirzi, and Daniel Khashabi. 2022.\\nWhen not to trust language models: Investigating\\neffectiveness and limitations of parametric and non-\\nparametric memories. CoRR, abs/2212.10511.\\nYuning Mao, Pengcheng He, Xiaodong Liu, Yelong\\nShen, Jianfeng Gao, Jiawei Han, and Weizhu Chen.\\n2021. Generation-augmented retrieval for open-\\ndomain question answering. In Proceedings of the\\n59th Annual Meeting of the Association for Compu-\\ntational Linguistics and the 11th International Joint\\nConference on Natural Language Processing, ACL/I-\\nJCNLP 2021, (Volume 1: Long Papers), Virtual Event,\\nAugust 1-6, 2021, pages 4089–4100. Association for\\nComputational Linguistics.\\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\\nRyan McDonald. 2020. On faithfulness and factu-\\nality in abstractive summarization. In Proceedings\\nof the 58th Annual Meeting of the Association for\\nComputational Linguistics, pages 1906–1919, On-\\nline. Association for Computational Linguistics.\\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\\nLong Ouyang, Christina Kim, Christopher Hesse,\\nShantanu Jain, Vineet Kosaraju, William Saunders,\\nXu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen\\nKrueger, Kevin Button, Matthew Knight, Benjamin\\nChess, and John Schulman. 2021. Webgpt: Browser-\\nassisted question-answering with human feedback.\\nCoRR, abs/2112.09332.\\nOpenAI. 2023. GPT-4 technical report. CoRR,\\nabs/2303.08774.\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\\nSandhini Agarwal, Katarina Slama, Alex Ray, John\\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\\nMaddie Simens, Amanda Askell, Peter Welinder,\\nPaul F. Christiano, Jan Leike, and Ryan Lowe. 2022.\\nTraining language models to follow instructions with\\nhuman feedback. CoRR, abs/2203.02155.\\nBaolin Peng, Michel Galley, Pengcheng He, Hao Cheng,\\nYujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou\\nYu, Weizhu Chen, and Jianfeng Gao. 2023. Check\\nyour facts and try again: Improving large language\\nmodels with external knowledge and automated feed-\\nback. CoRR, abs/2302.12813.\\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\\nPatrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu,\\nand Alexander H. Miller. 2019. Language mod-\\nels as knowledge bases? In Proceedings of the\\n2019 Conference on Empirical Methods in Natu-\\nral Language Processing and the 9th International\\nJoint Conference on Natural Language Processing,\\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\\nber 3-7, 2019 , pages 2463–2473. Association for\\nComputational Linguistics.\\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\\nNoah A Smith, and Mike Lewis. 2022. Measuring\\nand narrowing the compositionality gap in language\\nmodels. arXiv preprint arXiv:2210.03350.\\nHongjing Qian, Yutao Zhu, Zhicheng Dou, Haoqi Gu,\\nXinyu Zhang, Zheng Liu, Ruofei Lai, Zhao Cao,\\nJian-Yun Nie, and Ji-Rong Wen. 2023. Webbrain:\\nLearning to generate factually correct articles for\\nqueries by grounding on large web corpus. CoRR,\\nabs/2304.04358.\\nYujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao\\nLiang, Kunlun Zhu, Yankai Lin, Xu Han, Ning Ding,\\nHuadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan\\nLiu, Maosong Sun, and Jie Zhou. 2023. Webcpm: In-\\nteractive web search for chinese long-form question\\nanswering. CoRR, abs/2305.06849.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\\nDario Amodei, and Ilya Sutskever. 2019. Language\\nmodels are unsupervised multitask learners. OpenAI\\nBlog, 1(8).\\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\\nShoham. 2023. In-context retrieval-augmented lan-\\nguage models. arXiv preprint arXiv:2302.00083.\\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\\nHow much knowledge can you pack into the param-\\neters of a language model? In Proceedings of the\\n2020 Conference on Empirical Methods in Natural\\nLanguage Processing, EMNLP 2020, Online, Novem-\\nber 16-20, 2020, pages 5418–5426. Association for\\nComputational Linguistics.\\nStephen E. Robertson and Hugo Zaragoza. 2009. The\\nprobabilistic relevance framework: BM25 and be-\\nyond. Found. Trends Inf. Retr., 3(4):333–389.\\nDevendra Singh Sachan, Siva Reddy, William L. Hamil-\\nton, Chris Dyer, and Dani Yogatama. 2021. End-to-\\nend training of multi-document reader and retriever\\nfor open-domain question answering. In Advances\\nin Neural Information Processing Systems 34: An-\\nnual Conference on Neural Information Processing\\nSystems 2021, NeurIPS 2021, December 6-14, 2021,\\nvirtual, pages 25968–25981.\\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\\nCancedda, and Thomas Scialom. 2023. Toolformer:\\nLanguage models can teach themselves to use tools.\\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon\\nSeo, Rich James, Mike Lewis, Luke Zettlemoyer, and\\nWen-tau Yih. 2023. REPLUG: retrieval-augmented\\nblack-box language models. CoRR, abs/2301.12652.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 12, 'page_label': '13'}, page_content='Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-\\nWei Chang. 2022. ASQA: factoid questions meet\\nlong-form answers. In Proceedings of the 2022 Con-\\nference on Empirical Methods in Natural Language\\nProcessing, EMNLP 2022, Abu Dhabi, United Arab\\nEmirates, December 7-11, 2022, pages 8273–8288.\\nAssociation for Computational Linguistics.\\nZhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and\\nDenny Zhou. 2022. Recitation-augmented language\\nmodels. CoRR, abs/2210.01296.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\\nAzhar, Aurélien Rodriguez, Armand Joulin, Edouard\\nGrave, and Guillaume Lample. 2023. Llama: Open\\nand efficient foundation language models. CoRR,\\nabs/2302.13971.\\nHarsh Trivedi, Niranjan Balasubramanian, Tushar\\nKhot, and Ashish Sabharwal. 2022. Interleav-\\ning retrieval with chain-of-thought reasoning for\\nknowledge-intensive multi-step questions. CoRR,\\nabs/2212.10509.\\nNeeraj Varshney, Man Luo, and Chitta Baral. 2022. Can\\nopen-domain QA reader utilize external knowledge\\nefficiently like humans? CoRR, abs/2211.12707.\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V .\\nLe, Ed H. Chi, and Denny Zhou. 2022. Self-\\nconsistency improves chain of thought reasoning in\\nlanguage models. CoRR, abs/2203.11171.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022.\\nChain of thought prompting elicits reasoning in large\\nlanguage models. CoRR, abs/2201.11903.\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\\nReact: Synergizing reasoning and acting in language\\nmodels. CoRR, abs/2210.03629.\\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,\\nMingxuan Ju, Soumya Sanyal, Chenguang Zhu,\\nMichael Zeng, and Meng Jiang. 2022. Generate\\nrather than retrieve: Large language models are\\nstrong context generators. CoRR, abs/2209.10063.\\nWenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng\\nJiang, and Ashish Sabharwal. 2023. Improving lan-\\nguage models via plug-and-play retrieval feedback.\\nCoRR, abs/2305.14002.\\nYury Zemlyanskiy, Michiel de Jong, Joshua Ainslie,\\nPanupong Pasupat, Peter Shaw, Linlu Qiu, Sumit\\nSanghai, and Fei Sha. 2022. Generate-and-retrieve:\\nUse your predictions to improve retrieval for seman-\\ntic parsing. In Proceedings of the 29th International\\nConference on Computational Linguistics, COLING\\n2022, Gyeongju, Republic of Korea, October 12-17,\\n2022, pages 4946–4951. International Committee on\\nComputational Linguistics.\\nFengji Zhang, Bei Chen, Yue Zhang, Jin Liu, Daoguang\\nZan, Yi Mao, Jian-Guang Lou, and Weizhu Chen.\\n2023. Repocoder: Repository-level code completion\\nthrough iterative retrieval and generation. CoRR,\\nabs/2303.12570.\\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\\nWang, and Luke Zettlemoyer. 2022. Opt: Open\\npre-trained transformer language models. ArXiv,\\nabs/2205.01068.\\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\\nXiaolei Wang, Yupeng Hou, Yingqian Min, Be-\\nichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,\\nChen Yang, Yushuo Chen, Zhipeng Chen, Jinhao\\nJiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang\\nLiu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.\\n2023. A survey of large language models. CoRR,\\nabs/2303.18223.\\nMing Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu\\nJiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and\\nJiawei Han. 2022. Towards a unified multi-\\ndimensional evaluator for text generation. In Pro-\\nceedings of the 2022 Conference on Empirical Meth-\\nods in Natural Language Processing, EMNLP 2022,\\nAbu Dhabi, United Arab Emirates, December 7-11,\\n2022, pages 2023–2038. Association for Computa-\\ntional Linguistics.\\nChunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab,\\nFrancisco Guzmán, Luke Zettlemoyer, and Marjan\\nGhazvininejad. 2021. Detecting hallucinated content\\nin conditional neural sequence generation. In Find-\\nings of the Association for Computational Linguis-\\ntics: ACL-IJCNLP 2021, pages 1393–1404, Online.\\nAssociation for Computational Linguistics.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 13, 'page_label': '14'}, page_content='A FLARE Implementation Details\\nFLAREinstruct implementation details We\\nfound that LMs can effectively combine retrieval\\nand downstream task-related skills and generate\\nmeaningful search queries while performing the\\ntask. However, there are two issues: (1) LMs tend\\nto generate fewer search queries than necessary.\\n(2) Generating excessive search queries can\\ndisrupt answer generation and adversely affect\\nperformance. We address these issues using two\\nmethods respectively. First, we increase the logit\\nof the token “[” by 2.0 to improve the chances\\nof LMs generating “[Search(query)]”. Second,\\nwhenever LMs generate a search query, we use it\\nto retrieve relevant information, promptly remove\\nit from the generation, and generate the next few\\ntokens while forbidding “[” by adding a large\\nnegative value to the logit of “[”.\\nThe initial query of FLARE. FLARE starts\\nwith the user input x as the initial query to re-\\ntrieve documents to generate the first sentence\\nˆs1 = LM([Dx, x]) to bootstrap the iterative gener-\\nation process. For the following steps, the tempo-\\nrary forward-looking sentence is generated without\\nretrieved documents.\\nSentence tokenization. For each step t, we gen-\\nerate 64 tokens which are longer than most sen-\\ntences, and use NLTK sentence tokenizer 5 to ex-\\ntract the first sentence and discard the rest.\\nEfficiency As shown in subsection 6.2, on aver-\\nage retrieval is triggered for 30% ∼ 60% of sen-\\ntences depending on downstream tasks. In compar-\\nision, KNN-LM (Khandelwal et al., 2020) retrieves\\nevery token, RETRO or IC-RALM (Borgeaud et al.,\\n2022; Ram et al., 2023) retrievers every 4∼32 to-\\nkens, and IRCoT (Trivedi et al., 2022) retrieves\\nevery sentence. Compared to single-time retrieval,\\nhowever, interleaving retrieval and generation with\\na naive implementation indeed increases overheads,\\nwhich we discuss in the limitation section (sec-\\ntion 9).\\nB Datasets and Settings\\nDatasets, metrics, and experimental settings are\\nsummarized in Table 7.\\n5https://www.nltk.org/api/nltk.tokenize.\\nPunktSentenceTokenizer.html\\nMultihop QA For “Why did the founder of Ver-\\nsus die?”, the output we aim to generate is “The\\nfounder of Versus was Gianni Versace. Gianni Ver-\\nsace was shot and killed on the steps of his Miami\\nBeach mansion on July 15, 1997. So the answer\\nis shot.” We use 8 exemplars from Trivedi et al.\\n(2022) listed in Prompt D.4 for in-context learn-\\ning, BM25 as the retriever, and Wikipedia articles\\nas the retrieval corpus. Similar to the observation\\nin Trivedi et al. (2022), we found incorporating\\nretrieval results for exemplars improves the per-\\nformance, we use the input x of each exemplar to\\nretrieve several documents and then add them using\\nthe format in Prompt D.1. We found increasing the\\nnumber of retrieval documents often increases per-\\nformance. Therefore, we use the maximum number\\nof documents that can fit within the input length\\nlimit of text-davinci-003, which is 2 for 2Wiki-\\nMultihopQA.\\nCommonsense Reasoning For “Would a pear\\nsink in water?”, the output we aim to generate is\\n“The density of a pear is about 0.6g/cm3, which is\\nless than water. Objects less dense than water float.\\nThus, a pear would float. So the final answer is no.”\\nWe use 6 exemplars from Wei et al. (2022) listed in\\nPrompt D.5, BM25 on the Wikipedia corpus, and 3\\nretrieved documents to run experiments.\\nLong-form QA For “Where do the Philadelphia\\nEagles play their home games?”, the output we\\naim to generate is “We need to consider the dif-\\nferent possible locations or venues that could be\\nconsidered the home field of the Philadelphia Ea-\\ngles. These include the city, the sports complex,\\nor the stadium. Therefore, this question has 3 in-\\nterpretations and the answers are: (1) The city is\\nPhiladelphia. (2) The sports complex is the South\\nPhiladelphia Sports Complex. (3) The stadium is\\nthe Lincoln Financial Field stadium.” For both the\\noriginal setting (ASQA) and the setting with hints\\n(ASQA-hint), we manually annotate 8 exemplars\\n(Prompt D.6 and D.8), use BM25 on the Wikipedia\\ncorpus, and 3 retrieved documents to run experi-\\nments.\\nOpen-domain Summarization The original\\nWikiAsp dataset is designed for multi-document\\nsummarization and provides a list of references to\\nsystems. We converted it into the open-domain\\nsetting by removing the associated references and\\ninstead gathering information from the open web.\\nFor “Generate a summary about Echo School (Ore-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 14, 'page_label': '15'}, page_content='gon) including the following aspects: academics,\\nhistory.”, the output we aim to generate is “# Aca-\\ndemics. In 2008, 91% of the school’s seniors re-\\nceived their high school diploma... # History. The\\nclass of 2008 was the 100th class in the school’s\\nhistory.” where # is used to indicate aspects. We\\nmanually annotate 4 exemplars (Prompt D.10), and\\nuse the Bing search engine to retrieve 5 documents\\nfrom the open web. To avoid leaking, we exclude\\nseveral Wikipedia-related domains listed in Table 8\\nfrom Bing’s search results.\\nC Hyperparameters\\nHyperparameters of FLARE on different datasets\\nare listed in Table 9.\\nD Prompts and Few-shot exemplars\\nThe prompt used to linearize multiple documents\\nis shown in Prompt D.1. The prompt used in self-\\nask (Press et al., 2022) is shown in Prompt D.2.\\nPrompts and exemplars of different tasks/datasets\\nare shown in Prompt D.3, D.4, D.5, D.6, D.8, and\\nD.10, respectively.\\nPrompt D.1: document formatting\\nSearch results:\\n[1] Document 1\\n[2] Document 2\\n...\\nThe user input x\\nPrompt D.2: multihop QA with self-ask\\nQuestion: Who lived longer, Theodor Haecker or Harry\\nVaughan Watkins?\\nAre follow up questions needed here: Yes.\\nFollow up: How old was Theodor Haecker when he died?\\nIntermediate answer: Theodor Haecker was 65 years old\\nwhen he died.\\nFollow up: How old was Harry Vaughan Watkins when he\\ndied?\\nIntermediate answer: Harry Vaughan Watkins was 69 years\\nold when he died.\\nSo the final answer is: Harry Vaughan Watkins.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 15, 'page_label': '16'}, page_content='Settings 2WikiMultihopQA StrategyQA ASQA WikiAsp\\n(Ho et al., 2020) (Geva et al., 2021) (Stelmakh et al., 2022) (Hayashi et al., 2021)\\nDataset statistics\\nTask multihop QA commonsense QA long-form QA open-domain summarization\\n#Examples 500 229 500 500\\nEvaluation settings\\nMetrics EM, F 1, Prec., Rec. EM EM, Disambig-F 1, ROUGE, DR UniEval, entity-F1, ROUGE\\nRetrieval settings\\nCorpus Wikipedia Wikipedia Wikipedia open web\\nRetriever BM25 BM25 BM25 Bing\\nTop-k 2 3 3 5\\nPrompt format\\n#Exemplars 8 6 8 4\\nRet. for exemplars ✓ ✗ ✗ ✗\\nTable 7: Dataset statistics and experimental settings of different tasks.\\nwikipedia.org, wikiwand.com, wiki2.org, wikimedia.org\\nTable 8: Wikipedia-related domains excluded from Bing’s search results.\\nDataset θ β Query formulation Combine single- & multi-time retrieval\\n2WikiMultihopQA 0.8 0.4 implicit ✗\\nStrategyQA 0.4 0.4 implicit ✗\\nASQA & ASQA-hint 0.8 0.4 explicit ✓\\nWikiAsp 0.8 0.4 explicit ✓\\nTable 9: Hyperparameters of FLARE on different datasets.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 16, 'page_label': '17'}, page_content='Prompt D.3: retrieval instructions for 2WikiMultihopQA\\nSkill 1. Use the Search API to look up relevant information by writing “[Search(term)]” where “term” is the search term you\\nwant to look up. For example:\\nQuestion: But what are the risks during production of nanomaterials?\\nAnswer (with Search): [Search(nanomaterial production risks)] Some nanomaterials may give rise to various kinds of lung\\ndamage.\\nQuestion: The colors on the flag of Ghana have the following meanings.\\nAnswer (with Search): Red is for [Search(Ghana flag red meaning)] the blood of martyrs, green for forests, and gold for\\nmineral wealth.\\nQuestion: Metformin is the first-line drug for what?\\nAnswer (with Search): [Search(Metformin first-line drug)] patients with type 2 diabetes and obesity.\\nSkill 2. Answer questions by thinking step-by-step. First, write out the reasoning steps, then draw the conclu-\\nsion. For example:\\nQuestion: When did the director of film Hypocrite (Film) die?\\nAnswer (with step-by-step): The film Hypocrite was directed by Miguel Morayta. Miguel Morayta died on 19 June 2013. So\\nthe answer is 19 June 2013.\\nQuestion: Are both Kurram Garhi and Trojkrsti located in the same country?\\nAnswer (with step-by-step): Kurram Garhi is located in the country of Pakistan. Trojkrsti is located in the country of\\nRepublic of Macedonia. Thus, they are not in the same country. So the answer is no.\\nQuestion: Do director of film Coolie No. 1 (1995 Film) and director of film The Sensational Trial have the same\\nnationality?\\nAnswer (with step-by-step): Coolie No. 1 (1995 film) was directed by David Dhawan. The Sensational Trial was directed by\\nKarl Freund. David Dhawan’s nationality is India. Karl Freund’s nationality is Germany. Thus, they do not have the same\\nnationality. So the answer is no.\\nQuestion: Who is Boraqchin (Wife Of Ögedei)’s father-in-law?\\nAnswer (with step-by-step): Boraqchin is married to Ögedei Khan. Ögedei Khan’s father is Genghis Khan. Thus, Boraqchin’s\\nfather-in-law is Genghis Khan. So the answer is Genghis Khan.\\nQuestion: Who was born first out of Martin Hodge and Ivania Martinich?\\nAnswer (with step-by-step): Martin Hodge was born on 4 February 1959. Ivania Martinich was born on 25 July 1995. Thus,\\nMartin Hodge was born first. So the answer is Martin Hodge.\\nQuestion: When did the director of film Laughter In Hell die?\\nAnswer (with step-by-step): The film Laughter In Hell was directed by Edward L. Cahn. Edward L. Cahn died on August 25,\\n1963. So the answer is August 25, 1963.\\nQuestion: Which film has the director died later, The Gal Who Took the West or Twenty Plus Two?\\nAnswer (with step-by-step): The film Twenty Plus Two was directed by Joseph M. Newman. The Gal Who Took\\nthe West was directed by Frederick de Cordova. Joseph M. Newman died on January 23, 2006. Fred de Cordova\\ndied on September 15, 2001. Thus, the person to die later from the two is Twenty Plus Two. So the answer is Twenty Plus Two.\\nQuestion: Who is the grandchild of Krishna Shah (Nepalese Royal)?\\nAnswer (with step-by-step): Krishna Shah has a child named Rudra Shah. Rudra Shah has a child named Prithvipati Shah.\\nThus, Krishna Shah has a grandchild named Prithvipati Shah. So the answer is Prithvipati Shah.\\nNow, combine the aforementioned two skills. First, write out the reasoning steps, then draw the conclusion,\\nwhere the reasoning steps should also utilize the Search API “[Search(term)]” whenever possible.\\nQuestion: Where did Minbyauk Thihapate’s wife die?\\nAnswer (with step-by-step & Search):'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 17, 'page_label': '18'}, page_content='Prompt D.4: exemplars of 2WikiMultihopQA\\nQuestion: When did the director of film Hypocrite (Film) die?\\nAnswer: The film Hypocrite was directed by Miguel Morayta. Miguel Morayta died on 19 June 2013. So the answer is 19\\nJune 2013.\\nQuestion: Are both Kurram Garhi and Trojkrsti located in the same country?\\nAnswer: Kurram Garhi is located in the country of Pakistan. Trojkrsti is located in the country of Republic of Macedonia.\\nThus, they are not in the same country. So the answer is no.\\nQuestion: Do director of film Coolie No. 1 (1995 Film) and director of film The Sensational Trial have the same\\nnationality?\\nAnswer: Coolie No. 1 (1995 film) was directed by David Dhawan. The Sensational Trial was directed by Karl Freund. David\\nDhawan’s nationality is India. Karl Freund’s nationality is Germany. Thus, they do not have the same nationality. So the\\nanswer is no.\\nQuestion: Who is Boraqchin (Wife Of Ögedei)’s father-in-law?\\nAnswer: Boraqchin is married to Ögedei Khan. Ögedei Khan’s father is Genghis Khan. Thus, Boraqchin’s father-in-law is\\nGenghis Khan. So the answer is Genghis Khan.\\nQuestion: Who was born first out of Martin Hodge and Ivania Martinich?\\nAnswer: Martin Hodge was born on 4 February 1959. Ivania Martinich was born on 25 July 1995. Thus, Martin Hodge was\\nborn first. So the answer is Martin Hodge.\\nQuestion: When did the director of film Laughter In Hell die?\\nAnswer: The film Laughter In Hell was directed by Edward L. Cahn. Edward L. Cahn died on August 25, 1963. So the\\nanswer is August 25, 1963.\\nQuestion: Which film has the director died later, The Gal Who Took the West or Twenty Plus Two?\\nAnswer: The film Twenty Plus Two was directed by Joseph M. Newman. The Gal Who Took the West was directed by\\nFrederick de Cordova. Joseph M. Newman died on January 23, 2006. Fred de Cordova died on September 15, 2001. Thus,\\nthe person to die later from the two is Twenty Plus Two. So the answer is Twenty Plus Two.\\nQuestion: Who is the grandchild of Krishna Shah (Nepalese Royal)?\\nAnswer: Krishna Shah has a child named Rudra Shah. Rudra Shah has a child named Prithvipati Shah. Thus, Krishna Shah\\nhas a grandchild named Prithvipati Shah. So the answer is Prithvipati Shah.\\nQuestion: Which country the director of film Citizen Mavzik is from?\\nAnswer:'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 18, 'page_label': '19'}, page_content='Prompt D.5: exemplars of StrategyQA\\nGenerate a yes or no answer to the following question.\\nQuestion: Do hamsters provide food for any animals?\\nAnswer: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So the final\\nanswer is yes.\\nGenerate a yes or no answer to the following question.\\nQuestion: Could Brooke Shields succeed at University of Pennsylvania?\\nAnswer: Brooke Shields went to Princeton University. Princeton University is about as academically rigorous as the Uni-\\nversity of Pennsylvania. Thus, Brooke Shields could also succeed at the University of Pennsylvania. So the final answer is yes.\\nGenerate a yes or no answer to the following question.\\nQuestion: Hydrogen’s atomic number squared exceeds number of Spice Girls?\\nAnswer: Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen’s atomic number\\nsquared is less than 5. So the final answer is no.\\nGenerate a yes or no answer to the following question.\\nQuestion: Is it common to see frost during some college commencements?\\nAnswer: College commencement ceremonies can happen in December, May, and June. December is in the winter, so there\\ncan be frost. Thus, there could be frost at some commencements. So the final answer is yes.\\nGenerate a yes or no answer to the following question.\\nQuestion: Could a llama birth twice during War in Vietnam (1945-46)?\\nAnswer: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6 months.\\nThus, a llama could not give birth twice during the War in Vietnam. So the final answer is no.\\nGenerate a yes or no answer to the following question.\\nQuestion: Would a pear sink in water?\\nAnswer: The density of a pear is about 0.6g/cm3, which is less than water. Objects less dense than water float. Thus, a pear\\nwould float. So the final answer is no.\\nGenerate a yes or no answer to the following question.\\nQuestion: Did Spiderman fight against Falcon in the MCU?\\nAnswer:'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 19, 'page_label': '20'}, page_content='Prompt D.6: exemplars of ASQA\\nGiven an ambiguous question, figure out its interpretations and answer them one by one.\\nQuestion: Who played bonnie in gone with the wind?\\nAnswer: This question is ambiguous in terms of which version or adaptation of Gone with the Wind is being referred to. In\\norder to figure out its interpretations, we need to consider different versions or adaptations of Gone with the Wind. Gone with\\nthe Wind has two versions or adaptations: the 1939 film Gone with the Wind or the 2008 musical Gone with the Wind.\\nTherefore, this question has 2 interpretations: (1) Who played Bonnie in the 1939 film Gone with the Wind? (2) Who played\\nBonnie in the 2008 musical Gone with the Wind? The answers to all interpretations are: (1) The 1939 film Gone with the\\nWind’s character Bonnie was played by Eleanore Cammack \"Cammie\" King. (2) The 2008 musical Gone with the Wind’s\\ncharacter Bonnie was played by Leilah de Meza.\\nGiven an ambiguous question, figure out its interpretations and answer them one by one.\\nQuestion: What is the second largest city in the usa?\\nAnswer: This question is ambiguous in terms of the criteria being used to determine the second largest city in the USA.\\nIn order to figure out its interpretations, we need to consider different criteria to determine a city’s size. City size can be\\nmeasured by two criteria: population or area. Therefore, this question has 2 interpretations: (1) What is the second largest\\ncity in the USA by population? (2) What is the second largest city in the USA by area? The answers to all interpretations are:\\n(1) The second largest city in the USA by population is Los Angeles, California. (2) The second largest city in the USA by\\narea is Juneau, Alaska.\\nGiven an ambiguous question, figure out its interpretations and answer them one by one.\\nQuestion: When was bohemian rhapsody released as a single?\\nAnswer: This question is ambiguous in terms of which country’s release of the single is being referred to. In order to figure\\nout its interpretations, we need to consider different countries where Bohemian Rhapsody is released. Bohemian Rhapsody\\nwas released in the United Kingdom and in the United States on different dates. Therefore, this question has 2 interpretations:\\n(1) When was Bohemian Rhapsody released as a single in the United Kingdom? (2) When was Bohemian Rhapsody released\\nas a single in the United States? The answers to all interpretations are: (1) Bohemian Rhapsody was released as a single in the\\nUnited Kingdom on 31 October 1975. (2) Bohemian Rhapsody was released as a single in the United States on December 1975.\\nGiven an ambiguous question, figure out its interpretations and answer them one by one.\\nQuestion: Where do the philadelphia eagles play their home games?\\nAnswer: This question is ambiguous in terms of which specific location or venue is being referred to. In order to figure out its\\ninterpretations, we need to consider the different possible locations or venues that could be considered the home field of the\\nPhiladelphia Eagles. These include the city, the sports complex, or the stadium. Therefore, this question has 3 interpretations:\\n(1) What city do the Philadelphia Eagles play their home games? (2) In what sports complex do the Philadelphia Eagles play\\ntheir home games? (3) What stadium do the Philadelphia Eagles play their home games? The answers to all interpretations\\nare: (1) Philadelphia Eagles play their home games in the city Philadelphia. (2) Philadelphia Eagles play their home games in\\nthe South Philadelphia Sports Complex. (3) Philadelphia Eagles play their home games in the Lincoln Financial Field stadium.\\nGiven an ambiguous question, figure out its interpretations and answer them one by one.\\nQuestion: When did xbox one come out in australia?\\nAnswer: This question is ambiguous in terms of which specific version of the Xbox One is being referred to. In order to\\nfigure out its interpretations, we need to consider the different versions of the Xbox One that have been released. Xbox One\\nhas two versions: the Xbox One video game console or the Xbox One X high-end model. Therefore, this question has 2\\ninterpretations: (1) When did the Xbox One release in Australia? (2) When did the Xbox One X release in Australia? The\\nanswers to all interpretations are: (1) The Xbox One video game console was released in Australia on November 22, 2013.\\n(2) The Xbox One X video game console was released in Australia on November 7, 2017.\\nGiven an ambiguous question, figure out its interpretations and answer them one by one.\\nQuestion: When does the movie summer of 84 come out?\\nAnswer: This question is ambiguous in terms of which release of the movie is being referred to. In order to figure out its\\ninterpretations, we need to consider different releases of the movie Summer of ’84. The movie Summer of ’84 is first released\\nat the Sundance Festival before it’s released throughout the US. Therefore, this question has 2 interpretations: (1) When\\ndid the movie Summer of ’84 first release at the Sundance Festival? (2) When did the movie Summer of ’84 first release\\nthroughout the US? The answers to all interpretations are: (1) Summer of ’84 was released at the Sundance Festival on\\nJanuary 22, 2018. (2) Summer of ’84 was released throughout the US on August 10, 2018.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 20, 'page_label': '21'}, page_content='Prompt D.7: exemplars of ASQA (cont.)\\nGiven an ambiguous question, figure out its interpretations and answer them one by one.\\nQuestion: What was roy orbison’s first number one hit?\\nAnswer: This question is ambiguous in terms of which specific chart or region is being referred to. In order to figure out its\\ninterpretations, we need to consider the different charts and regions where Roy Orbison’s music was popular. Roy Orbison is\\npopular in both the US Hot 100 and Canada, and the UK and Ireland. Therefore, this question has 2 interpretations: (1) What\\nwas Roy Orbison’s first number one hit in the US Hot 100 and Canada? (2) What was Roy Orbison’s first number one hit in\\nthe UK and Ireland? The answers to all interpretations are: (1) Running Scared was the first number one hit for Roy Orbison\\nin the US Hot 100 and Canada. (2) Only the Lonely (Know the Way I Feel) was the first number one hit for Roy Orbison in\\nthe UK and Ireland.\\nGiven an ambiguous question, figure out its interpretations and answer them one by one.\\nQuestion: What is the criminal’s name in the breakfast club?\\nAnswer: This question is ambiguous in terms of which specific name is being referred to - the character’s name or the actor’s\\nname. In order to figure out its interpretations, we need to consider both possibilities: the character’s name or the actor’s\\nname. Therefore, this question has 2 interpretations: (1) What is the criminal’s character name in The Breakfast Club? (2)\\nWhat is the the name of the actor who played the criminal in The Breakfast Club? The answers to all interpretations are: (1)\\nJohn Bender was the name of the criminal’s character in The Breakfast Club. (2) Judd Nelson was the actor of the criminal in\\nThe Breakfast Club.\\nGiven an ambiguous question, figure out its interpretations and answer them one by one.\\nQuestion: How many state parks are there in virginia?\\nAnswer:'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 21, 'page_label': '22'}, page_content='Prompt D.8: exemplars of ASQA-hint\\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpretations and\\nanswer them one by one.\\nQuestion: Who played bonnie in gone with the wind?\\nHint: This question is ambiguous in terms of which version or adaptation of Gone with the Wind is being referred to.\\nAnswer: In order to figure out its interpretations, we need to consider different versions or adaptations of Gone with the Wind.\\nGone with the Wind has two versions or adaptations: the 1939 film Gone with the Wind or the 2008 musical Gone with the\\nWind. Therefore, this question has 2 interpretations: (1) Who played Bonnie in the 1939 film Gone with the Wind? (2) Who\\nplayed Bonnie in the 2008 musical Gone with the Wind? The answers to all interpretations are: (1) The 1939 film Gone with\\nthe Wind’s character Bonnie was played by Eleanore Cammack \"Cammie\" King. (2) The 2008 musical Gone with the Wind’s\\ncharacter Bonnie was played by Leilah de Meza.\\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-\\ntions and answer them one by one.\\nQuestion: What is the second largest city in the usa?\\nHint: This question is ambiguous in terms of the criteria being used to determine the second largest city in the USA.\\nAnswer: In order to figure out its interpretations, we need to consider different criteria to determine a city’s size. City size can\\nbe measured by two criteria: population or area. Therefore, this question has 2 interpretations: (1) What is the second largest\\ncity in the USA by population? (2) What is the second largest city in the USA by area? The answers to all interpretations are:\\n(1) The second largest city in the USA by population is Los Angeles, California. (2) The second largest city in the USA by\\narea is Juneau, Alaska.\\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-\\ntions and answer them one by one.\\nQuestion: When was bohemian rhapsody released as a single?\\nHint: This question is ambiguous in terms of which country’s release of the single is being referred to.\\nAnswer: In order to figure out its interpretations, we need to consider different countries where Bohemian Rhapsody is\\nreleased. Bohemian Rhapsody was released in the United Kingdom and in the United States on different dates. Therefore,\\nthis question has 2 interpretations: (1) When was Bohemian Rhapsody released as a single in the United Kingdom? (2) When\\nwas Bohemian Rhapsody released as a single in the United States? The answers to all interpretations are: (1) Bohemian\\nRhapsody was released as a single in the United Kingdom on 31 October 1975. (2) Bohemian Rhapsody was released as a\\nsingle in the United States on December 1975.\\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-\\ntions and answer them one by one.\\nQuestion: Where do the philadelphia eagles play their home games?\\nHint: This question is ambiguous in terms of which specific location or venue is being referred to.\\nAnswer: In order to figure out its interpretations, we need to consider the different possible locations or venues that could be\\nconsidered the home field of the Philadelphia Eagles. These include the city, the sports complex, or the stadium. Therefore,\\nthis question has 3 interpretations: (1) What city do the Philadelphia Eagles play their home games? (2) In what sports\\ncomplex do the Philadelphia Eagles play their home games? (3) What stadium do the Philadelphia Eagles play their home\\ngames? The answers to all interpretations are: (1) Philadelphia Eagles play their home games in the city Philadelphia. (2)\\nPhiladelphia Eagles play their home games in the South Philadelphia Sports Complex. (3) Philadelphia Eagles play their\\nhome games in the Lincoln Financial Field stadium.\\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-\\ntions and answer them one by one.\\nQuestion: When did xbox one come out in australia?\\nHint: This question is ambiguous in terms of which specific version of the Xbox One is being referred to.\\nAnswer: In order to figure out its interpretations, we need to consider the different versions of the Xbox One that have been\\nreleased. Xbox One has two versions: the Xbox One video game console or the Xbox One X high-end model. Therefore, this\\nquestion has 2 interpretations: (1) When did the Xbox One release in Australia? (2) When did the Xbox One X release\\nin Australia? The answers to all interpretations are: (1) The Xbox One video game console was released in Australia on\\nNovember 22, 2013. (2) The Xbox One X video game console was released in Australia on November 7, 2017.\\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-\\ntions and answer them one by one.\\nQuestion: When does the movie summer of 84 come out?\\nHint: This question is ambiguous in terms of which release of the movie is being referred to.\\nAnswer: In order to figure out its interpretations, we need to consider different releases of the movie Summer of ’84. The\\nmovie Summer of ’84 is first released at the Sundance Festival before it’s released throughout the US. Therefore, this question\\nhas 2 interpretations: (1) When did the movie Summer of ’84 first release at the Sundance Festival? (2) When did the movie\\nSummer of ’84 first release throughout the US? The answers to all interpretations are: (1) Summer of ’84 was released at the\\nSundance Festival on January 22, 2018. (2) Summer of ’84 was released throughout the US on August 10, 2018.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 22, 'page_label': '23'}, page_content='Prompt D.9: exemplars of ASQA-hint (cont.)\\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpretations and\\nanswer them one by one.\\nQuestion: What was roy orbison’s first number one hit?\\nHint: This question is ambiguous in terms of which specific chart or region is being referred to.\\nAnswer: In order to figure out its interpretations, we need to consider the different charts and regions where Roy Orbison’s\\nmusic was popular. Roy Orbison is popular in both the US Hot 100 and Canada, and the UK and Ireland. Therefore, this\\nquestion has 2 interpretations: (1) What was Roy Orbison’s first number one hit in the US Hot 100 and Canada? (2) What\\nwas Roy Orbison’s first number one hit in the UK and Ireland? The answers to all interpretations are: (1) Running Scared\\nwas the first number one hit for Roy Orbison in the US Hot 100 and Canada. (2) Only the Lonely (Know the Way I Feel) was\\nthe first number one hit for Roy Orbison in the UK and Ireland.\\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-\\ntions and answer them one by one.\\nQuestion: What is the criminal’s name in the breakfast club?\\nHint: This question is ambiguous in terms of which specific name is being referred to - the character’s name or the actor’s\\nname.\\nAnswer: In order to figure out its interpretations, we need to consider both possibilities: the character’s name or the actor’s\\nname. Therefore, this question has 2 interpretations: (1) What is the criminal’s character name in The Breakfast Club? (2)\\nWhat is the the name of the actor who played the criminal in The Breakfast Club? The answers to all interpretations are: (1)\\nJohn Bender was the name of the criminal’s character in The Breakfast Club. (2) Judd Nelson was the actor of the criminal in\\nThe Breakfast Club.\\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-\\ntions and answer them one by one.\\nQuestion: How many state parks are there in virginia?\\nHint: This question is ambiguous in terms of the time frame or period being referred to.\\nAnswer:'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 23, 'page_label': '24'}, page_content='Prompt D.10: exemplars of WikiAsp\\nGenerate a summary about Aslanhane Mosque including the following aspects: location, history with one aspect per line.\\n# Location\\nThe mosque is in the old quarter of ankara next to ankara castle. With an altitude of 947 metres (3,107 ft) it overlooks ankara\\nat 39°56’12\"N 32°51’55\"E.\\n# History\\nThe mosque is one of the oldest mosques in Turkey still standing. It was built during the reign of Mesud II of the Anatolian\\nSeljuks in 1290. Its architect was Ebubekir Mehmet. It was commissioned by two Ahi leaders named Hüsamettin and\\nHasaneddin. However, in 1330, it was repaired by another Ahi leader named ¸ Serafettin after whom the mosque was named.\\nAfter several minor repairs the mosque was restored by the directorate general of foundations in 2010-2013 term.\\nGenerate a summary about Untold Legends: The Warrior’s Code including the following aspects: reception,\\ngameplay, development with one aspect per line.\\n# Reception\\nThe game received \"mixed or average reviews\" according to video game review aggregator Metacritic.\\n# Gameplay\\nThe warrior’s code is a hack n’ slash action role-playing game, which concentrates on action-oriented combat.\\n# Development\\nAs a pre-order bonus, the game was shipped with a small action figure of the Guardian class.\\nGenerate a summary about Raid on St. Augustine including the following aspects: aftermath, background with\\none aspect per line.\\n# Aftermath\\nOnce the English had gone Menéndez and the rest of the Spanish settlers returned to find a smoldering ruins and very little\\nleft. He soon and begged for help from the viceroy of Cuba and the settlement took a while to build itself back up. The\\ndestroyed fort was replaced with the present day Castillo de San Marcos.\\n# Background\\nWar had already been unofficially declared by Philip II of Spain after the Treaty of Nonsuch in which Elizabeth I had\\noffered her support to the rebellious Protestant Dutch rebels. The Queen through Francis Walsingham ordered Sir Francis\\nDrake to lead an expedition to attack the Spanish New World in a kind of preemptive strike. Sailing from Plymouth,\\nEngland, he struck first at Santiago in November 1585 then across the Atlantic at the Spanish new world city of Santo\\nDomingo of which was captured and ransomed on 1 January 1586 and following that successfully attacked the important\\ncity of Cartagena on 19 February. Drake wanted to strike at another Spanish city on the Main before finally visiting and\\nreplenishing Sir Walter Raleigh’s new colony of Roanoke Colony on the American East Coast. Then after this he hoped\\nto make the Transatlantic crossing back to England. The fleet headed north, and in late April Drake put into the Spanish\\nCuban mainland and his men dug wells in search of fresh water and gathered supplies to help counter an outbreak of\\ndysentery after which he moved on. The fleet traveled north within sight of land on the Florida peninsula sailing past\\nthe West coast. On 27 May 1586 as they approached further north a small fort was spotted on the shore, with a small\\ninlet close by. This was the location of St Augustine, the most northerly town in Spain’s New World Empire, and the\\noldest permanent colonial settlement in North America. Drake knew of the place and was also aware of the fact that\\nthe spanish under Pedro Menéndez de Avilés had ordered all of the French Huguenot colonists that had tried to settle\\nin the area executed. Drake decided on one final opportunity to raid and plunder, and a chance to avenge his fellow Protestants.\\nGenerate a summary about Lakewood (Livingston, Alabama) including the following aspects: architecture, his-\\ntory with one aspect per line.\\n# Architecture\\nThe house has a plan that is relatively rare in early Alabama architecture. The plan features a brick ground floor that is topped\\nby one-and-a-half-stories of wood-frame construction. The ground floor originally contained domestic spaces, with the\\nformal rooms on the principle floor and bedrooms on the upper floor. A central hallway is present on all levels. The facade is\\nfive bays wide, with central entrance doors on the ground and principle floors. The bays are divided by two-story Doric\\npilasters, with the middle third of the facade occupied by a two-tiered tetrastyle Doric portico. Two curved wrought iron\\nstaircases ascend from ground level to the front center of the upper portico, leading to the formal entrance.\\n# History\\nLakewood was built for Joseph lake, a native of North Carolina, by Hiram W. Bardwell, a master builder. Construction\\nwas completed in 1840. Located adjacent to the University of West Alabama, Julia Strudwick Tutwiler, a Lake relative,\\nperiodically resided in the house from 1881 to 1910 while she served as president of the university. It was then known as\\nLivingston Normal College. The house was extensively photographed by Alex Bush for the Historic American Buildings\\nSurvey in November and December 1936. Lakewood has continued to be owned by descendants of the Lake family to the\\ncurrent day. The house and its surviving 10 acres (4.0 ha) of grounds were listed on the Places in Peril in 2012 due to the\\nimmediate threat of its acquisition by developers.\\nGenerate a summary about Carlos Moedas including the following aspects: biography, early life, political career\\nwith one aspect per line.'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='arXiv:2505.10792v2  [cs.CL]  19 May 2025\\nFinetune-RAG: Fine-Tuning Language Models to\\nResist Hallucination in Retrieval-Augmented\\nGeneration\\nZhan Peng Lee\\nPints AI Labs\\nzhanpeng.lee@pints.co\\nAndre Lin∗\\nPints AI Labs\\nandrelim444@gmail.com\\nCalvin Tan\\nPints AI Labs\\ncalvin@pints.co\\nAbstract\\nRetrieval-Augmented Generation (RAG) has emerged as a powerful framework to\\nimprove factuality in large language models (LLMs) by grounding their outputs in\\nretrieved documents. However, ensuring perfect retrieval of relevant information\\nremains challenging, and when irrelevant content is passed downstream to an LLM,\\nit can lead to hallucinations. In this work, we propose Finetune-RAG, a simple\\nand effective fine-tuning approach that features the first-of-its-kind RAG training\\ndataset constructed to mimic real-world imperfections. Experimental results show\\nthat Finetune-RAG improves factual accuracy by 21.2% over the base model. We\\nalso propose Bench-RAG, an LLM-as-a-judge evaluation pipeline that stress tests\\nmodels under realistic imperfect retrieval scenarios. Our codebase2 and dataset3\\nare fully open sourced for community use.\\n1 Introduction\\nLarge Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of\\nnatural language processing tasks (Wang et al., 2023; Rozière et al., 2024; Cui et al., 2025; Yasunaga\\net al., 2022; Liu et al., 2024). However, their tendency to \"hallucinate\", that is, to produce fluent\\nbut factually incorrect information, remains a persistent challenge (Li et al., 2024a; Duan et al.,\\n2024; Zhang et al., 2023), particularly in high-stakes domains such as healthcare, law, and finance\\n(Agarwal et al., 2024; Dahl et al., 2024; Kang and Liu, 2023). To address this, Retrieval-Augmented\\nGeneration (RAG)has become a popular solution. Instead of relying solely on parametric memory,\\nRAG systems retrieve external documents and condition the model’s response on this evidence.\\nIn practice, retrieval accuracy in RAG is far from flawless. Retrieved documents may be outdated,\\nmisleading, or topically adjacent but factually incorrect. These errors can propagate downstream,\\nleading models to blend inaccurate context into fluent but false answers. This is especially concerning\\nin domains such as law, compliance, financial reporting, or medicine, where mistakes can have\\nwide-ranging repercussions.\\nMost prior work has addressed this issue from the retrieval perspective, focusing on improving\\nretrievers, reranking mechanisms, or applying filtering heuristics (Sawarkar et al., 2024; Dong et al.,\\n2024; Zhou and Chen, 2025). In contrast, relatively little attention has been given to improving the\\nmodel’s ability to resist using the incorrect information.\\nIn this paper, we introduce Finetune-RAG, a method that directly targets hallucination by fine-tuning\\nthe model with imperfect RAG samples that mimic real-world retrieval scenarios. We constructed a\\ndiverse dataset covering legal documents, scientific literature, books, and web data, each paired with\\n∗Work was done during an internship at Pints AI\\n2https://github.com/Pints-AI/Finetune-Bench-RAG\\n3https://huggingface.co/datasets/pints-ai/Finetune-RAG'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='a plausible but fictitious counterpart. We then fine-tune instruction-tuned LLMs, specifically Meta’s\\nLlama 3.1-8B-Instruct (Grattafiori et al., 2024), on this dataset using two prompt variants: a Baseline\\nformat and a Structured XMLvariant. This setup allows us to assess generalization and prompt\\nsensitivity. To our knowledge, Finetune-RAG provides the first RAG dataset of its kind, as existing\\nRAG finetuning datasets implicitly assume perfect information retrieval, and mostly focus only the\\nLLM’s ability to extract coherent answers from relevant chunks.\\nOur key insight is that LLMs struggle to identify contextual clues that are obvious to the human eye,\\nsuch as financial reports from a similarly named company or outdated information based on dates\\nindicated by document metadata. Through fine-tuning models with a controlled mixture of true and\\nfalse context placed alongside, we teach them to ground their answers exclusively in the reliable\\ninformation provided.\\nWe evaluated the effectiveness of Finetune-RAG usingBench-RAG, a custom benchmarking suite\\nwe have created that leverages GPT-4o (OpenAI, 2024) as an automated judge to assess the accu-\\nracy, relevance, helpfulness and depth of the LLM response. Our results show that Finetune-RAG\\nsubstantially improves factual correctness while maintaining output quality across other dimensions,\\ndemonstrating that generation-time defenses are a viable complement to improved retrieval.\\nOur contributions are as follows:\\n• Fine-tuning Approach. We propose a novel fine-tuning strategy for RAG systems that\\nteaches models to ignore misleading context and generate answers based solely on factual\\ninput.\\n• Training Dataset.We release a curated, multi-domain dataset designed for hallucination\\nresistance training, with both factual and fictitious content.\\n• Evaluation Setup.We benchmark the effectiveness of our approach using GPT-4o-based\\nevaluations and show significant gains in factual accuracy without compromising helpfulness\\nor relevance.\\n• Open-source release. We make our code, models, dataset, and evaluation framework\\npublicly available to facilitate further research. They can be accessed in our open-source\\nrepository4 and dataset5.\\nBy fine-tuning LLMs on RAG examples containing both factual and fictitious documents, we show\\nthat it is possible to build models that can reliably choose truth over noise. Our dataset reflects noisy,\\ndomain-diverse retrieval as encountered in practice, making it a strong foundation for stress-testing\\nhallucination resistance in future RAG systems.\\n2 Background\\n2.1 Retrieval-Augmented Generation\\nRetrieval-Augmented Generation (RAG) augments large language models by incorporating external\\ndocuments into the generation process. Rather than relying solely on the model’s internal parameters,\\nRAG retrieves relevant passages from a knowledge base and feeds them, along with the user query,\\ninto the model to guide its response (Zhou et al., 2024).\\nA standard RAG system operates in two phases:\\n• Retrieval. A retriever model selects the top-k most relevant documents for a given query.\\n• Generation. A language model generates a response conditioned on both the query and the\\nretrieved documents.\\nThe appeal of RAG lies in its ability to dynamically access up-to-date or domain-specific information,\\nwhich is especially useful in fast-changing or specialized fields. However, it also introduces new\\nfailure modes, particularly when the retrieval quality is imperfect (Barnett et al., 2024).\\n4https://github.com/Pints-AI/Finetune-Bench-RAG\\n5https://huggingface.co/datasets/pints-ai/Finetune-RAG\\n2'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='2.2 Hallucination in Language Models\\nHallucination refers to the phenomenon where language models produce outputs that are factually\\nincorrect or unsupported by the input, resulting in unfaithful outputs (Rawte et al., 2023). In RAG\\nsystems, hallucination can be especially problematic when the model is presented with a mixture of\\nrelevant and irrelevant (or even misleading) context. Even with carefully worded prompts, models\\ncan inadvertently \"trust\" incorrect sources and generate plausible but wrong answers (Yoran et al.,\\n2024).\\nDespite the presence of external context, most current models lack mechanisms to actively filter or\\nignore misleading information once it is included in the prompt (Shi et al., 2023). Finetune-RAG\\nspecifically targets this weakness by training models to develop this filtering capability.\\n3 Related Works\\n3.1 Mitigating Hallucination with Synthetic Prompt Tuning\\nSYNTRA (Jones et al., 2023) reduces hallucinations in large language models by modifying the\\nmodel’s instructions rather than adjusting its internal weights. SYNTRA does this by attaching a\\nsmall, trainable embedding vector to the system message, which acts as an additional instruction\\nprefix. This vector is optimized using a synthetic task where hallucinations are easy to measure. For\\nexample, the model is prompted to return names starting with a specific letter from a visible list, and\\nany incorrect or invented names are counted as hallucinations. By learning to avoid such mistakes in\\na controlled setting, the model can generalize to reduce hallucinations in downstream tasks. However,\\nbecause SYNTRA focuses on modifying prompts and not the model’s internal reasoning, it does not\\nenable the model to distinguish between factual and misleading content, failing to address real-world\\nRAG scenarios (Barnett et al., 2024)(Shi et al., 2023).\\n3.2 Refusal-Aware Fine-Tuning\\nZhang et al. (2024) propose a fine-tuning method, R-Tuning, that teaches language models to express\\nuncertainty and decline to answer when a question falls outside their pre-trained knowledge. This is\\nachieved by identifying questions the model answers incorrectly during training and appending an\\nuncertainty statement such as “I am unsure” to those responses. The result is a model that behaves\\nmore conservatively and with improved confidence calibration. However, R-Tuning is designed for\\nclosed-book settings, where the model relies only on its internal knowledge without a RAG system.\\n3.3 Constrained Reasoning with Decompose-and-Query (D&Q)\\nCao et al. (2023) propose the Decompose-and-Query (D&Q) framework, which extends retrieval-\\naugmented generation (RAG) by teaching language models to break down complex queries, retrieve\\nrelevant information using external tools, and generate answers based on a structured knowledge\\nsource. In particular, D&Q introduces a curated question–answer (QA) base, which is a collection of\\nverified QA pairs that the model consults during reasoning. This setup helps reduce hallucinations\\nby constraining the model to reliable content and allowing it to backtrack when inconsistencies are\\ndetected.\\nHowever, the effectiveness of D&Q depends strongly on the quality and coverage of its QA base. In\\npractical RAG applications, where retrieved content can be noisy, ambiguous, or incomplete (Shi\\net al., 2023), relying on a fixed and curated source may become a limitation. Since the framework\\nlacks mechanisms to dynamically assess the reliability of new information, it remains susceptible to\\nhallucinations caused by misleading or inaccurate context.\\n4 Methodology\\nWe introduce Finetune-RAG, a fine-tuning method designed to train large language models (LLMs)\\nto distinguish between correct and fictitious context within a Retrieval-Augmented Generation (RAG)\\nsetup. Unlike prior work that attempts to improve factuality by enhancing the retrieval phase,\\nFinetune-RAG focuses on improving the model’s generation behavior when faced with imperfect or\\n3'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='misleading inputs. Our core idea is to fine-tune the model using examples where both correct and\\nincorrect information are explicitly presented to model, allowing it to learn the ability to sift out the\\ncorrect information to use for its response.\\n4.1 Problem Setup\\nIn a typical RAG system, the model is given a user query q and a set of retrieved documents\\n{d1, d2, ..., dk} (Zhou et al., 2024). When any of the documents is irrelevant or misleading, the model\\nmay generate incorrect responses (Yoran et al., 2024).\\nIn Finetune-RAG, we simulate this scenario during training by constructing prompts that include:\\n• One correct (factual) document chunk dcorrect\\n• One fictitious (misleading) document chunk dfictitious\\n• A corresponding question q\\n• A reference answer a, written using only dcorrect as the reference\\nThe model is then trained using supervised fine-tuning to produce the answer a despite having access\\nto both dcorrect and dfictitious in the input.\\nIn Bayesian modeling, we can think of the task as a conditional generation problem where the goal is\\nto maximize the probability of generating a truthful answer a given a question q and a mixed set of\\ncontexts (some correct dcorrect, some fictitious dfictitious).\\nWe aim to model:\\nP(a | q, dcorrect, dfictitious) (1)\\nHowever, this is the observed conditional probability, and what we want the model to learn is to\\nignore dfictitious and generate the answer as if conditioned only on dcorrect. So our training objective is\\nto align to the following idealized posterior:\\nP∗(a | q, dcorrect, dfictitious) → P(a | q, dcorrect) (2)\\nIn other words, even though the model receives both correct and fictitious information, it must assign\\nzero (or negligible) attention/mass to dfictitious during decoding.\\n4.2 Prompt Construction\\nEach training example in Finetune-RAG is processed to include a system message and a user\\nmessage, following the standard instruction-tuning format (Ouyang et al., 2022) used in chat-style\\nlanguage models. The system message defines the behavior of the assistant, while the user message\\nprovides the question along with correct and fictitious information.\\n4.2.1 System Message\\nThe system message is consistent in all training examples. It instructs the assistant to rely solely on\\nthe provided context and discourages the use of prior knowledge or hallucination:\\n\"Some information is retrieved from the database as provided based on the\\nuser’s question. The assistant is to answer the question to the best of\\nhis/her ability, using only the information provided. The assistant must\\nnot add his/her own knowledge.\"\\n4.2.2 User Message\\nTo help the model distinguish between factual and fictitious context more effectively, we explore the\\nuse of XML-like (Bray et al., 1998) structured input. We hypothesize that introducing a consistent\\nand explicit hierarchy, where document chunks are clearly labeled and separated, can make it easier\\n4'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='for the model to parse and evaluate different sources of information. This is especially important in\\nRAG settings, where hallucinations often result from the model blending or misattributing content\\nacross documents. Our approach aligns with findings from recent work such as StructRAG (Li et al.,\\n2024b) and SRAG (Lin et al., 2025), which demonstrates that task-specific structured representations\\nsuch as tables or graphs can significantly improve the performance of LLMs on knowledge-intensive\\nreasoning tasks. Our use of XML aims to impose syntactic clarity and boundary enforcement at the\\ninput level.\\nTo test this, we compare two user message formats: an unstructuredBaseline Formatand a structured\\nXML Format. Both present a question along with two document chunks, one factual and one\\nfictitious, but differ in how the information is presented. Refer to Section 6.4 for the exact prompt\\nstructure.\\n5 Experimental Setup\\n5.1 Model\\nWe fine-tuned Meta’s Llama 3.1–8B-Instruct (Grattafiori et al., 2024), an instruct-tuned model that\\nsupports chat-style interaction and long context windows. We adapt the system and user message\\nformatting based on the chosen prompt structure described in Section 4.2.2.\\n5.2 Dataset and Preprocessing\\nOur dataset contains a total of 1,653 examples from diverse domains, such as legal documents,\\nscientific papers, news articles, and technical reports. For the complete structure of each example in\\nthe dataset, refer to Annex A.\\nEach example is formatted in both the baseline and XML structures. The dataset is then partitioned\\ninto training (80%), validation (10%), and test (10%) sets.\\n5.3 Hyperparameters\\nWe selected hyperparameter values that balance model performance with computational efficiency.\\nRefer to Table 1 for the complete set of hyperparameters used.\\nTable 1: Fine-tuning hyperparameters used on Llama 3.1-8B-Instruct\\nParameter Value\\nSteps 20\\nBatch size 64\\nLearning rate 2e-5\\nWarmup ratio 0.1\\nLR Scheduler Cosine decay\\nOptimizer AdamW\\nβ1 0.9\\nβ2 0.95\\nWeight decay 0.1\\nMixed precision BF16\\n5.4 Checkpoints and Reproducibility\\nWe have released the model checkpoints fine-tuned with both Baseline6 and XML7 formats on Hug-\\ngingFace. Each prompt structure has two repositories, and each repository contains five checkpoints,\\ntotaling 10 checkpoints each.\\n6https://huggingface.co/pints-ai/Llama-3.1-8B-Instruct-RAG_Baseline_tuned-1\\nhttps://huggingface.co/pints-ai/Llama-3.1-8B-Instruct-RAG_Baseline_tuned-2\\n7https://huggingface.co/pints-ai/Llama-3.1-8B-Instruct-RAG_XML_tuned-1\\nhttps://huggingface.co/pints-ai/Llama-3.1-8B-Instruct-RAG_XML_tuned-2\\n5'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='6 Evaluation\\nWe evaluate Finetune-RAG’s ability to generate factually accurate answers when presented with both\\ncorrect and fictitious context. Our evaluation framework focuses on measuring whether the model\\nis able to selectively use only the correct information, and we assess output quality across four key\\ndimensions.\\n6.1 Bench-RAG\\nWe adopt a custom benchmarking pipeline, namely Bench-RAG, using GPT-4o model (OpenAI,\\n2024) in a LLM-as-a-judge inspired by prior work(Zheng et al., 2023; Gu et al., 2025; Li et al., 2025).\\nUsing structured prompts to elicit consistent evaluations for each model output, we measure:\\n• Accuracy: A binary metric indicating whether the generated answer is factually correct and\\nbased solely on the correct chunk. (True/False)\\n• Helpfulness: A score from 1 to 10 assessing how useful the answer is in addressing the\\nuser’s question.\\n• Relevance: A score from 1 to 10 measuring how relevant the content is to the query.\\n• Depth: A score from 1 to 10 reflecting the level of detail or insight present in the answer.\\nEach generated output is rated using a structured prompt format, which requests scores across these\\ncategories and a brief justification. Refer to Appendix B for the full structure. This methodology\\ndraws from recent research demonstrating that LLMs can align closely with human preferences when\\nprompted properly, achieving high inter-rater agreement, i.e. multiple evaluators provide consistent\\nratings for the same outputs (Gu et al., 2025; Li et al., 2025).\\n6.2 Checkpoints Evaluated\\nFor each prompt structure, we evaluate all 10 model checkpoints saved during training (see Section\\n5.4). These checkpoints represent the model’s learning trajectory over the course of a single fine-\\ntuning epoch. At each checkpoint, we generate answers to the test dataset questions using both the\\ncorrect context dcorrect and the fictitious context dfictitious. The generated answers are then submitted to\\nthe evaluator for scoring. Refer to Appendix B.1 and B.2 for the structure of the prompt used for\\nevaluation.\\n6.3 Results\\nWe report quantitative results from our fine-tuning experiments scored across 4 dimensions: factual\\naccuracy, helpfulness, relevance, and depth. Evaluation was performed using GPT-4o (OpenAI,\\n2024) as an LLM judge, as described in Section 6.1. We then aggregate the scores of each sequence\\nin the test dataset to derive the final evaluation result for each checkpoint:\\n¯Accuracy =\\n \\n1\\nntest\\nntestX\\ni=1\\n1[Accuracyi = T rue]\\n!\\n× 100% (3)\\n¯Helpfulness = 1\\nntest\\nntestX\\ni=1\\nHelpfulness i (4)\\n¯Relevance = 1\\nntest\\nntestX\\ni=1\\nRelevancei (5)\\n¯Depth = 1\\nntest\\nntestX\\ni=1\\nDepthi (6)\\nFigures 1 and 2 summarize performance trends across training steps. We observe consistent improve-\\nments in factual accuracy over time, particularly in the Baseline format. In most cases, gains in\\n6'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='accuracy are achieved without sacrificing helpfulness or relevance, and in later checkpoints, all four\\nmetrics reach strong levels of performance.\\nNotably, accuracy rises from 76.97% at step 0 to 98.18% at step 20 in the Baseline format, demon-\\nstrating the model’s increasing ability to ignore fictitious context. Helpfulness and depth also improve\\nsteadily, with a dip at the first generated checkpoint.\\nFigure 1: Evaluation results across training steps (Baseline format). Accuracy is plotted on the right\\ny-axis, and other metrics use the left y-axis.\\nStep Acc. (%) Help Rel. Depth\\n0 76.97 8.81 9.55 8.32\\n2 67.88 7.08 7.48 6.76\\n4 91.52 8.08 8.47 7.15\\n6 93.94 9.58 9.83 8.81\\n8 96.36 9.38 9.61 8.55\\n10 97.58 9.33 9.62 8.51\\n12 96.36 9.52 9.78 8.80\\n14 96.97 9.73 9.91 9.01\\n16 97.58 9.78 9.95 9.06\\n18 97.58 9.77 9.95 9.05\\n20 98.18 9.77 9.95 9.02 0 2 4 6 8 10 12 14 16 18 20\\n5\\n6\\n7\\n8\\n9\\n10\\nStep\\nScore\\n50\\n60\\n70\\n80\\n90\\n100\\nAccuracy (%)\\nHelpfulness Relevance Depth Accuracy\\nFigure 2: Evaluation results across training steps (XML format). Accuracy is plotted on the right\\ny-axis, and other metrics use the left y-axis.\\nStep Acc. Help Rel Depth\\n0 78.79 8.81 9.56 8.19\\n2 52.73 5.79 6.16 5.24\\n4 87.88 6.56 7.09 5.47\\n6 95.76 9.46 9.73 8.75\\n8 94.55 9.09 9.35 8.21\\n10 94.55 8.93 9.32 8.01\\n12 95.76 8.95 9.33 8.05\\n14 95.76 9.28 9.59 8.52\\n16 97.58 9.35 9.61 8.61\\n18 97.58 9.28 9.50 8.50\\n20 96.97 9.40 9.64 8.64 0 2 4 6 8 10 12 14 16 18 20\\n5\\n6\\n7\\n8\\n9\\n10\\nStep\\nScore\\n50\\n60\\n70\\n80\\n90\\n100\\nAccuracy (%)\\nHelpfulness Relevance Depth Accuracy\\n7'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='6.4 Ablation: Effect of Prompt Structure\\nTo assess the impact of prompt formatting on hallucination resistance, we perform an ablation study\\ncomparing two versions of Finetune-RAG: one trained using the Baseline formatand another using\\na more structured XML format. Both models were fine-tuned on the same dataset with identical\\nhyperparameters and evaluated using the same GPT-4o-based benchmarking pipeline.\\nPrompt Format Differences The Baseline format presents context in a flat, unstructured layout,\\nwhile the XML format uses nested tags to explicitly delineate retrieved content blocks (see Section\\n4.2.2). We hypothesized that structured formatting might help the model better separate and reason\\nabout distinct chunks.\\nBaseline Format This format presents the retrieved content in a plain and direct layout:\\nFilename: {filename1}\\nInformation:\\n{content1}\\nFilename: {filename2}\\nInformation:\\n{content2}\\nQuestion: {question}\\nXML Format This version wraps the content in an XML-like structure for clearer boundaries:\\n<Results>\\n<Result>\\n<Filename>{filename1}</Filename>\\n<Information>{content1}</Information>\\n</Result>\\n<Result>\\n<Filename>{filename2}</Filename>\\n<Information>{content2}</Information>\\n</Result>\\n</Results>\\nQuestion: {question}\\nResults As shown in Figures 1 and 2, both models demonstrate strong improvements over time.\\nHowever, the Baseline model consistently achieves higher accuracy and better overall scores in the\\nlater checkpoints:\\n• At step 20, the Baseline-tuned model achieves an accuracy of 98.18%, compared to 96.97%\\nfor the XML-tuned model.\\n• The Baseline-tuned model also maintains slightly higher scores for helpfulness (9.77 vs\\n9.40) and depth (9.02 vs 8.64).\\nInterpretation These results suggest that while XML-style formatting introduces clear structural\\nboundaries that aid human readers, it did not consistently outperform the simpler Baseline prompt. We\\noffer two possible explanations: (1) the model may have developed inductive biases from pretraining\\nthat favor interpreting flat, plain-text layouts, such as those seen in summaries or abstracts, and (2)\\nfine-tuning datasets used in LLaMA or similar models may have predominantly featured unstructured\\nprompts, making the model more adept at handling them.\\nThis suggests that while prompt formatting is an important factor, training data design and supervision\\nsignal play a larger role in hallucination resistance.\\n8'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='7 Discussion\\nOur results show that Finetune-RAG significantly improves a model’s ability to resist hallucinations\\nin a RAG setting, even when the prompt includes both correct and misleading context. Fine-tuning\\nwith dual-context examples leads to consistent improvements in factual accuracy, while preserving\\nhelpfulness, relevance, and depth.\\n7.1 Inductive Bias Emergence in Structure-Agnostic Learning\\nA significant and perhaps unexpected result in our study is that models trained on unstructured\\nprompts (Baseline format) performed better, especially in factual accuracy, compared to those trained\\nwith structured XML prompts. This challenges the common belief that clear structure always aids\\nreasoning. Instead, it suggests a deeper learning process, which involves the development of stronger\\nbuilt-in tendencies for selecting content when structure is absent. This raises a potential area that can\\nbe further researched upon.\\n7.2 Limitations\\nDespite promising results, several limitations remain:\\n• Synthetic dataset generation: The fictitious content is generated using GPT-4o (OpenAI,\\n2024), which may introduce distributional artifacts that differ from real-world retrieval\\nerrors. Additionally, the size of the dataset can be further increased for effective fine-tuning\\nin larger models.\\n• Binary supervision: We treat hallucination as a binary decision at the generation level.\\nHowever, hallucination is often more nuanced, involving partial truths, omissions, or subtle\\nphrasing, which our current framework may not sufficiently address.\\n• Controlled context pairing: During training, each example includes exactly one correct\\nand one incorrect document chunk. This creates a simplified binary contrast that may not\\ngeneralize to real-world scenarios where multiple retrieved documents vary in quality. A\\nstronger training approach can be constructed using our existing dataset to create more\\nvaried and robust scenarios that the model can train on.\\n• Compute requirements: While our method is simpler and less resource-intensive than\\nalternatives such as full retraining or reinforcement learning, it still requires access to a\\nhigh-memory GPU (e.g., H100) to fine-tune long-context models with large batch sizes.\\nThis may limit accessibility for some users or institutions.\\n7.3 Future Work\\nThere are several promising extensions to Finetune-RAG that could further improve its robustness\\nand applicability:\\n• Training with more in-context RAG: Real-world retrieval often returns more than two\\ndocuments, and the context window of LLMs are increasing rapidly. At the time of our\\nwork, we focused on relatively low context window of 8k, which would realistically be used\\nfor two to three RAG documents using up to 3k context window. With increasing context\\nwindow, future work can explore training with more RAG chunks to optimize LLMs RAG\\nperformance even at high level of stresses caused by more retrieved chunks. To support this,\\nwe future-proofed our dataset by including two additional relevant chunks per example to\\nsupport generating more complex multi-document training scenarios.\\n• Joint retrieval-generation optimization: While Finetune-RAG focuses on improving the\\ngeneration component, combining it with learned retrieval mechanisms such as reranker-\\naware retrievers or contrastively trained retrievers could lead to further improvements in\\nfactual accuracy and context filtering.\\n• Multimodal extensions: Hallucination is not limited to text-based models. Ex-\\ntending Finetune-RAG to multimodal settings, such as image-caption retrieval or\\ncode+documentation generation, may help build more robust grounded systems in other\\ndomains.\\n9'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='• Evaluation on downstream tasks: While our benchmarking focuses on controlled hallucina-\\ntion settings, future work should assess Finetune-RAG’s impact on end-to-end performance\\nin downstream RAG applications such as open-domain question answering, legal document\\nsummarization, and domain-specific information retrieval.\\n8 Conclusion\\nIn this work, we present Finetune-RAG, a simple yet effective method for reducing hallucination in\\nRetrieval-Augmented Generation (RAG) through supervised fine-tuning. Rather than focusing on\\nretrieval quality, Finetune-RAG trains the generation model to rely solely on factual context while\\nignoring misleading information, with no architectural changes required.\\nWe constructed a diverse training set and evaluate using Bench-RAG, a technique that leverages\\nGPT-4o as an automatic judge. Results show substantial gains in factual accuracy while preserving\\nhelpfulness, relevance, and depth. Ablation studies further reveal that prompt structure subtly impacts\\nrobustness, with less structured formats sometimes aiding discrimination.\\nDespite its simplicity, Finetune-RAG demonstrates that generation-stage fine-tuning can meaningfully\\nimprove hallucination resistance in noisy retrieval environments. We release our code, dataset, and\\ncheckpoints to support further research in this direction, and highlight future extensions including\\nmulti-document training, joint retrieval-generation optimization, and adaptation to multimodal tasks.\\n10'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='References\\nAgarwal, V ., Jin, Y ., Chandra, M., Choudhury, M. D., Kumar, S., and Sastry, N. (2024). Medhalu:\\nHallucinations in responses to healthcare queries by large language models.\\nBarnett, S., Kurniawan, S., Thudumu, S., Brannelly, Z., and Abdelrazek, M. (2024). Seven failure\\npoints when engineering a retrieval augmented generation system.\\nBray, T., Paoli, J., Sperberg-McQueen, C. M., Maler, E., and Yergeau, F. (1998). Extensible markup\\nlanguage (xml) 1.0. https://www.w3.org/TR/REC-xml/. W3C Recommendation.\\nCao, H., An, Z., Feng, J., Xu, K., Chen, L., and Zhao, D. (2023). A step closer to comprehensive\\nanswers: Constrained multi-stage question decomposition with large language models.\\nCui, M., Gao, P., Liu, W., Luan, J., and Wang, B. (2025). Multilingual machine translation with open\\nlarge language models at practical scale: An empirical study.\\nDahl, M., Magesh, V ., Suzgun, M., and Ho, D. E. (2024). Large legal fictions: Profiling legal\\nhallucinations in large language models. Journal of Legal Analysis, 16(1):64–93.\\nDong, J., Fatemi, B., Perozzi, B., Yang, L. F., and Tsitsulin, A. (2024). Don’t forget to connect!\\nimproving rag with graph-based reranking.\\nDuan, H., Yang, Y ., and Tam, K. Y . (2024). Do llms know about hallucination? an empirical\\ninvestigation of llm’s hidden states.\\nGrattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., and et al.\\n(2024). The llama 3 herd of models.\\nGu, J., Jiang, X., Shi, Z., Tan, H., Zhai, X., Xu, C., Li, W., Shen, Y ., Ma, S., Liu, H., Wang, S., Zhang,\\nK., Wang, Y ., Gao, W., Ni, L., and Guo, J. (2025). A survey on llm-as-a-judge.\\nJones, E., Palangi, H., Simões, C., Chandrasekaran, V ., Mukherjee, S., Mitra, A., Awadallah, A., and\\nKamar, E. (2023). Teaching language models to hallucinate less with synthetic tasks.\\nKang, H. and Liu, X.-Y . (2023). Deficiency of large language models in finance: An empirical\\nexamination of hallucination.\\nLi, D., Jiang, B., Huang, L., Beigi, A., Zhao, C., Tan, Z., Bhattacharjee, A., Jiang, Y ., Chen, C.,\\nWu, T., Shu, K., Cheng, L., and Liu, H. (2025). From generation to judgment: Opportunities and\\nchallenges of llm-as-a-judge.\\nLi, J., Chen, J., Ren, R., Cheng, X., Zhao, W. X., Nie, J.-Y ., and Wen, J.-R. (2024a). The dawn after\\nthe dark: An empirical study on factuality hallucination in large language models.\\nLi, Z., Chen, X., Yu, H., Lin, H., Lu, Y ., Tang, Q., Huang, F., Han, X., Sun, L., and Li, Y . (2024b).\\nStructrag: Boosting knowledge intensive reasoning of llms via inference-time hybrid information\\nstructurization.\\nLin, T., Zhu, Y ., Luo, Y ., and Tang, N. (2025). Srag: Structured retrieval-augmented generation for\\nmulti-entity question answering over wikipedia graph.\\nLiu, Y ., Shi, K., He, K., Ye, L., Fabbri, A., Liu, P., Radev, D., and Cohan, A. (2024). On learning to\\nsummarize with large language models as references. In Duh, K., Gomez, H., and Bethard, S.,\\neditors, Proceedings of the 2024 Conference of the North American Chapter of the Association\\nfor Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages\\n8647–8664, Mexico City, Mexico. Association for Computational Linguistics.\\nOpenAI (2024). Gpt-4o system card.\\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal,\\nS., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A.,\\nWelinder, P., Christiano, P., Leike, J., and Lowe, R. (2022). Training language models to follow\\ninstructions with human feedback.\\n11'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='Rawte, V ., Chakraborty, S., Pathak, A., Sarkar, A., Tonmoy, S. M. T. I., Chadha, A., Sheth, A. P., and\\nDas, A. (2023). The troubling emergence of hallucination in large language models – an extensive\\ndefinition, quantification, and prescriptive remediations.\\nRozière, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y ., Liu, J., Sauvestre, R.,\\nRemez, T., Rapin, J., Kozhevnikov, A., Evtimov, I., Bitton, J., Bhatt, M., Ferrer, C. C., Grattafiori,\\nA., Xiong, W., Défossez, A., Copet, J., Azhar, F., Touvron, H., Martin, L., Usunier, N., Scialom, T.,\\nand Synnaeve, G. (2024). Code llama: Open foundation models for code.\\nSawarkar, K., Mangal, A., and Solanki, S. R. (2024). Blended rag: Improving rag (retriever-\\naugmented generation) accuracy with semantic search and hybrid query-based retrievers. In 2024\\nIEEE 7th International Conference on Multimedia Information Processing and Retrieval (MIPR),\\nvolume 24, page 155–161. IEEE.\\nShi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, E., Schärli, N., and Zhou, D. (2023). Large\\nlanguage models can be easily distracted by irrelevant context.\\nWang, Y ., Le, H., Gotmare, A. D., Bui, N. D. Q., Li, J., and Hoi, S. C. H. (2023). Codet5+: Open\\ncode large language models for code understanding and generation.\\nYasunaga, M., Ren, H., Bosselut, A., Liang, P., and Leskovec, J. (2022). Qa-gnn: Reasoning with\\nlanguage models and knowledge graphs for question answering.\\nYoran, O., Wolfson, T., Ram, O., and Berant, J. (2024). Making retrieval-augmented language models\\nrobust to irrelevant context.\\nZhang, H., Diao, S., Lin, Y ., Fung, Y . R., Lian, Q., Wang, X., Chen, Y ., Ji, H., and Zhang, T. (2024).\\nR-tuning: Instructing large language models to say ‘i don’t know’.\\nZhang, Y ., Li, Y ., Cui, L., Cai, D., Liu, L., Fu, T., Huang, X., Zhao, E., Zhang, Y ., Chen, Y ., Wang,\\nL., Luu, A. T., Bi, W., Shi, F., and Shi, S. (2023). Siren’s song in the ai ocean: A survey on\\nhallucination in large language models.\\nZheng, L., Chiang, W.-L., Sheng, Y ., Zhuang, S., Wu, Z., Zhuang, Y ., Lin, Z., Li, Z., Li, D., Xing,\\nE. P., Zhang, H., Gonzalez, J. E., and Stoica, I. (2023). Judging llm-as-a-judge with mt-bench and\\nchatbot arena.\\nZhou, J. and Chen, L. (2025). Openrag: Optimizing rag end-to-end via in-context retrieval learning.\\nZhou, Y ., Liu, Y ., Li, X., Jin, J., Qian, H., Liu, Z., Li, C., Dou, Z., Ho, T.-Y ., and Yu, P. S. (2024).\\nTrustworthiness in retrieval-augmented generation systems: A survey.\\n12'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}, page_content='A Dataset Example Format\\nAligning with Section 4.1, each example in the dataset is structured as follows:\\n{\\n\"content\": <factual chunk>,\\n\"filename\": <original document filename>,\\n\"fictitious_content\": <misleading chunk>,\\n\"fictitious_filename\": <filename of misleading chunk>,\\n\"question\": <user query>,\\n\"answer\": <GPT-4o generated answer based only on correct content>,\\n}\\nB Bench-RAG Prompt Structure\\nGiven both the correct and fictitious document chunks, the fine-tuned model checkpoints are used\\nto generate answers for questions on the test dataset. The outputs are stored in a structured jsonl\\nformat, with each entry containing the following fields:\\n{\\n\"filename\": <original document filename>,\\n\"content\": <factual chunk>,\\n\"question\": <user query>,\\n\"response\": <model’s generated answer>\\n}\\nWith these output, we curate a prompt for the four measurements derived from our evaluation.\\nB.1 System Message for Evaluation\\nAccuracy\\n\"Please act as an impartial judge and evaluate the quality of the\\nresponse provided by an AI assistant to the user question displayed\\nbelow, based solely on a piece of information extracted from a file\\nprovided below. Your evaluation should consider the accuracy of the\\nresponse.\\nYou will check whether the response contains extra details not found\\nin the piece of information provided. If extra details are found,\\naccuracy is false. Otherwise, accuracy is true. Take note that if the\\nresponse partially addresses the question, but did not provide extra\\ndetails not found in the piece of information provided, the response\\nwill still be considered accurate (hence accuracy = true).\\nBegin your evaluation by providing a short explanation. Be as objective\\nas possible. After providing your explanation, you must rate the accuracy\\nwith true or false by strictly following this JSON format:\\n{\\n\"accuracy_explanation\":\\n<provide an explanation on accuracy, whether extra details\\noutside the content were found.>,\\n\"accuracy\": <true/false>\\n}\"\\nHelpfulness\\n\"Please act as an impartial judge and evaluate the quality of the\\nresponse provided by an AI assistant to the user question displayed\\n13'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}, page_content='below, based solely on a piece of information extracted from a file\\nprovided below. Your evaluation should consider the helpfulness of the\\nresponse.\\nYou will check whether the AI assistant is helpful in answering the\\nquestion based on the response.\\nBegin your evaluation by providing a short explanation. Be as objective\\nas possible. After providing your explanation, you must rate the\\nhelpfulness on a scale of 1 to 10 by strictly following this JSON format:\\n{\\n\"helpfulness_explanation\": <provide an explanation on helpfulness>,\\n\"helpfulness\": <score>\\n}\"\\nRelevance\\n\"Please act as an impartial judge and evaluate the quality of the\\nresponse provided by an AI assistant to the user question displayed\\nbelow, based solely on a piece of information extracted from a file\\nprovided below. Your evaluation should consider the relevance of the\\nresponse.\\nYou will check the relevance of the response by evaluating whether the\\nresponse fully addresses the question.\\nBegin your evaluation by providing a short explanation. Be as objective\\nas possible. After providing your explanation, you must rate the\\nrelevance on a scale of 1 to 10 by strictly following this JSON format:\\n{\\n\"relevance_explanation\": <provide an explanation on relevance>,\\n\"relevance\": <score>\\n}\"\\nDepth\\n\"Please act as an impartial judge and evaluate the quality of the\\nresponse provided by an AI assistant to the user question displayed\\nbelow, based solely on a piece of information extracted from a file\\nprovided below. Your evaluation should consider the depth of the\\nresponse.\\nYou will check the depth of the response by evaluating the level of\\ndetail of the response in answering the question.\\nBegin your evaluation by providing a short explanation. Be as objective\\nas possible. After providing your explanation, you must rate the\\ndepth on a scale of 1 to 10 by strictly following this JSON format:\\n{\\n\"depth_explanation\": <provide an explanation on depth>,\\n\"depth\": <score>\\n}\"\\nB.2 User Message for Evaluation\\nAll measurements utilizes the same user message structure for evaluation. Note that the content used\\nis the correct content, rather than the fictitious one:\\n[The Start of Provided Information Extracted from a File]\\n14'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15'}, page_content='Filename: {filename}\\nInformation: {content}\\n[The End of Provided Information]\\n[Question]\\n{question}\\n[The Start of Assistant’s Response]\\n{response}\\n[The End of Assistant’s Response]\\n15')]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunking"
      ],
      "metadata": {
        "id": "O2gakOBZ0ywb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#split text into chunks\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=20,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
        ")\n",
        "docs = text_splitter.split_documents(pages)"
      ],
      "metadata": {
        "id": "2t9UvisvzfMc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Whcv4rAu1kin",
        "outputId": "2cb49682-94d7-427a-a95f-1c967d8c4b82"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='Preprint, Under Review\\nRAFT: Adapting Language Model to Domain Specific RAG\\nTianjun Zhang ∗\\nDepartment of Computer Science\\nUC Berkeley\\nBerkeley, CA 94720, USA\\n{tianjunz}@berkeley.edu\\nShishir G. Patil, Naman Jain, Sheng Shen\\nDepartment of Computer Science\\nUC Berkeley\\nBerkeley, CA 94720, USA\\n{shishirpatil,naman_jain,sheng.s}@berkeley.edu\\nMatei Zaharia, Ion Stoica, Joseph E. Gonzalez\\nDepartment of Computer Science\\nUC Berkeley\\nBerkeley, CA 94720, USA\\n{matei,istoica,jegonzal}@berkeley.edu\\nAbstract\\nPretraining Large Language Models (LLMs) on large corpora of textual\\ndata is now a standard paradigm. When using these LLMs for many\\ndownstream applications, it is common to additionally incorporate new in-\\nformation into the pretrained model either through RAG-based-prompting,\\nor finetuning. However, the best methodology to incorporate information\\nremains an open question. In this paper, we present Retrieval Augmented\\nFine Tuning (RAFT), a training recipe which improves the model’s ability'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='to answer questions in \"open-book\" in-domain settings. In training RAFT,\\ngiven a question, and a set of retrieved documents, we train the model to\\nignore those documents that don’t help in answering the question, which\\nwe call, distractor documents. RAFT accomplishes this by citing verbatim\\nthe right sequence from the relevant document to help answer the question.\\nThis coupled with RAFT’s chain-of-thought-style response helps improve\\nthe model’s ability to reason. In domain specific RAG, RAFT consistently\\nimproves the model’s performance across PubMed, HotpotQA, and Gorilla\\ndatasets, presenting a post-training recipe to improve pre-trained LLMs to\\nin-domain RAG.\\n1 Introduction\\nTrained on vast quantities of public data, Large Language Models LLMs have achieved\\nsignificant advances in a wide range of general knowledge reasoning tasks Brown et al.\\n(2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='domains to support tasks ranging from code completion for specific software frameworks\\nto question answering on specific document collections (e.g., legal or medical documents).\\nIn these settings, general knowledge reasoning is less critical and instead the primary goal\\nis to maximize accuracy based on a given set of documents. Indeed, adapting LLMs to the\\nspecialized domains (e.g., recent news, enterprise private documents, or program resources\\nconstructed after the training cutoff) is essential to many emerging applications (Vu et al.,\\n2023; Lazaridou et al., 2022) and is the focus of this work.\\nThis paper studies the following question – How do we adapt pre-trained LLMs for Retrieval\\nAugmented Generation (RAG) in specialized domains?\\nWhen it comes to adapting LLMs to specialized domains, we consider the following two\\ncandidates: in-context learning through Retrieval-Augmented Generation (RAG) and super-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='vised fine-tuning. RAG based methods allow the LLM to reference the documents when\\n∗Corresponding author, personal website: tianjunz.github.io\\n1\\narXiv:2403.10131v2  [cs.CL]  5 Jun 2024'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='Preprint, Under Review\\n“Open book”\\nquery\\n answer\\n“Closed book”\\nquery\\n answer\\nBake in Knowledge at Train Time Model can use External Docs at Test\\nRAFT (Proposed)\\nquery\\n answer\\nTeach Model to use External Docs at Test\\nFigure 1: How best to prepare for an Exam?(a) Fine-tuning based approaches implement\\n\"studying\" by either directly \"memorizing\" the input documents or answering practice\\nQA without referencing the documents. (b) Alternatively, in-context retrieval methods fail\\nto leverage the learning opportunity afforded by the fixed domain and are equivalent to\\ntaking an open-book exam without studying. In contrast, our approach (c) RAFT leverages\\nfine-tuning with question-answer pairs while referencing the documents in a simulated\\nimperfect retrieval setting — thereby effectively preparing for the open-book exam setting.\\nanswering questions. However, RAG based in-context learning methods fail to leverage'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='the learning opportunity afforded by the fixed domain setting and early access to the test\\ndocuments. Alternatively, supervised fine-tuning offers the opportunity to learn more\\ngeneral patterns in the documents and better align to end tasks and user preferences Zhou\\net al. (2023). However, existing fine-tuning based approaches either fail to leverage the\\ndocuments at test time (don’t incorporate RAG) or fail to account for the imperfections in\\nretrieval process during training.\\nWe can draw an analogy to an open-book exam. Existing in-context retrieval methods are\\nequivalent to taking an open-book exam without studying. Alternatively, existing fine-\\ntuning based approaches implement “studying\" by either directly “memorizing\" Xiong\\net al. (2023) the input documents or answering practice questions Wang et al. (2022) without\\nreferencing the documents. While these approaches leverage in-domain learning they fail to\\nprepare for the open-book nature of the test setting.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='In this paper, we study how to combine instruction fine-tuning (IFT) with retrieval aug-\\nmented generation (RAG). We propose a novel adaptation strategy – Retrieval-Augmented\\nFine Tuning (RAFT). RAFT specifically addresses the challenge of fine-tuning LLMs to both\\nincorporate domain knowledge while also improving in-domain RAG performance. RAFT\\naims to not only enable models to learn domain-specific knowledge through fine-tuning,\\nbut also to ensure robustness against distracting retrieved information. This is achieved\\nby training the models to understand the dynamics between the question (prompt), the\\ndomain-specific documents retrieved, and the right answer. Going back to our analogy to\\nthe open book exam, our approach is analogous to studying for an open-book exam by\\nrecognizing relevant, and irrelevant retrieved documents.\\nIn RAFT, we train the model to answer the question (Q) from Document(s) (D*) to generate'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='answer (A*), where A* includes chain-of-thought reasoning Wei et al. (2022); Anthropic\\n(2023), and in the presence of distractor documents (Dk). We explain the methodology in\\nSection 3 and analyze the sensitivity to the number of distractor documents (k) at train- and\\ntest- time in Section 5. RAFT consistently outperforms Supervised-finetuning both with-\\nand without- RAG across PubMed Dernoncourt & Lee (2017), HotPot QA Yang et al. (2018),\\nand HuggingFace Hub, Torch Hub, and Tensorflow Hub Gorilla datasets Patil et al. (2023),\\npresenting a novel, yet simple technique to improve pre-trained LLMs for in-domain RAG.\\nOur code is available at https://github.com/ShishirPatil/gorilla.\\n2 LLMs for Open-Book Exam\\nTo understand our goal better, we expand on our analogy between training an LLM with\\nthe real-world setting of prepararing for an exam.\\nClosed-Book Exam A closed book exam often refers to the scenario where the LLMs do'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='not have access to any additional documents or references to answer the questions during\\n2'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='Preprint, Under Review\\nFigure 2: Overview of our RAFT method. The top-left figure depicts our approach of\\nadapting LLMs to reading solution from a set of positive and distractor documents in\\ncontrast to standard RAG setup where models are trained based on the retriever outputs,\\nwhich is a mixture of both memorization and reading. At test time, all methods follow the\\nstandard RAG setting, provided with a top-k retrieved documents in the context.\\nthe exam. For LLMs, this is equivalent to the scenario, for example, in which the LLM is\\nused as a chatbot. In this scenario the LLM draws from the knowledge baked in during\\npre-training and supervised-finetuning to respond to the users’ prompt.\\nOpen Book Exam In contrast, we liken the open-book exam setting to the scenario in\\nwhich the LLM can refer to external sources of information (e.g., a website or a book chapter).\\nIn such scenarios, typically, the LLM is paired with retriever which retrieves ‘k’ documents'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='(or specific segments of the document) which are appended to the users’ prompt. It is\\nonly through these documents retrieved that the LLM gains access to “domain-specific\\ninformation”. As a result, we argue that the LLM’s performance in these settings, where it\\nis trained as a general-purpose LLM is largely dependent on the quality of the retriever and\\nhow accurately the retriever can identify the most relevant piece of information.\\nDomain-Specific Open-Book Exam In this paper, we focus on the narrower but increas-\\ningly popular domain than the general open book exam, which we call the domain-specific\\nopen-book exam. Here, we know apriori the domain in which the LLM will be tested. The\\nLLM can respond to the users’ prompt using use any and all information from this specific\\ndomain, which it has been fine-tuned on. Examples of domain specific examples include\\nenterprise documents, code repositories belonging to an organization, etc. In all these'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='scenarios, the LLM will be used to respond to the questions, whose answers can be found\\nwithin a collection of documents. The retrieval technique itself has little to no-impact on the\\nmechanism (though it may impact the accuracy). This paper studies the domain-specific\\nopen-book setting and how to adapt a pretrained LLM to this specific domain, including\\nhow to make it more robust to a varying number of retrieved documents and distractors.\\n3 RAFT\\nIn this section, we present RAFT, a novel way of training LLMs for domain-specific open-\\nbook exams. We first introduce the classical technique of supervised fine-tuning, followed\\nwith the key takeaways from our experiments. Then, we introduce RAFT , a modified\\nversion of general instruction tuning. Lastly, we provide an overview of the experiments to\\nexpect in the later sections.\\nSupervised Finetuning\\nConsider the supervised fine-tuning (SFT) setting for a Question-Answer dataset. The'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='formulation consists of the Dataset (D) from which a set of Question (Q) and corresponding\\nanswer (A) pairs are derived or already available. In the classical SFT setting, the model is\\ntrained to improve it’s ability to answer the questions based on it’s knowledge - obtained\\neither during pre-training, or during the SFT training phase. The model so trained can also\\n3'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content='Preprint, Under Review\\nbe used at test-time with Retrieval Augmented Generation (RAG) setting, where additional\\ndocuments can be introduced in the prompt to help the model answer the question. This\\ncan be represented as follows:\\n{Train: Q → A}, {0-shot Inference: Q → A}, {RAG Inference: Q + D → A}\\nRAFT: Retrieval Augmented Fine-Tuning (RAFT), presents a novel recipe to prepare fine-\\ntuning data to tailor the models for domain-specific open-book setting, equivalent to in-\\ndomain RAG In RAFT, we prepare the training data such that each data point contains a\\nquestion (Q), a set of documents (Dk), and a corresponding Chain-of-though style answer\\n(A∗) generated from one of the document ( D∗). We differentiate between two types of\\ndocuments: ‘golden’ documents ( D∗) i.e. the documents from which the answer to the\\nquestion can be deduced, and ‘distractor’ documents ( Di) that do not contain answer-\\nrelevant information. As an implementation detail, the ‘golden’ document doesn’t need to'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content='be a single document, but can be more than one document, as is the case in HotpotQA Yang\\net al. (2018). Then, for P fraction of the questions (qi) in the dataset, we retain the golden\\ndocument (d∗\\ni ) along with distractor documents (dk−1). For (1 − P) fraction of the questions\\n(qi) in the dataset, we include no golden document and only include distractor documents\\n(dk). We then fine-tune the language model using standard supervised training (SFT)\\ntechnique, training it to generate answers from the provided documents and question. Fig. 2\\nillustrates the high-level design principal for RAFT .\\nWe demonstrate that our RAG approach trains the model to perform better RAG on the set\\nof documents it is trained on i.e., in-domain. By removing the golden documents in some\\ninstances, we are compelling the model to memorize answers instead of deriving them from\\nthe context. The training data for RAFT is as follows, and an example training data can be\\nseen in Fig. 3:'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content='seen in Fig. 3:\\nP % of data: Q + D∗ + D1 + D2 + . . . + Dk → A∗\\n(1 − P) % of data: Q + D1 + D2 + . . . + Dk → A∗\\nSubsequently, for the test scenario, the model is provided with the Q and top-k documents\\nretrieved by the RAG pipeline. Note that RAFT is independent of the retriever used.\\nA key factor in enhancing training quality is the generation of a reasoning process, such\\nas Chain-of-Thought, to explain the provided answers. RAFT approach is similar: we\\ndemonstrate that creating a full reasoning chain and in-addition, clearly citing sources\\nenhances the model’s accuracy in answering questions. In Fig. 3, we illustrate this set-\\nup. Generating the training data in this fashion, involves presenting the model with a\\nquestion, context, and verified answers, and then requesting it to form a reasoning chain\\nthat appropriately references the original context.\\nFor all the datasets in our experiments, we generate the answers using the technique'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content='described above. Note that the Gorilla APIBench dataset, already includes reasoning\\nin the answers. We provide an example of the generation step in Fig. 3, the detailed\\nreasoning answer includes a citation from the original context inside ##begin_quote## and\\n##end_quote## as well as the detailed explanation on how to reach the conclusion based on\\nthe citations. We demonstrate that adding detailed reasoning paragraphs can help boost the\\nmodel’s performance in our experiment section.\\n4 Evaluation\\nWe design our experiments to study how well RAFT performs compared to various base-\\nlines. We find that the RAFT-7B model (a finetuned version of LlaMA-2) is better at reading\\nand extracting information from in-domain documents, than domain-specific finetuned\\nmodel, and general-purpose model with RAG. As an ablation, we also demonstrate how\\nimportant it is for the model to learn with Chain-of-Thought responses. In this section,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content='we will first introduce all the datasets we used in the experiments, then all the baseline\\nmodel/fine-tuning techniques that we benchmark against.\\n4'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5'}, page_content='Preprint, Under Review\\nQuestion: The Oberoi family is part of a hotel company that has a head office\\nin what city?\\ncontext: [The Oberoi family is an Indian family that is famous for its\\ninvolvement in hotels, namely through The Oberoi Group]...[It is located in\\ncity center of Jakarta, near Mega Kuningan, adjacent to the sister JW Marriott\\nHotel. It is operated by The Ritz-Carlton Hotel Company. The complex has two\\ntowers that comprises a hotel and the Airlangga Apartment respectively]...[The\\nOberoi Group is a hotel company with its head office in Delhi.]\\nInstruction: Given the question, context and answer above, provide a logical\\nreasoning for that answer. Please use the format of: ##Reason: {reason}\\n##Answer: {answer}.\\n--------------------------------------------------------------------------------------\\nCoT Answer: ##Reason: The document ##begin_quote## The Oberoi family is an\\nIndian family that is famous for its involvement in hotels, namely through The'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5'}, page_content='Oberoi Group. ##end_quote## establishes that the Oberoi family is involved in\\nthe Oberoi group, and the document ##begin_quote## The Oberoi Group is a hotel\\ncompany with its head office in Delhi. ##end_quote## establishes the head\\noffice of The Oberoi Group. Therefore, the Oberoi family is part of a hotel\\ncompany whose head office is in Delhi. ##Answer: Delhi\\nFigure 3: RAFT prompt to help LLM evaluate its own generated reasoning and answers,\\ncontrasting them with the correct reasoning and answers. The LLM is prompted to identify\\nerrors in its reasoning and extract key insights for improvement. This figure specifically\\nrepresents the ‘GenerateExplanation‘ step in the RAFT algorithm (Section 3).\\nTable 1: RAFT improves RAG performance for all specialized domains: Across PubMed,\\nHotPot, HuggingFace, Torch Hub, and Tensorflow Hub, we see that Domain-specific Fine-\\ntuning improves significantly of the performance of the base model, RAFT consistently'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5'}, page_content='outperforms the existing domain-specific finetuning method with or without RAG. This\\nsuggests the need to train the model with context. We compare our model with LLaMA\\nfinetuning receipes, and provide GPT-3.5 for reference.\\nPubMed HotPot HuggingFace Torch Hub TensorFlow\\nGPT-3.5 + RAG 71.60 41.5 29.08 60.21 65.59\\nLLaMA2-7B 56.5 0.54 0.22 0 0\\nLLaMA2-7B + RAG 58.8 0.03 26.43 08.60 43.06\\nDSF 59.7 6.38 61.06 84.94 86.56\\nDSF + RAG 71.6 4.41 42.59 82.80 60.29\\nRAFT (LLaMA2-7B) 73.30 35.28 74.00 84.95 86.86\\nDatasets In our experiments, we use the following datasets to evaluate our model and\\nall baselines. We selected these datasets to represent both popular and diverse domains\\nincluding Wikipedia, Coding/API documents, and question-answering on medical docu-\\nments. Natural Questions (NQ) Kwiatkowski et al. (2019), Trivia QA Joshi et al. (2017) and\\nHotpotQA Yang et al. (2018) are the open-domain question-answers based on Wikipedia,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5'}, page_content='mainly focused on common knowledge (e.g., movies, sports, etc). HuggingFace, Torch Hub,\\nand TensorFlow Hub are from the APIBench Patil et al. (2023) proposed in the Gorilla paper.\\nThese benchmarks measure how to generate the correct, functional, and executable API\\ncalls based on the documentation. PubMed QA Jin et al. (2019) is a question-answering\\ndataset tailored only for biomedical-research question-answering. It mainly focuses on\\nanswering medical and biology questions based on a given set of documents. We would\\n5'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='Preprint, Under Review\\nlike to highlight that (NQ, Trivia QA, and HotpotQA) are relatively general domain whereas\\nthe latter two domains are on domain-specific documents.\\nBaselines We consider the following baselines for our experiments:\\n• LlaMA2-7B-chat model with 0-shot prompting: this is the commonly used\\ninstruction-finetuned model for QA tasks, where we provide clearly written instruc-\\ntions, but no reference documentation.\\n• LlaMA2-7B-chat model with RAG (Llama2 + RAG): similar to the previous setting,\\nexcept here we include reference documents. This is a popular technique when\\ndealing with domain-specific QA tasks.\\n• Domain-Specific Finetuning with 0-shot prompting (DSF): Standard supervised-\\nfinetuning, without documents in context. We find that its mostly useful to align\\nthe answering style of the model as well as get familiar with the domain context.\\n• Domain-Specific Finetuning with RAG (DSF + RAG): Equip a domain-specific'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='finetuned-model with external knowledge using RAG. So, for the “knowledge” the\\nmodel does not know, it can still refer to the context.\\n4.1 Results\\nUsing the above datasets and baselines, we evaluate our model RAFT and demonstrate\\nthe effectiveness of RAFT in Tab. 1. We see that RAFT consistently and significantly\\noutperforms the baselines. Compared with the base Llama-2 instruction-tuned model,\\nRAFT with RAG does much better in terms of extracting information as well as being\\nrobust towards distractors. The gain can be as big as 35.25% on Hotpot QA and 76.35% on\\nTorch Hub evaluation. Compared with DSF on the specific dataset, our model does better at\\nrelying on the provided context to solve the problem. RAFT does much better on the tasks\\nlike Hotpot and HuggingFace datasets (30.87% on Hotpot and 31.41% on HuggingFace).\\nNote that for PubMed QA, since it is a binary yes/no question, we don’t observe significant'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='gains when we compare our model with DSF + RAG. Even compared with a much larger\\nand better model GPT-3.5, RAFT demonstrates significant advantages.\\nOverall, the LLaMA-7B model, both with and without the RAG, performs poorly due to its\\nanswering style not aligning with the ground truth. By applying domain-specific tuning,\\nwe significantly enhance its performance. This process enables the model to learn and adopt\\nthe appropriate style of answering. However, introducing RAG to a domain-specifically\\nfine-tuned (DSF) model doesn’t invariably lead to better outcomes. This might indicate that\\nthe model lacks training in context processing and extracting useful information from it. By\\nincorporating our method, RAFT , we train the model not only to match its answering style\\nwith that required but also to improve its document processing capabilities. Consequently,\\nour approach outperforms all others.\\n4.2 Effect of CoT'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='4.2 Effect of CoT\\nWe also conduct an analysis to evaluate the effectiveness of the Chain-of-Thought approach\\nin enhancing the model’s performance. As indicated in Table 2, simply providing the answer\\nto a question may not always be adequate. This approach can lead to a rapid decrease\\nin loss, resulting in the model beginning to overfit. Incorporating a reasoning chain that\\nnot only guides the model to the answer but also enriches the model’s understanding can\\nimprove the overall accuracy and prevent overfitting to concise answers. In our experiments,\\nintegrating the Chain-of-Thought significantly enhances training robustness. We employ\\nGPT-4-1106 to generate our Chain-of-Thought prompts and include an example of the\\nprompt we used in Figure 3.\\n4.3 Qualitative Analysis\\nTo illustrate the potential advantages of RAFT over the domain-specifically fine-tuned\\n(DSF) approach, we present a comparative example in Figure 4. This example qualitatively\\n6'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7'}, page_content='Preprint, Under Review\\nTable 2: Ablation on Chain-of-Thought : The numbers of RAFT and RAFT without\\nCoT. Results on various datasets show that adding CoT can significantly improve the\\nperformance of the finetuned model. With a gains of 9.66% and 14.93% in the Hotpot QA\\nand HuggingFace datasets respectively.\\nPubMed HotpotQA HuggingFace Torch Hub TensorFlow\\nRAFT w.o CoT 68.30 25.62 59.07 86.56 83.21\\nRAFT 73.30 35.28 74.00 84.95 86.86\\ndemonstrates a scenario where the DSF model becomes confused by a question asking for\\nthe identity of a screenwriter. Instead of providing the correct name, it mistakenly cites one\\nof the films written by the screenwriter. In contrast, the RAFT model accurately answers the\\nquestion. This discrepancy suggests that training a model solely with question-answer pairs\\nmay impair its ability to derive relevant context from provided documents. The comparison\\nunderscores the importance of incorporating both standard instructional tuning and context'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7'}, page_content='comprehension into the training dataset to preserve and enhance the model’s ability to\\nprocess text effectively.\\nHotPot QA\\nQuestion: What screenwriter with credits for “Evolution”[0/1879]e a film starring\\nNicolas Cage and Téa Leoni?\\nDocuments: . . . David Weissman is a screenwriter and director. His film credits\\ninclude “The Family Man” (2000), “Evolution” (2001), and “When in Rome” (2010).\\n. . .\\nThe Family Man is a 2000 American romantic comedy-drama film directed by Brett\\nRatner, written by David Diamond and David Weissman, and starringNicolas Cage\\nand Téa Leoni. . . .\\n—————————————————————————————————————\\n—————————–\\nRAFT: ##Reason: The screenwriter with credits for the film “Evolution,” starring\\nNicolas Cage and Téa Leoni, is David Weissman. This information is provided in the\\nreference documents where it mentions David Weissman as a screenwriter with film\\ncredits including “The Family Man” (2000), “Evolution” (2001), and “When in Rome”'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7'}, page_content='(2010). Therefore, the screenwriter for “Evolution” is David Weissman. ##Answer:\\nDavid Weissman ✓\\nDSF: The Family Man ✗\\nFigure 4: Comparison of RAFT and DSF : On the HotPot QA dataset, we can see that DSF\\nmodel extracts the wrong information from the context when the question is asking who is\\nthe screen writer and it answers a film name. RAFT manages to get the accurate results .\\n4.4 Should we train the LLM always with the golden context for RAG?\\nIn our exploration of whether large language models (LLMs) should always be trained with\\nthe golden context for Retrieval-Augmented Generation (RAG), we address a key question:\\nwhat proportion (p%) of the training data should include golden documents? Intuitively,\\none might assume that for effective training in reading and extracting information from\\ncontext (e.g., RAG tasks), the golden document should always be included during training\\n(P = 100%). However, our findings challenge this assumption: incorporating a portion of'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7'}, page_content='the training data without the golden document in the context (P = 80%) appears to enhance\\nthe model’s performance on RAG tasks.\\n7'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='Preprint, Under Review\\n0 20 40 60 80 100\\nP % Golden Retrieved Context at Training\\n0.25\\n0.30\\n0.35\\n0.40\\n0.45Final Accuracy\\nT est Domain: NQ\\n0 20 40 60 80 100\\n% Golden Retrieved Context at Training\\n0.50\\n0.55\\n0.60\\n0.65Final Accuracy\\nT est Domain: TQA\\n0 20 40 60 80 100\\nP % Golden Retrieved Context at Training\\n0.40\\n0.45\\n0.50\\n0.55\\n0.60Final Accuracy\\nT est Domain: HoPo\\nFigure 5: How many golden documents to involve? We study the hyperparameter P%\\nwhere it indicates how much portion of training data is with golden document. Results\\non NQ, TQA and HotpotQA suggest that mixing some amount of data that the golden\\ndocument is not put in the context is helpful for in-domain RAG.\\nFigure 5 presents our investigation into the hyperparameter P%, which represents the\\npercentage of training instances that should include golden documents. We find that the\\noptimal proportion varies across datasets, with P% ranging from 40%, 60%, and 100%. This'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='indicates that training your LLM without the correct corresponding context at times can be\\nbeneficial for the downstream task of answering questions related to the documents. In our\\ntraining setup, we include four distractor documents alongside the golden document, and at\\ntest time, we maintain this format by providing the golden document with four distractors.\\nOur findings suggest that, for domain-specific RAG tasks, including a certain percentage of\\ntraining data without the golden documents in the context proves to be advantageous.\\n5 RAFT Generalizes to Top-K RAG\\nWe now study another important problem: How does the number of distractor documents\\nin RAFT affect the model’s performance when augmented with top-k RAG results during\\nevaluation? Previous research has highlighted the vulnerability of LLMs to irrelevant text\\n(see studies (Shi et al., 2023a; Weston & Sukhbaatar, 2023; Liu et al., 2023)). This issue is'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='particularly critical for LLMs + RAG since top-k RAG is frequently employed at test time to\\nensure high recall. Such a scenario necessitates the model to have the ability to discern and\\ndisregard irrelevant content, focusing solely on pertinent information.\\n5.1 Making Model Robust to top-K RAG\\nTo tackle the challenge of enhancing large language models’ (LLMs) ability to sift through\\nirrelevant text within the retrieval pipeline, our analysis revealed that training solely with\\ngolden (highly relevant) documents can inadvertently diminish the model’s ability to dis-\\ncern and disregard irrelevant information. To address this, our algorithm, RAFT , adopts\\na strategy that integrates golden documents with a mix of irrelevant ones. This method-\\nology prompts us to investigate the ideal fraction of distractor (irrelevant) documents to\\nincorporate throughout the training process and to assess how well this training approach'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='adapts to different volumes of documents encountered by the Retrieval-Augmented Gen-\\neration (RAG) during the test phase. Our aim is to refine the balance between relevant\\nand irrelevant information to strenghten the model’s efficiency in identifying and utilizing\\npertinent content. Notice that Sec 4.4 looked what what P% of training data should include\\ndistractors, while in this section, we study test-time scenarios.\\nTraining with Distractor Documents To enhance the robustness of LLMs against irrelevant\\ntext in retrieved documents, we adopted a finetuning approach that incorporates both\\ngolden (highly relevant) documents and distractor (irrelevant) documents. The model was\\ntrained with varying numbers of distractor documents, but consistently evaluated using\\nthe top-3 documents obtained from the retriever - not to be confused with p. Our findings,\\ndetailed in Fig. 6, reveal that finetuning with only the golden document frequently results in'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='inferior performance compared to configurations that include a greater number of distractor\\ndocuments. As we can see in the figure, the better performance for Natural Questions is\\n8'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='Preprint, Under Review\\n2 4 6 8 10\\n# T est Documents (T op-k)\\n0.22\\n0.24\\n0.26\\n0.28\\n0.30\\n0.32Final Accuracy\\nNatural Questions\\nTrain D*\\nTrain D* + 1D\\nTrain D* + 2D\\nTrain D* + 3D\\n2 4 6 8 10\\n# T est Documents (T op-k)\\n0.125\\n0.150\\n0.175\\n0.200\\n0.225\\n0.250Final Accuracy\\nHotpot QA\\nTrain D*\\nTrain D* + 1D\\nTrain D* + 2D\\nTrain D* + 3D\\nFigure 6: Test-Time Documents Varying: To analyze how robust RAFT is to varying number\\nof test-time documents, we study three domains – NQ, Trivia QA and HotPot QA. In NQ,\\nwe find that training with 4 documents leads to optimal performance, and this changes to 3\\nand 2 for for Trivia QA and HotPot QA respectively. However, we see that training with\\nonly golden documents leads to poor performance.\\ntraining with D∗ + 3D and it is D∗ + 1D documents with Hotpot QA. This insight has been\\nparticularly beneficial for our algorithm, RAFT . In our experiments, we consistently employ\\na training setup consisting of one golden document alongside four distractor documents.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='Generalization to a variable number of test-time documents. We extended our research\\nto examine the impact of different quantities of test-time documents on the model’s per-\\nformance. Specifically, our experiments focused on assessing how models, trained with\\nvarying numbers of distractor documents, respond to changes in the number of documents\\npresented at test time. The results, illustrated in Fig. 6, confirm that the inclusion of distrac-\\ntor documents during training indeed makes the model more resilient to fluctuations in the\\nnumber of documents encountered during testing. This ability to maintain consistent perfor-\\nmance despite variations in test-time document numbers further validates the robustness of\\nour approach, RAFT . This finding underscores the importance of a well-calibrated training\\nenvironment to prepare the model for a range of scenarios it may encounter in real-world.\\n6 Related Works\\nRetrieval-Augmented Language Models Retrieval-Augmented Language Models (RALMs)'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='enhance LLMs by integrating a retrieval module that sources relevant information from\\nexternal knowledge bases, significantly improving performance across various NLP tasks,\\nincluding language modeling (Guu et al., 2020; Borgeaud et al., 2022; Khandelwal et al.,\\n2019; Shi et al., 2023d; Lin et al., 2023b; Shi et al., 2023c; Asai et al., 2023; Xu et al., 2023;\\nWang et al., 2023) and open-domain question answering (Izacard et al., 2023; Lewis et al.,\\n2020). For instance, Atlas (Izacard et al., 2023) fine-tunes T5 models with the retriever,\\ntreating documents as latent variables, while RETRO (Borgeaud et al., 2022) modifies the\\ndecoder-only architecture to include retrieved texts and conducts pre-training from scratch.\\nkNN-LM (Khandelwal et al., 2019) interpolates between the LM’s next token distribution\\nand distributions computed from retrieved tokens at inference. (Shi et al., 2023d; Ram\\net al., 2023) assume black-box access to an LLM, combining it with either off-the-shelf or'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='fine-tuned retriever.\\nMemorization A key question around large neural language models is whether they truly\\n“understand” text (Feldman, 2020; Power et al., 2022) or simply rely on surface pattern\\nmemorization (Carlini et al., 2019; Tänzer et al., 2022). (Feldman, 2020; Carlini et al., 2019;\\n2022) develop methodologies to quantify the extent of memorization in neural models.\\n(Brown et al., 2020; Power et al., 2022; Liu et al., 2022) further explored how memorization\\nimpacts the models’ generalization capabilities. (Carlini et al., 2021; Shi et al., 2023b)\\ndemonstrated the ability of language models to memorize and regurgitate training data,\\nraising significant privacy concerns (Kandpal et al., 2022; Pan et al., 2020).\\nFinetuning for RAG More recently, several papers have been exploring the idea of fine-\\ntuning a pretrained LLM to be better at RAG tasks (Lin et al., 2023a; Wang et al., 2023; Xu\\n9'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content='Preprint, Under Review\\net al., 2023; Liu et al., 2024). These works focus on constructing a combination of finetuning\\ndataset for RAG and train a model to perform well on these tasks. In particular, in their\\nsettings, at test time, the domain or documents can be different than the training time;\\nwhereas our paper studies a slightly opposite scenario where we only care about testing the\\nLLM on the same set of documents.\\n7 Conclusion\\nRAFT is a training strategy designed to enhance the model’s performance in answering\\nquestions within a specific domain, in \"open-book\" settings. We highlight several crucial\\ndesign decisions, such as training the model alongside distractor documents, organizing the\\ndataset so a portion lacks golden documents in their context, and formulating answers in a\\nchain-of-thought manner with direct quotations from the relevant text. Our evaluations on\\nPubMed, HotpotQA, and Gorilla API Bench underline RAFT’s significant potential.\\nReferences'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content='References\\nAnthropic. Prompt engineering for claude’s long context window. 2023.\\nAsai, A., Wu, Z., Wang, Y., Sil, A., and Hajishirzi, H. Self-rag: Learning to retrieve, generate,\\nand critique through self-reflection. arXiv preprint arXiv:2310.11511, 2023.\\nBorgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., Van Den Driess-\\nche, G. B., Lespiau, J.-B., Damoc, B., Clark, A., et al. Improving language models by\\nretrieving from trillions of tokens. In International conference on machine learning, pp.\\n2206–2240. PMLR, 2022.\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P ., Neelakantan, A.,\\nShyam, P ., Sastry, G., Askell, A., et al. Language models are few-shot learners.Advances\\nin neural information processing systems, 33:1877–1901, 2020.\\nCarlini, N., Liu, C., Erlingsson, Ú., Kos, J., and Song, D. The secret sharer: Evaluating and\\ntesting unintended memorization in neural networks. In 28th USENIX Security Symposium'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content='(USENIX Security 19), pp. 267–284, 2019.\\nCarlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., Roberts, A.,\\nBrown, T., Song, D., Erlingsson, U., et al. Extracting training data from large language\\nmodels. In 30th USENIX Security Symposium (USENIX Security 21), pp. 2633–2650, 2021.\\nCarlini, N., Ippolito, D., Jagielski, M., Lee, K., Tramer, F., and Zhang, C. Quantifying\\nmemorization across neural language models. In The Eleventh International Conference on\\nLearning Representations, 2022.\\nDernoncourt, F. and Lee, J. Y. Pubmed 200k rct: a dataset for sequential sentence classification\\nin medical abstracts. arXiv preprint arXiv:1710.06071, 2017.\\nFeldman, V . Does learning require memorization? a short tale about a long tail. InProceedings\\nof the 52nd Annual ACM SIGACT Symposium on Theory of Computing, pp. 954–959, 2020.\\nGuu, K., Lee, K., Tung, Z., Pasupat, P ., and Chang, M. Retrieval augmented language model'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content='pre-training. In International conference on machine learning, pp. 3929–3938. PMLR, 2020.\\nIzacard, G., Lewis, P ., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J.,\\nJoulin, A., Riedel, S., and Grave, E. Atlas: Few-shot learning with retrieval augmented\\nlanguage models. Journal of Machine Learning Research, 24(251):1–43, 2023. URL http:\\n//jmlr.org/papers/v24/23-0037.html.\\nJin, Q., Dhingra, B., Liu, Z., Cohen, W. W., and Lu, X. Pubmedqa: A dataset for biomedical\\nresearch question answering. arXiv preprint arXiv:1909.06146, 2019.\\n10'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='Preprint, Under Review\\nJoshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551,\\n2017.\\nKandpal, N., Wallace, E., and Raffel, C. Deduplicating training data mitigates privacy risks\\nin language models. In International Conference on Machine Learning, pp. 10697–10707.\\nPMLR, 2022.\\nKhandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., and Lewis, M. General-\\nization through memorization: Nearest neighbor language models. arXiv preprint\\narXiv:1911.00172, 2019.\\nKwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D.,\\nPolosukhin, I., Devlin, J., Lee, K., et al. Natural questions: a benchmark for question\\nanswering research. Transactions of the Association for Computational Linguistics, 7:453–466,\\n2019.\\nLazaridou, A., Gribovskaya, E., Stokowiec, W., and Grigorev, N. Internet-augmented'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='language models through few-shot prompting for open-domain question answering.\\narXiv preprint arXiv:2203.05115, 2022.\\nLewis, P ., Perez, E., Piktus, A., Petroni, F., Karpukhin, V ., Goyal, N., Küttler, H., Lewis, M.,\\nYih, W.-t., Rocktäschel, T., et al. Retrieval-augmented generation for knowledge-intensive\\nnlp tasks. Advances in Neural Information Processing Systems, 33:9459–9474, 2020.\\nLin, X. V ., Chen, X., Chen, M., Shi, W., Lomeli, M., James, R., Rodriguez, P ., Kahn, J., Szilvasy,\\nG., Lewis, M., et al. Ra-dit: Retrieval-augmented dual instruction tuning. arXiv preprint\\narXiv:2310.01352, 2023a.\\nLin, X. V ., Chen, X., Chen, M., Shi, W., Lomeli, M., James, R., Rodriguez, P ., Kahn, J., Szilvasy,\\nG., Lewis, M., et al. Ra-dit: Retrieval-augmented dual instruction tuning. arXiv preprint\\narXiv:2310.01352, 2023b.\\nLiu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P . Lost'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172,\\n2023.\\nLiu, Z., Kitouni, O., Nolte, N. S., Michaud, E., Tegmark, M., and Williams, M. Towards\\nunderstanding grokking: An effective theory of representation learning. Advances in\\nNeural Information Processing Systems, 35:34651–34663, 2022.\\nLiu, Z., Ping, W., Roy, R., Xu, P ., Shoeybi, M., and Catanzaro, B. Chatqa: Building gpt-4 level\\nconversational qa models. arXiv preprint arXiv:2401.10225, 2024.\\nPan, X., Zhang, M., Ji, S., and Yang, M. Privacy risks of general-purpose language models.\\nIn 2020 IEEE Symposium on Security and Privacy (SP), pp. 1314–1331. IEEE, 2020.\\nPatil, S. G., Zhang, T., Wang, X., and Gonzalez, J. E. Gorilla: Large language model connected\\nwith massive apis. arXiv preprint arXiv:2305.15334, 2023.\\nPower, A., Burda, Y., Edwards, H., Babuschkin, I., and Misra, V . Grokking: Generalization\\nbeyond overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177, 2022.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='Ram, O., Levine, Y., Dalmedigos, I., Muhlgay, D., Shashua, A., Leyton-Brown, K.,\\nand Shoham, Y. In-context retrieval-augmented language models. arXiv preprint\\narXiv:2302.00083, 2023.\\nShi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, E. H., Schärli, N., and Zhou, D. Large\\nlanguage models can be easily distracted by irrelevant context. In International Conference\\non Machine Learning, pp. 31210–31227. PMLR, 2023a.\\nShi, W., Ajith, A., Xia, M., Huang, Y., Liu, D., Blevins, T., Chen, D., and Zettlemoyer, L.\\nDetecting pretraining data from large language models. arXiv preprint arXiv:2310.16789,\\n2023b.\\n11'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12'}, page_content='Preprint, Under Review\\nShi, W., Min, S., Lomeli, M., Zhou, C., Li, M., Lin, V ., Smith, N. A., Zettlemoyer, L., Yih, S.,\\nand Lewis, M. In-context pretraining: Language modeling beyond document boundaries.\\narXiv preprint arXiv:2310.10638, 2023c.\\nShi, W., Min, S., Yasunaga, M., Seo, M., James, R., Lewis, M., Zettlemoyer, L., and Yih, W.-t.\\nReplug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652,\\n2023d.\\nTänzer, M., Ruder, S., and Rei, M. Memorisation versus generalisation in pre-trained lan-\\nguage models. In Proceedings of the 60th Annual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers), pp. 7564–7578, 2022.\\nVu, T., Iyyer, M., Wang, X., Constant, N., Wei, J., Wei, J., Tar, C., Sung, Y.-H., Zhou, D., Le,\\nQ., et al. Freshllms: Refreshing large language models with search engine augmentation.\\narXiv preprint arXiv:2310.03214, 2023.\\nWang, B., Ping, W., McAfee, L., Xu, P ., Li, B., Shoeybi, M., and Catanzaro, B. Instructretro:'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12'}, page_content='Instruction tuning post retrieval-augmented pretraining. arXiv preprint arXiv:2310.07713,\\n2023.\\nWang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H.\\nSelf-instruct: Aligning language models with self-generated instructions. arXiv preprint\\narXiv:2212.10560, 2022.\\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V ., Zhou, D., et al.\\nChain-of-thought prompting elicits reasoning in large language models. Advances in\\nNeural Information Processing Systems, 35:24824–24837, 2022.\\nWeston, J. and Sukhbaatar, S. System 2 attention (is something you might need too). arXiv\\npreprint arXiv:2311.11829, 2023.\\nXiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P ., Hou, R., Martin, L., Rungta, R.,\\nSankararaman, K. A., Oguz, B., et al. Effective long-context scaling of foundation models.\\narXiv preprint arXiv:2309.16039, 2023.\\nXu, P ., Ping, W., Wu, X., McAfee, L., Zhu, C., Liu, Z., Subramanian, S., Bakhturina, E.,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-06T01:09:34+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-06T01:09:34+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-3.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12'}, page_content='Shoeybi, M., and Catanzaro, B. Retrieval meets long context large language models. arXiv\\npreprint arXiv:2310.03025, 2023.\\nYang, Z., Qi, P ., Zhang, S., Bengio, Y., Cohen, W. W., Salakhutdinov, R., and Manning, C. D.\\nHotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint\\narXiv:1809.09600, 2018.\\nZhou, C., Liu, P ., Xu, P ., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P ., Yu, L., et al. Lima:\\nLess is more for alignment. arXiv preprint arXiv:2305.11206, 2023.\\n12'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1'}, page_content='Retrieval-Augmented Generation for\\nKnowledge-Intensive NLP Tasks\\nPatrick Lewis†‡, Ethan Perez⋆,\\nAleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\\nMike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†\\n†Facebook AI Research; ‡University College London; ⋆New York University;\\nplewis@fb.com\\nAbstract\\nLarge pre-trained language models have been shown to store factual knowledge\\nin their parameters, and achieve state-of-the-art results when ﬁne-tuned on down-\\nstream NLP tasks. However, their ability to access and precisely manipulate knowl-\\nedge is still limited, and hence on knowledge-intensive tasks, their performance\\nlags behind task-speciﬁc architectures. Additionally, providing provenance for their\\ndecisions and updating their world knowledge remain open research problems. Pre-\\ntrained models with a differentiable access mechanism to explicit non-parametric'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1'}, page_content='memory have so far been only investigated for extractive downstream tasks. We\\nexplore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation\\n(RAG) — models which combine pre-trained parametric and non-parametric mem-\\nory for language generation. We introduce RAG models where the parametric\\nmemory is a pre-trained seq2seq model and the non-parametric memory is a dense\\nvector index of Wikipedia, accessed with a pre-trained neural retriever. We com-\\npare two RAG formulations, one which conditions on the same retrieved passages\\nacross the whole generated sequence, and another which can use different passages\\nper token. We ﬁne-tune and evaluate our models on a wide range of knowledge-\\nintensive NLP tasks and set the state of the art on three open domain QA tasks,\\noutperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract\\narchitectures. For language generation tasks, we ﬁnd that RAG models generate'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1'}, page_content='more speciﬁc, diverse and factual language than a state-of-the-art parametric-only\\nseq2seq baseline.\\n1 Introduction\\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowl-\\nedge from data [47]. They can do so without any access to an external memory, as a parameterized\\nimplicit knowledge base [51, 52]. While this development is exciting, such models do have down-\\nsides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into\\ntheir predictions, and may produce “hallucinations” [38]. Hybrid models that combine parametric\\nmemory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these\\nissues because knowledge can be directly revised and expanded, and accessed knowledge can be\\ninspected and interpreted. REALM [ 20] and ORQA [ 31], two recently introduced models that\\ncombine masked language models [8] with a differentiable retriever, have shown promising results,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1'}, page_content='arXiv:2005.11401v4  [cs.CL]  12 Apr 2021'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 1, 'page_label': '2'}, page_content='The\\tDivine\\nComedy\\t(x) q \\nQuery \\nEncoder \\nq(x) \\nMIPS p θ \\nGenerator\\xa0pθ\\n(Parametric) \\nMargin- \\nalize \\nThis\\t14th\\tcentury\\twork\\nis\\tdivided\\tinto\\t3\\nsections:\\t\"Inferno\",\\n\"Purgatorio\"\\t&\\n\"Paradiso\"\\t\\t\\t\\t\\t\\t\\t\\t\\t(y)\\nEnd-to-End Backprop through q  and\\xa0p θ \\nBarack\\tObama\\twas\\nborn\\tin\\tHawaii.(x)\\nFact Veriﬁcation: Fact Query\\nsupports\\t(y)\\nQuestion Generation\\nFact Veriﬁcation:\\nLabel Generation\\nDocument \\nIndex \\nDefine\\t\"middle\\tear\"(x)\\nQuestion Answering:\\nQuestion Query\\nThe\\tmiddle\\tear\\tincludes\\nthe\\ttympanic\\tcavity\\tand\\nthe\\tthree\\tossicles.\\t\\t(y)\\nQuestion Answering:\\nAnswer GenerationRetriever pη \\n(Non-Parametric) \\nz 4 \\nz 3 \\nz 2 \\nz 1 \\nd(z) \\nJeopardy Question\\nGeneration:\\nAnswer Query\\nFigure 1: Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document\\nIndex) with a pre-trained seq2seq model (Generator) and ﬁne-tune end-to-end. For query x, we use\\nMaximum Inner Product Search (MIPS) to ﬁnd the top-K documents zi. For ﬁnal prediction y, we'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 1, 'page_label': '2'}, page_content='treat zas a latent variable and marginalize over seq2seq predictions given different documents.\\nbut have only explored open-domain extractive question answering. Here, we bring hybrid parametric\\nand non-parametric memory to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models.\\nWe endow pre-trained, parametric-memory generation models with a non-parametric memory through\\na general-purpose ﬁne-tuning approach which we refer to as retrieval-augmented generation (RAG).\\nWe build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the\\nnon-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural\\nretriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The\\nretriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents conditioned on\\nthe input, and the seq2seq model (BART [32]) then conditions on these latent documents together with'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 1, 'page_label': '2'}, page_content='the input to generate the output. We marginalize the latent documents with a top-K approximation,\\neither on a per-output basis (assuming the same document is responsible for all tokens) or a per-token\\nbasis (where different documents are responsible for different tokens). Like T5 [51] or BART, RAG\\ncan be ﬁne-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.\\nThere has been extensive previous work proposing architectures to enrich systems with non-parametric\\nmemory which are trained from scratch for speciﬁc tasks, e.g. memory networks [ 64, 55], stack-\\naugmented networks [25] and memory layers [ 30]. In contrast, we explore a setting where both\\nparametric and non-parametric memory components are pre-trained and pre-loaded with extensive\\nknowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is\\npresent without additional training.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 1, 'page_label': '2'}, page_content='Our results highlight the beneﬁts of combining parametric and non-parametric memory with genera-\\ntion for knowledge-intensive tasks—tasks that humans could not reasonably be expected to perform\\nwithout access to an external knowledge source. Our RAG models achieve state-of-the-art results\\non open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform\\nrecent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being\\nextractive tasks, we ﬁnd that unconstrained generation outperforms previous extractive approaches.\\nFor knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question\\ngeneration, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and\\ndiverse than a BART baseline. For FEVER [56] fact veriﬁcation, we achieve results within 4.3% of\\nstate-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 1, 'page_label': '2'}, page_content='the non-parametric memory can be replaced to update the models’ knowledge as the world changes.1\\n2 Methods\\nWe explore RAG models, which use the input sequencexto retrieve text documents zand use them\\nas additional context when generating the target sequence y. As shown in Figure 1, our models\\nleverage two components: (i) a retriever pη(z|x) with parameters ηthat returns (top-K truncated)\\ndistributions over text passages given a query xand (ii) a generator pθ(yi|x,z,y 1:i−1) parametrized\\n1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-\\ners Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/\\nexamples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/\\n2'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 2, 'page_label': '3'}, page_content='by θthat generates a current token based on a context of the previous i−1 tokens y1:i−1, the original\\ninput xand a retrieved passage z.\\nTo train the retriever and generator end-to-end, we treat the retrieved document as a latent variable.\\nWe propose two models that marginalize over the latent documents in different ways to produce a\\ndistribution over generated text. In one approach, RAG-Sequence, the model uses the same document\\nto predict each target token. The second approach, RAG-Token, can predict each target token based\\non a different document. In the following, we formally introduce both models and then describe the\\npη and pθ components, as well as the training and decoding procedure.\\n2.1 Models\\nRAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate\\nthe complete sequence. Technically, it treats the retrieved document as a single latent variable that\\nis marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 2, 'page_label': '3'}, page_content='top K documents are retrieved using the retriever, and the generator produces the output sequence\\nprobability for each document, which are then marginalized,\\npRAG-Sequence(y|x) ≈\\n∑\\nz∈top-k(p(·|x))\\npη(z|x)pθ(y|x,z) =\\n∑\\nz∈top-k(p(·|x))\\npη(z|x)\\nN∏\\ni\\npθ(yi|x,z,y 1:i−1)\\nRAG-Token Model In the RAG-Token model we can draw a different latent document for each\\ntarget token and marginalize accordingly. This allows the generator to choose content from several\\ndocuments when producing an answer. Concretely, the top K documents are retrieved using the\\nretriever, and then the generator produces a distribution for the next output token for each document,\\nbefore marginalizing, and repeating the process with the following output token, Formally, we deﬁne:\\npRAG-Token(y|x) ≈\\nN∏\\ni\\n∑\\nz∈top-k(p(·|x))\\npη(z|x)pθ(yi|x,z,y 1:i−1)\\nFinally, we note that RAG can be used for sequence classiﬁcation tasks by considering the target class'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 2, 'page_label': '3'}, page_content='as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.\\n2.2 Retriever: DPR\\nThe retrieval component pη(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:\\npη(z|x) ∝exp\\n(\\nd(z)⊤q(x)\\n)\\nd(z) =BERTd(z), q(x) =BERTq(x)\\nwhere d(z) is a dense representation of a document produced by a BERTBASE document encoder [8],\\nand q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating\\ntop-k(pη(·|x)), the list of kdocuments zwith highest prior probability pη(z|x), is a Maximum Inner\\nProduct Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use\\na pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This\\nretriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and\\nNatural Questions [29]. We refer to the document index as the non-parametric memory.\\n2.3 Generator: BART'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 2, 'page_label': '3'}, page_content='2.3 Generator: BART\\nThe generator component pθ(yi|x,z,y 1:i−1) could be modelled using any encoder-decoder. We use\\nBART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input\\nxwith the retrieved content zwhen generating from BART, we simply concatenate them. BART was\\npre-trained using a denoising objective and a variety of different noising functions. It has obtained\\nstate-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5\\nmodels [32]. We refer to the BART generator parameters θas the parametric memory henceforth.\\n2.4 Training\\nWe jointly train the retriever and generator components without any direct supervision on what\\ndocument should be retrieved. Given a ﬁne-tuning training corpus of input/output pairs (xj,yj), we\\n3'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 3, 'page_label': '4'}, page_content='minimize the negative marginal log-likelihood of each target, ∑\\nj−log p(yj|xj) using stochastic\\ngradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as\\nit requires the document index to be periodically updated as REALM does during pre-training [20].\\nWe do not ﬁnd this step necessary for strong performance, and keep the document encoder (and\\nindex) ﬁxed, only ﬁne-tuning the query encoder BERTq and the BART generator.\\n2.5 Decoding\\nAt test time, RAG-Sequence and RAG-Token require different ways to approximatearg maxyp(y|x).\\nRAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera-\\ntor with transition probability: p′\\nθ(yi|x,y1:i−1) = ∑\\nz∈top-k(p(·|x)) pη(zi|x)pθ(yi|x,zi,y1:i−1) To\\ndecode, we can plug p′\\nθ(yi|x,y1:i−1) into a standard beam decoder.\\nRAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 3, 'page_label': '4'}, page_content='token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for\\neach document z, scoring each hypothesis using pθ(yi|x,z,y 1:i−1). This yields a set of hypotheses\\nY, some of which may not have appeared in the beams of all documents. To estimate the probability\\nof an hypothesis y we run an additional forward pass for each document z for which y does not\\nappear in the beam, multiply generator probability with pη(z|x) and then sum the probabilities across\\nbeams for the marginals. We refer to this decoding procedure as “Thorough Decoding.” For longer\\noutput sequences, |Y|can become large, requiring many forward passes. For more efﬁcient decoding,\\nwe can make a further approximation that pθ(y|x,zi) ≈0 where ywas not generated during beam\\nsearch from x,zi. This avoids the need to run additional forward passes once the candidate set Y has\\nbeen generated. We refer to this decoding procedure as “Fast Decoding.”\\n3 Experiments'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 3, 'page_label': '4'}, page_content='3 Experiments\\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use\\na single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and\\nKarpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint\\n100-word chunks, to make a total of 21M documents. We use the document encoder to compute an\\nembedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical\\nNavigable Small World approximation for fast retrieval [37]. During training, we retrieve the top\\nkdocuments for each query. We consider k∈{5,10}for training and set kfor test time using dev\\ndata. We now discuss experimental details for each task.\\n3.1 Open-domain Question Answering\\nOpen-domain question answering (QA) is an important real-world application and common testbed\\nfor knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x,y)'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 3, 'page_label': '4'}, page_content='and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to\\nthe popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved\\ndocuments, relying primarily on non-parametric knowledge. We also compare to “Closed-Book\\nQA” approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead\\nrelying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural\\nQuestions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As\\nCT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG\\nmodel. We use the same train/dev/test splits as prior work [ 31, 26] and report Exact Match (EM)\\nscores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.\\n3.2 Abstractive Question Answering\\nRAG models can go beyond simple extractive QA and answer questions with free-form, abstractive'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 3, 'page_label': '4'}, page_content='text generation. To test RAG’s natural language generation (NLG) in a knowledge-intensive setting,\\nwe use the MSMARCO NLG task v2.1 [ 43]. The task consists of questions, ten gold passages\\nretrieved from a search engine for each question, and a full sentence answer annotated from the\\nretrieved passages. We do not use the supplied passages, only the questions and answers, to treat\\n4'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 4, 'page_label': '5'}, page_content='MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be\\nanswered in a way that matches the reference answer without access to the gold passages, such as\\n“What is the weather in V olcano, CA?” so performance will be lower without using gold passages.\\nWe also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here,\\nRAG can rely on parametric knowledge to generate reasonable responses.\\n3.3 Jeopardy Question Generation\\nTo evaluate RAG’s generation abilities in a non-QA setting, we study open-domain question gen-\\neration. Rather than use questions from standard open-domain QA tasks, which typically consist\\nof short, simple questions, we propose the more demanding task of generating Jeopardy questions.\\nJeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity.\\nFor example, “The World Cup” is the answer to the question “In 1986 Mexico scored as the ﬁrst'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 4, 'page_label': '5'}, page_content='country to host this international sports competition twice.” As Jeopardy questions are precise,\\nfactual statements, generating Jeopardy questions conditioned on their answer entities constitutes a\\nchallenging knowledge-intensive generation task.\\nWe use the splits from SearchQA [ 10], with 100K train, 14K dev, and 27K test examples. As\\nthis is a new task, we train a BART model for comparison. Following [67], we evaluate using the\\nSQuAD-tuned Q-BLEU-1 metric [ 42]. Q-BLEU is a variant of BLEU with a higher weight for\\nmatching entities and has higher correlation with human judgment for question generation than\\nstandard metrics. We also perform two human evaluations, one to assess generation factuality, and\\none for speciﬁcity. We deﬁne factuality as whether a statement can be corroborated by trusted external\\nsources, and speciﬁcity as high mutual dependence between the input and output [ 33]. We follow'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 4, 'page_label': '5'}, page_content='best practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two\\ngenerated questions, one from BART and one from RAG. They are then asked to pick one of four\\noptions—quuestion A is better, question B is better, both are good, or neither is good.\\n3.4 Fact Veriﬁcation\\nFEVER [ 56] requires classifying whether a natural language claim is supported or refuted by\\nWikipedia, or whether there is not enough information to decide. The task requires retrieving\\nevidence from Wikipedia relating to the claim and then reasoning over this evidence to classify\\nwhether the claim is true, false, or unveriﬁable from Wikipedia alone. FEVER is a retrieval problem\\ncoupled with an challenging entailment reasoning task. It also provides an appropriate testbed for\\nexploring the RAG models’ ability to handle classiﬁcation rather than generation. We map FEVER\\nclass labels (supports, refutes, or not enough info) to single output tokens and directly train with'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 4, 'page_label': '5'}, page_content='claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on\\nretrieved evidence. In many real-world applications, retrieval supervision signals aren’t available, and\\nmodels that do not require such supervision will be applicable to a wider range of tasks. We explore\\ntwo variants: the standard 3-way classiﬁcation task (supports/refutes/not enough info) and the 2-way\\n(supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.\\n4 Results\\n4.1 Open-domain Question Answering\\nTable 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA\\ntasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines\\nthe generation ﬂexibility of the “closed-book” (parametric only) approaches and the performance of\\n\"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 4, 'page_label': '5'}, page_content='without expensive, specialized “salient span masking” pre-training [20]. It is worth noting that RAG’s\\nretriever is initialized using DPR’s retriever, which uses retrieval supervision on Natural Questions\\nand TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based “cross-\\nencoder” to re-rank documents, along with an extractive reader. RAG demonstrates that neither a\\nre-ranker nor extractive reader is necessary for state-of-the-art performance.\\nThere are several advantages to generating answers even when it is possible to extract them. Docu-\\nments with clues about the answer but do not contain the answer verbatim can still contribute towards\\na correct answer being generated, which is not possible with standard extractive approaches, leading\\n5'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 5, 'page_label': '6'}, page_content='Table 1: Open-Domain QA Test Scores. For TQA,\\nleft column uses the standard test set for Open-\\nDomain QA, right column uses the TQA-Wiki\\ntest set. See Appendix D for further details.\\nModel NQ TQA WQ CT\\nClosed\\nBook\\nT5-11B [52] 34.5 - /50.1 37.4 -\\nT5-11B+SSM[52] 36.6 - /60.5 44.7 -\\nOpen\\nBook\\nREALM [20] 40.4 - / - 40.7 46.8\\nDPR [26] 41.5 57.9/ - 41.1 50.6\\nRAG-Token 44.1 55.2/66.1 45.5 50.0\\nRAG-Seq. 44.5 56.8/68.0 45.2 52.2\\nTable 2: Generation and classiﬁcation Test Scores.\\nMS-MARCO SotA is [4], FEVER-3 is [68] and\\nFEVER-2 is [ 57] *Uses gold context/evidence.\\nBest model without gold access underlined.\\nModel Jeopardy MSMARCO FVR3 FVR2\\nB-1 QB-1 R-L B-1 Label Acc.\\nSotA - - 49.8* 49.9* 76.8 92.2 *\\nBART 15.1 19.7 38.2 41.6 64.0 81.1\\nRAG-Tok. 17.3 22.2 40.1 41.5 72.5 89.5RAG-Seq. 14.7 21.4 40.8 44.2\\nto more effective marginalization over documents. Furthermore, RAG can generate correct answers\\neven when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 5, 'page_label': '6'}, page_content='cases for NQ, where an extractive model would score 0%.\\n4.2 Abstractive Question Answering\\nAs shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu\\npoints and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is\\nimpressive given that (i) those models access gold passages with speciﬁc information required to\\ngenerate the reference answer , (ii) many questions are unanswerable without the gold passages, and\\n(iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers\\nfrom our models. Qualitatively, we ﬁnd that RAG models hallucinate less and generate factually\\ncorrect text more often than BART. Later, we also show that RAG generations are more diverse than\\nBART generations (see §4.5).\\n4.3 Jeopardy Question Generation\\nTable 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 5, 'page_label': '6'}, page_content='with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452\\npairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual\\nthan RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and\\nBART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on\\nthe task over a state-of-the-art generation model. Evaluators also ﬁnd RAG generations to be more\\nspeciﬁc by a large margin. Table 3 shows typical generations from each model.\\nJeopardy questions often contain two separate pieces of information, and RAG-Token may perform\\nbest because it can generate responses that combine content from several documents. Figure 2 shows\\nan example. When generating “Sun”, the posterior is high for document 2 which mentions “The\\nSun Also Rises”. Similarly, document 1 dominates the posterior when “A Farewell to Arms” is'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 5, 'page_label': '6'}, page_content='generated. Intriguingly, after the ﬁrst token of each book is generated, the document posterior ﬂattens.\\nThis observation suggests that the generator can complete the titles without depending on speciﬁc\\ndocuments. In other words, the model’s parametric knowledge is sufﬁcient to complete the titles. We\\nﬁnd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding\"The\\nSun. BART completes the generation \"The Sun Also Rises\" is a novel by this author of \"The Sun\\nAlso Rises\" indicating the title \"The Sun Also Rises\" is stored in BART’s parameters. Similarly,\\nBART will complete the partial decoding \"The Sun Also Rises\" is a novel by this author of \"A\\nwith \"The Sun Also Rises\" is a novel by this author of \"A Farewell to Arms\". This example shows\\nhow parametric and non-parametric memories work together—the non-parametric component helps\\nto guide the generation, drawing out speciﬁc knowledge stored in the parametric memory.\\n4.4 Fact Veriﬁcation'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 5, 'page_label': '6'}, page_content='Table 2 shows our results on FEVER. For 3-way classiﬁcation, RAG scores are within 4.3% of\\nstate-of-the-art models, which are complex pipeline systems with domain-speciﬁc architectures and\\nsubstantial engineering, trained using intermediate retrieval supervision, which RAG does not require.\\n6'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 6, 'page_label': '7'}, page_content='Document 1: his works are considered classics of American\\nliterature ... His wartime experiences formed the basis for his novel\\n”A Farewell to Arms”(1929) ...\\nDocument 2: ... artists of the 1920s ”Lost Generation” expatriate\\ncommunity. His debut novel,”The Sun Also Rises”, was published\\nin 1926.\\nBOS\\n”\\nTheSunAlso\\nR ises\\n” is a\\nnovel\\nby this\\nauthor\\nof ” A\\nFarewellto\\nArms\\n”\\nDoc 1\\nDoc 2\\nDoc 3\\nDoc 4\\nDoc 5\\nFigure 2: RAG-Token document posterior p(zi|x,yi,y−i) for each generated token for input “Hem-\\ningway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high\\nwhen generating “A Farewell to Arms\" and for document 2 when generating “The Sun Also Rises\".\\nTable 3: Examples from generation tasks. RAG models generate more speciﬁc and factually accurate\\nresponses. ‘?’ indicates factually incorrect responses, * indicates partially correct responses.\\nTask Input Model Generation\\nMS-\\nMARCO\\ndeﬁne middle\\near'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 6, 'page_label': '7'}, page_content='deﬁne middle\\near\\nBART ?The middle ear is the part of the ear between the middle ear and the nose.\\nRAG-T The middle ear is the portion of the ear internal to the eardrum.\\nRAG-S The middle ear includes the tympanic cavity and the three ossicles.\\nwhat currency\\nneeded in\\nscotland\\nBART The currency needed in Scotland is Pound sterling.\\nRAG-T Pound is the currency needed in Scotland.\\nRAG-S The currency needed in Scotland is the pound sterling.\\nJeopardy\\nQuestion\\nGener\\n-ation\\nWashington\\nBART ?This state has the largest number of counties in the U.S.\\nRAG-T It’s the only U.S. state named for a U.S. president\\nRAG-S It’s the state where you’ll ﬁnd Mount Rainier National Park\\nThe Divine\\nComedy\\nBART *This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio\\nRAG-T Dante’s \"Inferno\" is the ﬁrst part of this epic poem\\nRAG-S This 14th century work is divided into 3 sections: \"Inferno\", \"Purgatorio\" & \"Paradiso\"'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 6, 'page_label': '7'}, page_content='For 2-way classiﬁcation, we compare against Thorne and Vlachos [57], who train RoBERTa [35]\\nto classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy\\nwithin 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence.\\nWe also analyze whether documents retrieved by RAG correspond to documents annotated as gold\\nevidence in FEVER. We calculate the overlap in article titles between the topkdocuments retrieved\\nby RAG and gold evidence annotations. We ﬁnd that the top retrieved document is from a gold article\\nin 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.\\n4.5 Additional Results\\nGeneration Diversity Section 4.3 shows that RAG models are more factual and speciﬁc than\\nBART for Jeopardy question generation. Following recent work on diversity-promoting decoding\\n[33, 59, 39], we also investigate generation diversity by calculating the ratio of distinct ngrams to'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 6, 'page_label': '7'}, page_content='total ngrams generated by different models. Table 5 shows that RAG-Sequence’s generations are\\nmore diverse than RAG-Token’s, and both are signiﬁcantly more diverse than BART without needing\\nany diversity-promoting decoding.\\nRetrieval Ablations A key feature of RAG is learning to retrieve relevant information for the task.\\nTo assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever\\nduring training. As shown in Table 6, learned retrieval improves results for all tasks.\\nWe compare RAG’s dense retriever to a word overlap-based BM25 retriever [53]. Here, we replace\\nRAG’s retriever with a ﬁxed BM25 system, and use BM25 retrieval scores as logits when calculating\\np(z|x). Table 6 shows the results. For FEVER, BM25 performs best, perhaps since FEVER claims are\\nheavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval\\nimproves results on all other tasks, especially for Open-Domain QA, where it is crucial.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 6, 'page_label': '7'}, page_content='Index hot-swapping An advantage of non-parametric memory models like RAG is that knowledge\\ncan be easily updated at test time. Parametric-only models like T5 or BART need further training to\\nupdate their behavior as the world changes. To demonstrate, we build an index using the DrQA [5]\\nWikipedia dump from December 2016 and compare outputs from RAG using this index to the newer\\nindex from our main results (December 2018). We prepare a list of 82 world leaders who had changed\\n7'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 7, 'page_label': '8'}, page_content='Table 4: Human assessments for the Jeopardy\\nQuestion Generation Task.\\nFactuality Speciﬁcity\\nBART better 7.1% 16.8%\\nRAG better 42.7% 37.4%\\nBoth good 11.7% 11.8%\\nBoth poor 17.7% 6.9%\\nNo majority 20.8% 20.1%\\nTable 5: Ratio of distinct to total tri-grams for\\ngeneration tasks.\\nMSMARCO Jeopardy QGen\\nGold 89.6% 90.0%\\nBART 70.7% 32.4%\\nRAG-Token 77.8% 46.8%\\nRAG-Seq. 83.5% 53.8%\\nTable 6: Ablations on the dev set. As FEVER is a classiﬁcation task, both RAG models are equivalent.\\nModel NQ TQA WQ CT Jeopardy-QGen MSMarco FVR-3 FVR-2\\nExact Match B-1 QB-1 R-L B-1 Label Accuracy\\nRAG-Token-BM25 29.7 41.5 32.1 33.1 17.5 22.3 55.5 48.4 75.1 91.6RAG-Sequence-BM25 31.8 44.1 36.6 33.8 11.1 19.5 56.5 46.9\\nRAG-Token-Frozen 37.8 50.1 37.1 51.1 16.7 21.7 55.9 49.4 72.9 89.4RAG-Sequence-Frozen 41.2 52.1 41.8 52.6 11.8 19.6 56.7 47.3\\nRAG-Token 43.5 54.8 46.5 51.9 17.9 22.6 56.2 49.4 74.5 90.6RAG-Sequence 44.0 55.8 44.9 53.4 15.3 21.5 57.2 47.5'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 7, 'page_label': '8'}, page_content='between these dates and use a template “Who is {position}?” (e.g. “Who is the President of Peru?”)\\nto query our NQ RAG model with each index. RAG answers 70% correctly using the 2016 index for\\n2016 world leaders and 68% using the 2018 index for 2018 world leaders. Accuracy with mismatched\\nindices is low (12% with the 2018 index and 2016 leaders, 4% with the 2016 index and 2018 leaders).\\nThis shows we can update RAG’s world knowledge by simply replacing its non-parametric memory.\\nEffect of Retrieving more documents Models are trained with either 5 or 10 retrieved latent\\ndocuments, and we do not observe signiﬁcant differences in performance between them. We have the\\nﬂexibility to adjust the number of retrieved documents at test time, which can affect performance and\\nruntime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves\\nOpen-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 7, 'page_label': '8'}, page_content='documents. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for\\nRAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.\\n10 20 30 40 50\\nKR e t r i e v e dD o c s\\n39\\n40\\n41\\n42\\n43\\n44NQ Exact Match RAG-Tok\\nRAG-Seq\\n10 20 30 40 50\\nKR e t r i e v e dD o c s\\n40\\n50\\n60\\n70\\n80NQ Answer Recall @ K\\nRAG-Tok\\nRAG-Seq\\nFixed DPR\\nBM25\\n10 20 30 40 50\\nKR e t r i e v e dD o c s\\n48\\n50\\n52\\n54\\n56Bleu-1 / Rouge-L score\\nRAG-Tok R-L\\nRAG-Tok B-1\\nRAG-Seq R-L\\nRAG-Seq B-1\\nFigure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall perfor-\\nmance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.\\n5 Related Work\\nSingle-Task Retrieval Prior work has shown that retrieval improves performance across a variety of\\nNLP tasks when considered in isolation. Such tasks include open-domain question answering [5, 29],\\nfact checking [ 56], fact completion [ 48], long-form question answering [ 12], Wikipedia article'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 7, 'page_label': '8'}, page_content='generation [36], dialogue [ 41, 65, 9, 13], translation [ 17], and language modeling [ 19, 27]. Our\\nwork uniﬁes previous successes in incorporating retrieval into individual tasks, showing that a single\\nretrieval-based architecture is capable of achieving strong performance across several tasks.\\n8'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 8, 'page_label': '9'}, page_content='General-Purpose Architectures for NLP Prior work on general-purpose architectures for NLP\\ntasks has shown great success without the use of retrieval. A single, pre-trained language model\\nhas been shown to achieve strong performance on various classiﬁcation tasks in the GLUE bench-\\nmarks [60, 61] after ﬁne-tuning [49, 8]. GPT-2 [50] later showed that a single, left-to-right, pre-trained\\nlanguage model could achieve strong performance across both discriminative and generative tasks.\\nFor further improvement, BART [32] and T5 [51, 52] propose a single, pre-trained encoder-decoder\\nmodel that leverages bi-directional attention to achieve stronger performance on discriminative\\nand generative tasks. Our work aims to expand the space of possible tasks with a single, uniﬁed\\narchitecture, by learning a retrieval module to augment pre-trained, generative language models.\\nLearned Retrieval There is signiﬁcant work on learning to retrieve documents in information'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 8, 'page_label': '9'}, page_content='retrieval, more recently with pre-trained, neural language models [ 44, 26] similar to ours. Some\\nwork optimizes the retrieval module to aid in a speciﬁc, downstream task such as question answering,\\nusing search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our\\nwork. These successes leverage different retrieval-based architectures and optimization techniques to\\nachieve strong performance on a single task, while we show that a single retrieval-based architecture\\ncan be ﬁne-tuned for strong performance on a variety of tasks.\\nMemory-based Architectures Our document index can be seen as a large external memory for\\nneural networks to attend to, analogous to memory networks [64, 55]. Concurrent work [14] learns\\nto retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our\\nwork. Other work improves the ability of dialog models to generate factual text by attending over'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 8, 'page_label': '9'}, page_content='fact embeddings [15, 13]. A key feature of our memory is that it is comprised of raw text rather\\ndistributed representations, which makes the memory both (i) human-readable, lending a form of\\ninterpretability to our model, and (ii) human-writable, enabling us to dynamically update the model’s\\nmemory by editing the document index. This approach has also been used in knowledge-intensive\\ndialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF\\nrather than end-to-end learnt retrieval [9].\\nRetrieve-and-Edit approaches Our method shares some similarities with retrieve-and-edit style\\napproaches, where a similar training input-output pair is retrieved for a given input, and then edited\\nto provide a ﬁnal output. These approaches have proved successful in a number of domains including\\nMachine Translation [ 18, 22] and Semantic Parsing [21]. Our approach does have several differences,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 8, 'page_label': '9'}, page_content='including less of emphasis on lightly editing a retrieved item, but on aggregating content from several\\npieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents\\nrather than related training pairs. This said, RAG techniques may work well in these settings, and\\ncould represent promising future work.\\n6 Discussion\\nIn this work, we presented hybrid generation models with access to parametric and non-parametric\\nmemory. We showed that our RAG models obtain state of the art results on open-domain QA. We\\nfound that people prefer RAG’s generation over purely parametric BART, ﬁnding RAG more factual\\nand speciﬁc. We conducted an thorough investigation of the learned retrieval component, validating\\nits effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model\\nwithout requiring any retraining. In future work, it may be fruitful to investigate if the two components'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 8, 'page_label': '9'}, page_content='can be jointly pre-trained from scratch, either with a denoising objective similar to BART or some\\nanother objective. Our work opens up new research directions on how parametric and non-parametric\\nmemories interact and how to most effectively combine them, showing promise in being applied to a\\nwide variety of NLP tasks.\\n9'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 9, 'page_label': '10'}, page_content='Broader Impact\\nThis work offers several positive societal beneﬁts over previous work: the fact that it is more\\nstrongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate” less\\nwith generations that are more factual, and offers more control and interpretability. RAG could be\\nemployed in a wide variety of scenarios with direct beneﬁt to society, for example by endowing it\\nwith a medical index and asking it open-domain questions on that topic, or by helping people be more\\neffective at their jobs.\\nWith these advantages also come potential downsides: Wikipedia, or any potential external knowledge\\nsource, will probably never be entirely factual and completely devoid of bias. Since RAG can be\\nemployed as a language model, similar concerns as for GPT-2 [50] are valid here, although arguably\\nto a lesser extent, including that it might be used to generate abuse, faked or misleading content in'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 9, 'page_label': '10'}, page_content='the news or on social media; to impersonate others; or to automate the production of spam/phishing\\ncontent [54]. Advanced language models may also lead to the automation of various jobs in the\\ncoming decades [16]. In order to mitigate these risks, AI systems could be employed to ﬁght against\\nmisleading content and automated spam/phishing.\\nAcknowledgments\\nThe authors would like to thank the reviewers for their thoughtful and constructive feedback on this\\npaper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors\\nwould also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP\\nthanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD\\nprogram.\\nReferences\\n[1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan\\nMajumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 9, 'page_label': '10'}, page_content='Stoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine\\nReading COmprehension Dataset. arXiv:1611.09268 [cs], November 2016. URL http:\\n//arxiv.org/abs/1611.09268. arXiv: 1611.09268.\\n[2] Petr Baudiš and Jan Šediv`y. Modeling of the question answering task in the yodaqa system. In\\nInternational Conference of the Cross-Language Evaluation Forum for European Languages,\\npages 222–228. Springer, 2015. URL https://link.springer.com/chapter/10.1007%\\n2F978-3-319-24027-5_20 .\\n[3] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase\\nfrom Question-Answer Pairs. In Proceedings of the 2013 Conference on Empirical Methods\\nin Natural Language Processing, pages 1533–1544, Seattle, Washington, USA, October 2013.\\nAssociation for Computational Linguistics. URL http://www.aclweb.org/anthology/\\nD13-1160.\\n[4] Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencod-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 9, 'page_label': '10'}, page_content='ing&autoregressive language model for context-conditioned generation. ArXiv, abs/2004.07159,\\n2020. URL https://arxiv.org/abs/2004.07159.\\n[5] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer\\nOpen-Domain Questions. In Proceedings of the 55th Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers), pages 1870–1879, Vancouver, Canada,\\nJuly 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL\\nhttps://www.aclweb.org/anthology/P17-1171.\\n[6] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and\\nJonathan Berant. Coarse-to-ﬁne question answering for long documents. In Proceedings of the\\n55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\\npages 209–220, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi:\\n10.18653/v1/P17-1020. URL https://www.aclweb.org/anthology/P17-1020.\\n10'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 10, 'page_label': '11'}, page_content='[7] Christopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Compre-\\nhension. arXiv:1710.10723 [cs], October 2017. URL http://arxiv.org/abs/1710.10723.\\narXiv: 1710.10723.\\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of\\nDeep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Con-\\nference of the North American Chapter of the Association for Computational Linguistics: Human\\nLanguage Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis,\\nMinnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.\\nURL https://www.aclweb.org/anthology/N19-1423.\\n[9] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wiz-\\nard of wikipedia: Knowledge-powered conversational agents. In International Conference on\\nLearning Representations, 2019. URL https://openreview.net/forum?id=r1l73iRqKm.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 10, 'page_label': '11'}, page_content='[10] Matthew Dunn, Levent Sagun, Mike Higgins, V . Ugur Guney, V olkan Cirik, and Kyunghyun\\nCho. SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine.\\narXiv:1704.05179 [cs], April 2017. URL http://arxiv.org/abs/1704.05179. arXiv:\\n1704.05179.\\n[11] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceed-\\nings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:\\nLong Papers), pages 889–898, Melbourne, Australia, July 2018. Association for Computational\\nLinguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anthology/\\nP18-1082.\\n[12] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:\\nLong form question answering. In Proceedings of the 57th Annual Meeting of the Association\\nfor Computational Linguistics, pages 3558–3567, Florence, Italy, July 2019. Association for\\nComputational Linguistics. doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 10, 'page_label': '11'}, page_content='anthology/P19-1346.\\n[13] Angela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers\\nwith KNN-based composite memory, 2020. URL https://openreview.net/forum?id=\\nH1gx1CNKPH.\\n[14] Thibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski.\\nEntities as experts: Sparse memory access with entity supervision. ArXiv, abs/2004.07202,\\n2020. URL https://arxiv.org/abs/2004.07202.\\n[15] Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen\\ntau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In AAAI\\nConference on Artiﬁcial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/\\nAAAI/AAAI18/paper/view/16710.\\n[16] Katja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, and Owain Evans. When will AI\\nexceed human performance? evidence from AI experts. CoRR, abs/1705.08807, 2017. URL\\nhttp://arxiv.org/abs/1705.08807.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 10, 'page_label': '11'}, page_content='[17] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural\\nmachine translation. In AAAI Conference on Artiﬁcial Intelligence , 2018. URL https:\\n//www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282.\\n[18] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural\\nmachine translation. In 32nd AAAI Conference on Artiﬁcial Intelligence, AAAI 2018 , 32nd\\nAAAI Conference on Artiﬁcial Intelligence, AAAI 2018, pages 5133–5140. AAAI press, 2018.\\n32nd AAAI Conference on Artiﬁcial Intelligence, AAAI 2018 ; Conference date: 02-02-2018\\nThrough 07-02-2018.\\n[19] Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by\\nediting prototypes. Transactions of the Association for Computational Linguistics, 6:437–450,\\n2018. doi: 10.1162/tacl_a_00030. URL https://www.aclweb.org/anthology/Q18-1031.\\n11'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 11, 'page_label': '12'}, page_content='[20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM:\\nRetrieval-augmented language model pre-training. ArXiv, abs/2002.08909, 2020. URL https:\\n//arxiv.org/abs/2002.08909.\\n[21] Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. A\\nretrieve-and-edit framework for predicting structured outputs. In S. Bengio,\\nH. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, ed-\\nitors, Advances in Neural Information Processing Systems 31 , pages 10052–\\n10062. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/\\n8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.\\npdf.\\n[22] Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. Simple and effective retrieve-\\nedit-rerank text generation. In Proceedings of the 58th Annual Meeting of the Association for\\nComputational Linguistics, pages 2532–2538, Online, July 2020. Association for Computa-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 11, 'page_label': '12'}, page_content='tional Linguistics. doi: 10.18653/v1/2020.acl-main.228. URL https://www.aclweb.org/\\nanthology/2020.acl-main.228.\\n[23] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. arXiv\\npreprint arXiv:1702.08734, 2017. URL https://arxiv.org/abs/1702.08734.\\n[24] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A Large Scale\\nDistantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the\\n55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\\npages 1601–1611, Vancouver, Canada, July 2017. Association for Computational Linguistics.\\ndoi: 10.18653/v1/P17-1147. URL https://www.aclweb.org/anthology/P17-1147.\\n[25] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-\\naugmented recurrent nets. In Proceedings of the 28th International Conference on\\nNeural Information Processing Systems - Volume 1 , NIPS’15, page 190–198, Cam-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 11, 'page_label': '12'}, page_content='bridge, MA, USA, 2015. MIT Press. URL https://papers.nips.cc/paper/\\n5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets .\\n[26] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and\\nWen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint\\narXiv:2004.04906, 2020. URL https://arxiv.org/abs/2004.04906.\\n[27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generaliza-\\ntion through memorization: Nearest neighbor language models. In International Conference on\\nLearning Representations, 2020. URL https://openreview.net/forum?id=HklBjCEKvH.\\n[28] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua\\nBengio and Yann LeCun, editors, 3rd International Conference on Learning Representations,\\nICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL\\nhttp://arxiv.org/abs/1412.6980.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 11, 'page_label': '12'}, page_content='[29] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh,\\nChris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Ken-\\nton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob\\nUszkoreit, Quoc Le, and Slav Petrov. Natural Questions: a Benchmark for Ques-\\ntion Answering Research. Transactions of the Association of Computational Lin-\\nguistics, 2019. URL https://tomkwiat.users.x20web.corp.google.com/papers/\\nnatural-questions/main-1455-kwiatkowski.pdf .\\n[30] Guillaume Lample, Alexandre Sablayrolles, Marc’ Aurelio Ranzato, Ludovic Denoyer, and\\nHerve Jegou. Large memory layers with product keys. In H. Wallach, H. Larochelle,\\nA. Beygelzimer, F. d’ Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural In-\\nformation Processing Systems 32, pages 8548–8559. Curran Associates, Inc., 2019. URL http:\\n//papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf .'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 11, 'page_label': '12'}, page_content='[31] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised\\nopen domain question answering. In Proceedings of the 57th Annual Meeting of the Association\\n12'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 12, 'page_label': '13'}, page_content='for Computational Linguistics, pages 6086–6096, Florence, Italy, July 2019. Association for\\nComputational Linguistics. doi: 10.18653/v1/P19-1612. URL https://www.aclweb.org/\\nanthology/P19-1612.\\n[32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,\\nOmer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence\\npre-training for natural language generation, translation, and comprehension. arXiv preprint\\narXiv:1910.13461, 2019. URL https://arxiv.org/abs/1910.13461.\\n[33] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting\\nobjective function for neural conversation models. In Proceedings of the 2016 Conference of the\\nNorth American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies, pages 110–119, San Diego, California, June 2016. Association for Computational\\nLinguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anthology/\\nN16-1014.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 12, 'page_label': '13'}, page_content='N16-1014.\\n[34] Margaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation\\nwith optimized questions and multi-turn comparisons. ArXiv, abs/1909.03087, 2019. URL\\nhttps://arxiv.org/abs/1909.03087.\\n[35] Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. Robust neural machine\\ntranslation with joint textual and phonetic embedding. In Proceedings of the 57th Annual\\nMeeting of the Association for Computational Linguistics, pages 3044–3049, Florence, Italy,\\nJuly 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL\\nhttps://www.aclweb.org/anthology/P19-1291.\\n[36] Peter J. Liu*, Mohammad Saleh*, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser,\\nand Noam Shazeer. Generating wikipedia by summarizing long sequences. In International\\nConference on Learning Representations, 2018. URL https://openreview.net/forum?\\nid=Hyg0vbWC-.\\n[37] Yury A. Malkov and D. A. Yashunin. Efﬁcient and robust approximate nearest neighbor search'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 12, 'page_label': '13'}, page_content='using hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis and\\nMachine Intelligence, 42:824–836, 2016. URL https://arxiv.org/abs/1603.09320.\\n[38] Gary Marcus. The next decade in ai: four steps towards robust artiﬁcial intelligence. arXiv\\npreprint arXiv:2002.06177, 2020. URL https://arxiv.org/abs/2002.06177.\\n[39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rocktäschel, Vassilis\\nPlachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies affect the\\nveriﬁability of generated text. arXiv preprint arXiv:1911.03587 , 2019. URL https:\\n//arxiv.org/abs/1911.03587.\\n[40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,\\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed\\nprecision training. In ICLR, 2018. URL https://openreview.net/forum?id=r1gs9JgRZ.\\n[41] Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploit-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 12, 'page_label': '13'}, page_content='ing background knowledge for building conversation systems. In Proceedings of the 2018\\nConference on Empirical Methods in Natural Language Processing, pages 2322–2332, Brus-\\nsels, Belgium, October-November 2018. Association for Computational Linguistics. doi:\\n10.18653/v1/D18-1255. URL https://www.aclweb.org/anthology/D18-1255.\\n[42] Preksha Nema and Mitesh M. Khapra. Towards a better metric for evaluating question generation\\nsystems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language\\nProcessing, pages 3950–3959, Brussels, Belgium, October-November 2018. Association for\\nComputational Linguistics. doi: 10.18653/v1/D18-1429. URL https://www.aclweb.org/\\nanthology/D18-1429.\\n[43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder,\\nand Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In\\nTarek Richard Besold, Antoine Bordes, Artur S. d’Avila Garcez, and Greg Wayne, editors,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 12, 'page_label': '13'}, page_content='Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic\\n13'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 13, 'page_label': '14'}, page_content='approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing\\nSystems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop\\nProceedings. CEUR-WS.org, 2016. URL http://ceur-ws.org/Vol-1773/CoCoNIPS_\\n2016_paper9.pdf.\\n[44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint\\narXiv:1901.04085, 2019. URL https://arxiv.org/abs/1901.04085.\\n[45] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,\\nand Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings\\nof the 2019 Conference of the North American Chapter of the Association for Computational\\nLinguistics (Demonstrations), pages 48–53, Minneapolis, Minnesota, June 2019. Association\\nfor Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://www.aclweb.\\norg/anthology/N19-4009.\\n[46] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 13, 'page_label': '14'}, page_content='Cho. Finding generalizable evidence by learning to convince q&a models. In Proceedings\\nof the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th\\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages\\n2402–2411, Hong Kong, China, November 2019. Association for Computational Linguistics.\\ndoi: 10.18653/v1/D19-1244. URL https://www.aclweb.org/anthology/D19-1244.\\n[47] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,\\nand Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019\\nConference on Empirical Methods in Natural Language Processing and the 9th International\\nJoint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463–2473, Hong\\nKong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/\\nD19-1250. URL https://www.aclweb.org/anthology/D19-1250.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 13, 'page_label': '14'}, page_content='[48] Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H.\\nMiller, and Sebastian Riedel. How context affects language models’ factual predictions. In\\nAutomated Knowledge Base Construction, 2020. URL https://openreview.net/forum?\\nid=025X0zPfn.\\n[49] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Im-\\nproving Language Understanding by Generative Pre-Training, 2018. URL\\nhttps://s3-us-west-2.amazonaws.com/openai-assets/research-covers/\\nlanguage-unsupervised/language_understanding_paper.pdf.\\n[50] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\\nSutskever. Language models are unsupervised multitask learners, 2019. URL\\nhttps://d4mucfpksywv.cloudfront.net/better-language-models/language_\\nmodels_are_unsupervised_multitask_learners.pdf.\\n[51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 13, 'page_label': '14'}, page_content='Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed\\ntext-to-text transformer. arXiv e-prints, 2019. URL https://arxiv.org/abs/1910.10683.\\n[52] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into\\nthe parameters of a language model? arXiv e-prints, 2020. URL https://arxiv.org/abs/\\n2002.08910.\\n[53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and\\nbeyond. Found. Trends Inf. Retr., 3(4):333–389, April 2009. ISSN 1554-0669. doi: 10.1561/\\n1500000019. URL https://doi.org/10.1561/1500000019.\\n[54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-V oss, Jeff Wu, Alec\\nRadford, and Jian-Bing Wang. Release strategies and the social impacts of language models.\\nArXiv, abs/1908.09203, 2019.\\n[55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory net-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 13, 'page_label': '14'}, page_content='works. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,Advances\\nin Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015.\\nURL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf .\\n14'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 14, 'page_label': '15'}, page_content='[56] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a\\nlarge-scale dataset for fact extraction and VERiﬁcation. In Proceedings of the 2018 Conference\\nof the North American Chapter of the Association for Computational Linguistics: Human\\nLanguage Technologies, Volume 1 (Long Papers), pages 809–819, New Orleans, Louisiana,\\nJune 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL\\nhttps://www.aclweb.org/anthology/N18-1074.\\n[57] James H. Thorne and Andreas Vlachos. Avoiding catastrophic forgetting in mitigating model\\nbiases in sentence-pair classiﬁcation with elastic weight consolidation. ArXiv, abs/2004.14366,\\n2020. URL https://arxiv.org/abs/2004.14366.\\n[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁ ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V . Luxburg,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 14, 'page_label': '15'}, page_content='S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,Advances in Neural\\nInformation Processing Systems 30, pages 5998–6008. Curran Associates, Inc., 2017. URL\\nhttp://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf .\\n[59] Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David\\nCrandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes.\\nAAAI Conference on Artiﬁcial Intelligence, 2018. URL https://www.aaai.org/ocs/index.\\nphp/AAAI/AAAI18/paper/view/17329.\\n[60] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.\\nGLUE: A multi-task benchmark and analysis platform for natural language understanding.\\nIn Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting\\nNeural Networks for NLP, pages 353–355, Brussels, Belgium, November 2018. Association for\\nComputational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 14, 'page_label': '15'}, page_content='anthology/W18-5446.\\n[61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix\\nHill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for General-\\nPurpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer,\\nF. d\\\\textquotesingle Alché-Buc, E. Fox, and R. Garnett, editors,Advances in Neural Information\\nProcessing Systems 32, pages 3261–3275. Curran Associates, Inc., 2019. URL https://\\narxiv.org/abs/1905.00537.\\n[62] Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang,\\nGerry Tesauro, Bowen Zhou, and Jing Jiang. R3: Reinforced ranker-reader for open-domain\\nquestion answering. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of\\nthe Thirty-Second AAAI Conference on Artiﬁcial Intelligence, (AAAI-18), the 30th innovative\\nApplications of Artiﬁcial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 14, 'page_label': '15'}, page_content='Advances in Artiﬁcial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7,\\n2018, pages 5981–5988. AAAI Press, 2018. URL https://www.aaai.org/ocs/index.\\nphp/AAAI/AAAI18/paper/view/16712.\\n[63] Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang,\\nTim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-\\nranking in open-domain question answering. In ICLR, 2018. URL https://openreview.\\nnet/forum?id=rJl3yM-Ab.\\n[64] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio\\nand Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR\\n2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015. URL\\nhttp://arxiv.org/abs/1410.3916.\\n[65] Jason Weston, Emily Dinan, and Alexander Miller. Retrieve and reﬁne: Improved sequence\\ngeneration models for dialogue. In Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 14, 'page_label': '15'}, page_content='International Workshop on Search-Oriented Conversational AI, pages 87–92, Brussels, Belgium,\\nOctober 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5713. URL\\nhttps://www.aclweb.org/anthology/W18-5713.\\n15'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 15, 'page_label': '16'}, page_content='[66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\\nMoi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,\\nPatrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain\\nGugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface’s transformers:\\nState-of-the-art natural language processing. ArXiv, abs/1910.03771, 2019.\\n[67] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi-\\nsupervised question answering. In Proceedings of the 2019 Conference on Empirical Meth-\\nods in Natural Language Processing and the 9th International Joint Conference on Natural\\nLanguage Processing (EMNLP-IJCNLP) , pages 2495–2509, Hong Kong, China, Novem-\\nber 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL\\nhttps://www.aclweb.org/anthology/D19-1253.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 15, 'page_label': '16'}, page_content='[68] Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and\\nJian Yin. Reasoning over semantic-level graph for fact checking. ArXiv, abs/1909.03745, 2019.\\nURL https://arxiv.org/abs/1909.03745.\\n16'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 16, 'page_label': '17'}, page_content='Appendices for Retrieval-Augmented Generation for\\nKnowledge-Intensive NLP Tasks\\nA Implementation Details\\nFor Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models.\\nFor RAG-Sequence models, we report test results using 50 retrieved documents, and we use the\\nThorough Decoding approach since answers are generally short. We use greedy decoding for QA as\\nwe did not ﬁnd beam search improved results. For Open-MSMarco and Jeopardy question generation,\\nwe report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence,\\nand we also train a BART-large model as a baseline. We use a beam size of four, and use the Fast\\nDecoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance.\\nB Human Evaluation\\nFigure 4: Annotation interface for human evaluation of factuality. A pop-out for detailed instructions\\nand a worked example appear when clicking \"view tool guide\".'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 16, 'page_label': '17'}, page_content='Figure 4 shows the user interface for human evaluation. To avoid any biases for screen position,\\nwhich model corresponded to sentence A and sentence B was randomly selected for each example.\\nAnnotators were encouraged to research the topic using the internet, and were given detailed instruc-\\ntions and worked examples in a full instructions tab. We included some gold sentences in order to\\nassess the accuracy of the annotators. Two annotators did not perform well on these examples and\\ntheir annotations were removed from the results.\\nC Training setup Details\\nWe train all RAG models and BART baselines using Fairseq [45].2 We train with mixed precision\\nﬂoating point arithmetic [40], distributing training across 8, 32GB NVIDIA V100 GPUs, though\\ntraining and inference can be run on one GPU. We ﬁnd that doing Maximum Inner Product Search\\nwith FAISS is sufﬁciently fast on CPU, so we store document index vectors on CPU, requiring∼100'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 16, 'page_label': '17'}, page_content='GB of CPU memory for all of Wikipedia. After submission, We have ported our code to HuggingFace\\nTransformers [66]3, which achieves equivalent performance to the previous version but is a cleaner\\nand easier to use implementation. This version is also open-sourced. We also compress the document\\nindex using FAISS’s compression tools, reducing the CPU memory requirement to 36GB. Scripts to\\nrun experiments with RAG can be found athttps://github.com/huggingface/transformers/\\nblob/master/examples/rag/README.md and an interactive demo of a RAG model can be found\\nat https://huggingface.co/rag/\\n2https://github.com/pytorch/fairseq\\n3https://github.com/huggingface/transformers\\n17'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 17, 'page_label': '18'}, page_content='D Further Details on Open-Domain QA\\nFor open-domain QA, multiple answer annotations are often available for a given question. These\\nanswer annotations are exploited by extractive models during training as typically all the answer\\nannotations are used to ﬁnd matches within documents when preparing training data. For RAG, we\\nalso make use of multiple annotation examples for Natural Questions and WebQuestions by training\\nthe model with each (q,a) pair separately, leading to a small increase in accuracy. For TriviaQA,\\nthere are often many valid answers to a given question, some of which are not suitable training targets,\\nsuch as emoji or spelling variants. For TriviaQA, we ﬁlter out answer candidates if they do not occur\\nin top 1000 documents for the query.\\nCuratedTrec preprocessing The answers for CuratedTrec are given in the form of regular expres-\\nsions, which has been suggested as a reason why it is unsuitable for answer-generation models [20].'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 17, 'page_label': '18'}, page_content='To overcome this, we use a pre-processing step where we ﬁrst retrieve the top 1000 documents for\\neach query, and use the answer that most frequently matches the regex pattern as the supervision\\ntarget. If no matches are found, we resort to a simple heuristic: generate all possible permutations for\\neach regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace.\\nTriviaQA Evaluation setups The open-domain QA community customarily uses public develop-\\nment datasets as test datasets, as test data for QA datasets is often restricted and dedicated to reading\\ncompehension purposes. We report our results using the datasets splits used in DPR [26], which are\\nconsistent with common practice in Open-domain QA. For TriviaQA, this test dataset is the public\\nTriviaQA Web Development split. Roberts et al.[52] used the TriviaQA ofﬁcial Wikipedia test set\\ninstead. Févry et al. [14] follow this convention in order to compare with Roberts et al. [52] (See'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 17, 'page_label': '18'}, page_content='appendix of [14]). We report results on both test sets to enable fair comparison to both approaches.\\nWe ﬁnd that our performance is much higher using the ofﬁcial Wiki test set, rather than the more\\nconventional open-domain test set, which we attribute to the ofﬁcial Wiki test set questions being\\nsimpler to answer from Wikipedia.\\nE Further Details on FEVER\\nFor FEVER classiﬁcation, we follow the practice from [ 32], and ﬁrst re-generate the claim, and\\nthen classify using the representation of the ﬁnal hidden state, before ﬁnally marginalizing across\\ndocuments to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The\\nﬁrst is to classify the claim as either \"Supported\", \"Refuted\" or \"Not Enough Info\", which is the task\\nwe explore in the main paper. FEVER’s other sub-task involves extracting sentences from Wikipedia\\nas evidence supporting the classiﬁcation prediction. As FEVER uses a different Wikipedia dump to'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 17, 'page_label': '18'}, page_content='us, directly tackling this task is not straightforward. We hope to address this in future work.\\nF Null Document Probabilities\\nWe experimented with adding \"Null document\" mechanism to RAG, similar to REALM [20] in order\\nto model cases where no useful information could be retrieved for a given input. Here, ifkdocuments\\nwere retrieved, we would additionally \"retrieve\" an empty document and predict a logit for the null\\ndocument, before marginalizing over k+ 1predictions. We explored modelling this null document\\nlogit by learning (i) a document embedding for the null document, (ii) a static learnt bias term, or\\n(iii) a neural network to predict the logit. We did not ﬁnd that these improved performance, so in\\nthe interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents\\ncannot always be retrieved, we observe that the model learns to always retrieve a particular set of'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 17, 'page_label': '18'}, page_content='documents for questions that are less likely to beneﬁt from retrieval, suggesting that null document\\nmechanisms may not be necessary for RAG.\\nG Parameters\\nOur RAG models contain the trainable parameters for the BERT-base query and document encoder of\\nDPR, with 110M parameters each (although we do not train the document encoder ourselves) and\\n406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable\\n18'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 18, 'page_label': '19'}, page_content='Table 7: Number of instances in the datasets used. *A hidden subset of this data is used for evaluation\\nTask Train Development Test\\nNatural Questions 79169 8758 3611\\nTriviaQA 78786 8838 11314\\nWebQuestions 3418 362 2033\\nCuratedTrec 635 134 635\\nJeopardy Question Generation 97392 13714 26849\\nMS-MARCO 153726 12468 101093*\\nFEVER-3-way 145450 10000 10000\\nFEVER-2-way 96966 6666 6666\\nparameters. The best performing \"closed-book\" (parametric only) open-domain QA model is T5-11B\\nwith 11 Billion trainable parameters. The T5 model with the closest number of parameters to our\\nmodels is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [52],\\nsubstantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/non-\\nparametric models require far fewer trainable parameters for strong open-domain QA performance.\\nThe non-parametric memory index does not consist of trainable parameters, but does consists of 21M'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 18, 'page_label': '19'}, page_content='728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit ﬂoating\\npoint precision to manage memory and disk footprints.\\nH Retrieval Collapse\\nIn preliminary experiments, we observed that for some tasks such as story generation [ 11], the\\nretrieval component would “collapse” and learn to retrieve the same documents regardless of the\\ninput. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents,\\nand the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit\\nrequirement for factual knowledge in some tasks, or the longer target sequences, which could result\\nin less informative gradients for the retriever. Perez et al.[46] also found spurious retrieval results\\nwhen optimizing a retrieval component in order to improve performance on downstream tasks.\\nI Number of instances per dataset\\nThe number of training, development and test datapoints in each of our datasets is shown in Table 7.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-2.pdf', 'total_pages': 19, 'page': 18, 'page_label': '19'}, page_content='19'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1'}, page_content='MuRAG: Multimodal Retrieval-Augmented Generator\\nfor Open Question Answering over Images and Text\\nWenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, William W. Cohen\\nGoogle Research\\n{wenhuchen,hexiang,patverga,wcohen}@google.com\\nAbstract\\nWhile language Models store a massive\\namount of world knowledge implicitly in their\\nparameters, even very large models often fail\\nto encode information about rare entities and\\nevents, while incurring huge computational\\ncosts. Recently, retrieval-augmented models,\\nsuch as REALM, RAG, and RETRO, have\\nincorporated world knowledge into language\\ngeneration by leveraging an external non-\\nparametric index and have demonstrated im-\\npressive performance with constrained model\\nsizes. However, these methods are restricted\\nto retrieving only textual knowledge, neglect-\\ning the ubiquitous amount of knowledge in\\nother modalities like images – much of which\\ncontains information not covered by any text.\\nTo address this limitation, we propose the'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1'}, page_content='ﬁrst Multimodal Retrieval-Augmented Trans-\\nformer (MuRAG), which accesses an external\\nnon-parametric multimodal memory to aug-\\nment language generation. MuRAG is pre-\\ntrained with a mixture of large-scale image-\\ntext and text-only corpora using a joint con-\\ntrastive and generative loss. We perform ex-\\nperiments on two different datasets that re-\\nquire retrieving and reasoning over both im-\\nages and text to answer a given query: We-\\nbQA, and MultimodalQA. Our results show\\nthat MuRAG achieves state-of-the-art accu-\\nracy, outperforming existing models by 10-\\n20% absolute on both datasets and under both\\ndistractor and full-wiki settings.\\n1 Introduction\\nPre-trained language models like GPT-3 (Brown\\net al., 2020), PaLM (Chowdhery et al., 2022), etc\\nhave been shown to capture a massive amount\\nof world knowledge implicitly in their parame-\\nters. However, using such large models incurs an\\nextremely high computation cost. As an alterna-\\ntive to a singular monolithic transformer, retrieval-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1'}, page_content='augmented architectures like KNN-LM (Khandel-\\nwal et al., 2019), REALM (Guu et al., 2020),\\nFigure 1: Visual information-seeking queries: These\\nqueries are unanswerable with text-only retrieval and\\nrequire retrieving and reasoning over images.\\nRAG (Lewis et al., 2020), FiD (Izacard and Grave,\\n2021), and RETRO (Borgeaud et al., 2021) have\\nbeen proposed to decouple world knowledge from\\nthe model’s parameters. More speciﬁcally, these\\nmodels are trained to access an external mem-\\nory to enhance the model’s predictions. Such\\nretrieval-augmented architectures have multiple\\nbeneﬁcial properties including: decreased model\\nsize (Borgeaud et al., 2021), better attribution/-\\nexplanation for model predictions (Lewis et al.,\\n2020), and adaptability to new information with-\\nout retraining (Verga et al., 2021). However, pre-\\nvious retrieval-augmented models are limited to\\nmemories that contain only text or structured data\\nand hence cannot make use of the massive amount'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1'}, page_content='of multimodal knowledge available on the web—\\nmuch of which contains information only available\\nin non-text modalities.\\nFigure 1, shows several information-seeking\\nqueries that require retrieving and reasoning over\\nvisual knowledge. Here, a user ﬁrst poses a ques-\\ntion such as “What can be found on the White\\nHouse balconies at Christmas”. The system then\\nretrieves relevant items from its memory, for exam-\\narXiv:2210.02928v2  [cs.CL]  20 Oct 2022'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 1, 'page_label': '2'}, page_content='ple, the ﬁrst image of Figure 1 with the caption\\n“White House during Christmas”, which it uses to\\nproduce the answer “wreaths and garlands”. Ex-\\nisting text retrieval-augmented models would strug-\\ngle with such queries because, in many cases, they\\nwould simply not have access to the answer as some\\nknowledge does not exist in text form. That, cou-\\npled with the abundance of multimodal knowledge\\nthat exists, leads to the conclusion that retrieval-\\naugmented models should ultimately be developed\\nto retrieve and reason over multiple modalities.\\nFigure 2: Model Overview: retrieval-and-predict pro-\\ncess of MuRAG on downstream datasets.\\nIn this paper, we are speciﬁcally interested in\\nendowing pre-trained language models with a non-\\nparametric multimodal memory containing images,\\ntext, or image-text pairs. To accomplish this, we\\nﬁrst combine pre-trained T5 (Raffel et al., 2020)\\nand ViT (Dosovitskiy et al., 2020) models to build\\na backbone encoder (Figure 3), which encodes'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 1, 'page_label': '2'}, page_content='image-text pairs, image-only, and text-only inputs\\ninto a multimodal representation. MuRAG uses the\\nbackbone encoder to embed items into an external\\nmemory as well as queries to retrieve multimodal\\nknowledge from that memory. These retrievals\\nthen augment a language model to generate more\\nvisually-grounded outputs.\\nWe pre-train MuRAG with a mixture of\\nimage-text and text-only datasets including\\nLAION (Schuhmann et al., 2021), Conceptual-\\nCaption (Sharma et al., 2018), VQA (An-\\ntol et al., 2015) and Probably-Asked-Questions\\n(PAQ) (Lewis et al., 2021). More speciﬁcally, we\\nreformulate these datasets in a retrieve-and-predict\\nformat. Here, the model’s input is an image along\\nwith a text prompt. The model then retrieves from\\na memory containing captions and passages, which\\nit uses to generate a target token sequence. The\\nmodel is trained with both a contrastive and a gen-\\nerative loss; this teaches the model to discriminate\\nrelevant from irrelevant memory entries, and guides'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 1, 'page_label': '2'}, page_content='the model to leverage the multimodal knowledge\\ninto generation.\\nUnlike the pre-training stage, during ﬁne-\\ntuning Figure 2 the model’s input is a question,\\nand the memory contains a collection of captioned\\nimages and text snippets. We ﬁne-tune MuRAG\\non the downstream datasets with a contrastive and\\ngenerative loss similar to pre-training. To avoid ex-\\ncessive computation cost, we develop a two-stage\\ntraining pipeline to ﬁrst train with small in-batch\\nmemory, and then with a statically encoded and\\nindexed large global memory.\\nOur experiments show that MuRAG achieves\\nstate-of-the-art performance on two different open-\\nmultimodal-QA datasets, both of which require\\nretrieving images and text from a large corpus to\\nanswer factoid questions: WebQA (Chang et al.,\\n2022) and MultimodalQA (Talmor et al., 2021). On\\nboth datasets, we outperform sophisticated base-\\nlines (Li et al., 2020; Radford et al., 2021; Zhang\\net al., 2021) by 10-20% accuracy under both dis-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 1, 'page_label': '2'}, page_content='tractor (from 40+ candidates) and full-wiki settings\\n(from 1M candidates). We also perform a compre-\\nhensive study to ablate different components of the\\npre-training to see their contributions. These em-\\npirical results demonstrate the effectiveness of our\\nproposed models to integrate multimodal knowl-\\nedge into pre-trained generation models and pave\\nthe way to uniﬁed retrieval-augmented frameworks.\\n2 Related Work\\nRetrieval Augmented Models Retrieval aug-\\nmented models are hybrid models containing\\nboth parameterized sequence models and a non-\\nparametric memory, infusing world knowledge into\\nexisting language models. Among them, KNN-\\nLM (Khandelwal et al., 2019) was ﬁrst proposed\\nto retrieve instances from a text training corpus to\\nhelp language modeling. Later, RETRO (Borgeaud\\net al., 2021) was proposed to scale up the text cor-\\npus to trillions of tokens, enabling the model to\\nachieve similar perplexity to GPT-3 (Brown et al.,\\n2020) with 25x fewer model parameters. Another'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 1, 'page_label': '2'}, page_content='family of models, such as REALM (Guu et al.,\\n2020), RAG (Lewis et al., 2020), and FiD (Izacard\\nand Grave, 2021), integrate Wikipedia passages as\\na datastore to beneﬁt downstream knowledge in-\\ntensive tasks (e.g. Question Answering). REALM\\nis an encoder-only model trained with masked lan-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 2, 'page_label': '3'}, page_content='guage modeling, while RAG and FiD adopt an\\nencoder-decoder model with a generative language\\nmodeling objective. Compared to them, MuRAG\\nis the ﬁrst retrieval-augmented model that is ca-\\npable of using knowledge presented in multiple\\nmodalities (i.e. visual and textual knowledge data),\\nwhereas all prior methods are restricted to using\\ntext-only knowledge.\\nMultimodal Transformers Multimodal trans-\\nformers have demonstrated strong performances\\nin learning cross-modal representation that are gen-\\nerally beneﬁcial on downstream vision and lan-\\nguage tasks, such as image-text retrieval (Karpa-\\nthy and Fei-Fei, 2015), image captioning (Chen\\net al., 2015), and VQA (Antol et al., 2015). These\\nmethods typically learn a joint transformer model\\non top of unimodal visual and textual backbones,\\nvia fusing deep features from each modality. The\\nearly version of multimodal transformers (Lu et al.,\\n2019; Chen et al., 2020; Li et al., 2020) usually\\nlearns a Transformer on pre-extracted unimodal'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 2, 'page_label': '3'}, page_content='features for contextualization, which makes it im-\\npossible to adjust those unimodal features to the\\ntarget tasks. Recently, SimVLM (Wang et al., 2022)\\nand COCA (Yu et al., 2022) proposed end-to-end\\ntraining for both deep multimodal transformers and\\nunimodal featurization networks and demonstrated\\nstrong performance in both multimodal and uni-\\nmodal downstream tasks. The multimodal memory\\nencoder of MuRAG is broadly similar to SimVLM\\nand CoCa, but has a different focus to encode and\\nretrieve multimodal knowledge ( i.e. images and\\ntexts) to augment language generation models.\\nMultimodal Question Answering The problem\\nof multimodal question answering has been ex-\\ntensively studied. VQA was the ﬁrst proposed to\\nanswer questions from visual-only inputs. Later,\\nOK-VQA (Marino et al., 2019) enlarged VQA’s\\nscope to annotate questions requiring both image\\nand implicit textual/common-sense knowledge to\\nanswer. More recently, MuMuQA (Reddy et al.,\\n2021), ManyModelQA (Hannan et al., 2020) and'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 2, 'page_label': '3'}, page_content='MIMOQA (Singh et al., 2021) provide questions\\nwhich require reasoning over images and explicitly\\nprovided text snippets. However, these datasets\\nare restricted to dealing with given text and images\\nwithout requiring any retrieval from the web: they\\nare analogous to machine-reading approaches to\\nQA from text like SQuAD, rather than open-book\\nQA. To study the more realistic open multimodal\\nQA task, WebQA (Chang et al., 2022) and Multi-\\nmodalQA (Talmor et al., 2021) have been proposed\\nto evaluate answers to open queries which require\\nretrieving and reasoning over a large-scale web\\nmultimodal corpus. Our model uses these datasets\\nto study open-world multimodal question answer-\\ning, obtaining state-of-the-art results.\\n3 Model\\n3.1 Backbone Encoder\\nFigure 3: Backbone encoder: ViT encodes image\\npatches into a sequence of vectors eI, while word em-\\nbedding converts text tokens into another sequence of\\nvectors eT. These vectors are concatenated to form'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 2, 'page_label': '3'}, page_content='fθ(e) and fed to a decoder for text generation.\\nMuRAG is built on top of a simpler model we\\ncall a “backbone” model, which is pre-trained to\\nencode image-text pairs such that they are suitable\\nfor both answer generation and retrieval. The back-\\nbone model’s encoder is used as a component of\\nthe MuRAG model. The backbone model is built\\nwith a pre-trained visual Transformer (Dosovitskiy\\net al., 2020) and a T5 text Transformer (Raffel et al.,\\n2020), and consists of a multimodal encoderfθ and\\ndecoder gθ. The encoder takes as input a sequence\\nof image-text pairs, where either the image or the\\ntext component can be empty to accommodate text-\\nonly and image-only cases.\\nAs depicted in Figure 3, the encoder can take a\\nsequence of images and text. For image input, we\\nﬁrst split each into 16x16 patches and feed them\\nto a ViT (Dosovitskiy et al., 2020) transformer to\\ngenerate a sequence of visual embedding denoted\\nas eI ∈RLi×D, where Li is the length of the im-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 2, 'page_label': '3'}, page_content='age tokens. For text input, we use word embedding\\nto produce another sequence of textual embedding\\neT ∈RLt×D. For k images and n text inputs, we\\nconcatenate all their embeddings in the input or-\\nder as e = [e1\\nI; e1\\nT; ··· ; ek\\nI; en\\nT] ∈R(kLt+nLi)×D,\\nwhich is fed to another bi-directional transformer\\nfθ initialized from T5. We enable cross-attention'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4'}, page_content='between the two modalities to produce a fused rep-\\nresentation, denoted as fθ(e) ∈ R(kLt+nLi)×D.\\nWe add a [CLS] token to obtain a pooled repre-\\nsentation fθ(e)[CLS] ∈RD for dense retrieval.\\n3.2 MuRAG\\nWe build MuRAG (shown in Figure 4) on top of\\nthe backbone model. During the retriever stage,\\nMuRAG takes a query q of any modality as in-\\nput and retrieves from a memory Mof image-text\\npairs. Speciﬁcally, we apply the backbone encoder\\nfθ to encode a query q, and use maximum inner\\nproduct search (MIPS (Guo et al., 2020)) over all of\\nthe memory candidates m ∈M to ﬁnd the Top-K\\nnearest neighbors TopK(M|q) = [ m1, ··· , mk].\\nFormally, we deﬁne TopK(M|q) as follows:\\nTopK(M|q) = TopK\\nm∈M\\nfθ(q)[CLS] ·fθ(m)[CLS]\\nDuring the reader stage, the retrievals (the raw im-\\nage patches) are combined with the query q as\\nan augmented input [m1, ··· , mk, q], which is fed\\nto the backbone encoder fθ to produce retrieval-\\naugmented encoding. The decoder model gθ uses'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4'}, page_content='attention over this representation to generate tex-\\ntual outputs y = y1, ··· , yn token by token.\\np(yi|yi−1) = gθ(yi|fθ(TopK(M|q); q); y1:i−1)\\nwhere y is decoded from a given vocabulary V.\\n3.3 Pre-training\\nThe pre-training implementation is depicted in the\\nupper portion of Figure 4, where the input query\\nis an image xI plus a text prompt xp. The exter-\\nnal memory Mcontains textual-only entries mT.\\nThe Top-K retrievalsmT\\n1 , ··· , mT\\nk are leveraged to\\ngenerate the textual output. To avoid the excessive\\ncomputation cost of backpropagation over the mas-\\nsive external memory, we adopt an in-batch mem-\\nory MB, dynamically constructed from the input\\nexamples in a batch. The small in-batch memory\\nenables MuRAG to continuously update the mem-\\nory encoder efﬁciently similar to TOME (de Jong\\net al., 2022) and QAMAT (Chen et al., 2022).\\nDataset The pre-training corpus consists of\\nLAION (Schuhmann et al., 2021), Conceptual-\\nCaption-12M+3M (CC) (Sharma et al., 2018;'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4'}, page_content='Changpinyo et al., 2021), VQA (Antol et al., 2015)\\nand PAQ (Lewis et al., 2021) Table 1. LAION is\\na publicly-released image-text dataset containing\\ncrawled image-text pairs ﬁltered by CLIP (Rad-\\nford et al., 2021). We apply rules to ﬁlter LAION\\nfrom 400M to 200M by removing text with HTTP-\\nURLs or image width/height beyond 1000 pixels.\\nCC contains 15M (image, anonymized alt-text)\\npairs crawled from the web but ﬁltered more ex-\\ntensively to maintain high alignment quality. VQA\\ncontains annotated QA pairs aligned to MSCOCO\\nimages. We further add captions to each image\\nfrom MSCOCO-Captioning (Lin et al., 2014) to\\ncreate (Image, Caption, QA) triples. PAQ is a text-\\nonly dataset containing 65M machine-generated\\nQA pairs along with their source Wikipedia pas-\\nsage.\\nDataset #Size Format Source\\nCC 15M (Image, Caption) Crawled\\nLAION 200M (Image, Alt-Text) Crawled\\nPAQ 65M (Passage, QA) Generated\\nVQA 400K (Image, Caption, QA) Annotated\\nTable 1: Pre-training Dataset Statistics'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4'}, page_content='For LAION and CC, we use the input image as\\nxI, and ‘generate caption:’ as the text promptxp.\\nFor VQA, we use the input image as xI and the\\nquestion as the prompt xp. For PAQ, we use an\\nempty array as the input image and the question\\nas the prompt. The in-batch memory MB is con-\\nstructed by stacking the captions associated with\\nthe input images in LAION/CC/VQA and the pas-\\nsages associated with the questions in PAQ. Each\\ntextual memory entry is denoted as mT. The de-\\ncoder is optimized to generate either a caption or\\nan answer, depending on the source dataset. Since\\nthe four dataset sizes are highly unbalanced, we\\nuse ﬁxed mixture sampling ratios to balance their\\npresence during pre-training.\\nWe train the model with a joint loss L = Lgen +\\nLcon as follows:\\nLcon = −log exp(fθ(xI, xp) ·fθ(mT))∑\\nm∈MB\\nexp(fθ(xI, xp) ·fθ(mT))\\nLgen = −log gθ(y|fθ(Mp; xI; xp))\\nMp =\\n{\\nTopK(MB|xI, xp) If (xI, xp) ∈PAQ/VQA\\nØ If (xI, xp) ∈LAION/CC\\nwhere Mp is the retrieved augmentation: if the'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 3, 'page_label': '4'}, page_content='input query is from PAQ/VQA, we use the retrieved\\nmemory entries, otherwise, we use null. The reason\\nfor setting it to null for LAION/CC is to avoid a\\ntrivial solution when the generation target (caption)\\nalso exactly appears in the memory.\\nThe contrastive loss Lcon is minimized to dis-\\ncriminate between the positive query-memory pairs'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 4, 'page_label': '5'}, page_content='Figure 4: Model Architecture: the model accesses an external memory to obtain multimodal knowledge contained\\nin images or text snippets, which is used to augment the generation. The upper part deﬁnes the pre-training\\nimplementation, while the lower part deﬁnes ﬁne-tuning implementation.\\nand all other query-memory pairs from the mem-\\nory. The pairwise matching score is computed as\\nthe dot product between query fθ(xI; xp)[CLS] and\\ncandidates fθ(mT)[CLS]. This objective enables\\nthe model to retrieve the most relevant knowledge\\nfrom the memory. The generative loss Lgen is min-\\nimized to generate target tokens y conditioned on\\nthe retrieval-augmented representation. This ob-\\njective enables the model to combine information\\nacross different modalities for text generation.\\n3.4 Fine-tuning\\nWe ﬁnetune MuRAG to align with the expected\\ninputs of the downstream datasets which require an-\\nswering text questions by retrieving image-caption\\npairs or text snippets from the external knowledge'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 4, 'page_label': '5'}, page_content='datastore. As depicted in the lower part of Figure 4,\\nthe input query for the downstream task is a text\\nquestion xq, and the memory Mcontaining (im-\\nage, text) pairs (mI, mT).1 The Top-K retrievals\\n{(mI\\n1, mT\\n1 ), ··· , (mI\\nk, mT\\nk)}are leveraged to gen-\\nerate the answer a. To minimize the computation\\ncost, we develop a two-stage pipeline to optimize\\nwith an in-batch memory and then resume with\\nﬁxed retrieval from global memory.\\nIn-Batch Training In this stage, we aim to mini-\\nmize the joint loss functionL = Lcon+Lgen based\\n1We set the image to a zero array if the memory entry is a\\ntext snippet.\\non the in-batch memory MB as follows:\\nLcon = −log exp(fθ(xq) ·fθ(mI; mT))∑\\nm∈MB\\nexp(fθ(xq) ·fθ(mI; mT))\\nLgen = −log gθ(y|fθ(TopK(MB|xq); xq))\\nThe in-batch memory MB is constructed in the\\nfollowing way: the k-th example in the dataset is\\nrepresented as (xq,k, yk, {mI\\ni, mI\\ni}k, {¯mI\\nj, ¯mT\\nj }k),\\nwhere m represents the positive (image, text)\\nsource, and ¯m represents the hard negative'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 4, 'page_label': '5'}, page_content='(image, text) source provided by the dataset 2.\\nFor a batch with B examples, we assemble\\nall the associated positive and negative knowl-\\nedge source as our in-batch memory MB =\\n{{mI\\ni, mI\\ni}1, {¯mI\\nj, ¯mT\\nj }1, ··· , {¯mI\\nj, ¯mT\\nj }B}.\\nFixed-Retrieval Training After in-batch train-\\ning, we encode all available cross-modal pairs, and\\nindex these encodings for fast MIPS retrieval. We\\nthen apply the trained retriever to search over the\\nfull multimodal corpus Mto obtain the global top-\\nK retrievals TopK(M|xq) and continue to opti-\\nmize Lgen. During this training phase, the stored\\nencodings are not updated. During inference time,\\nwe use ﬁxed encodings to generate the answers.\\n2These hard negatives are mined through Bing Search API\\nand Wikipedia page, refer to (Chang et al., 2022) for details.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 5, 'page_label': '6'}, page_content='4 Experiments\\n4.1 Implementation Details\\nThe backbone model uses T5-base (Raffel et al.,\\n2020) and a ViT-large model (Dosovitskiy et al.,\\n2020) as described in Table 2. We adopt the\\nsentence-piece model from T5 with a vocabulary\\nsize of 32128. The ViT model was pre-trained\\non the JFT dataset. We resize every image into\\n224x224 pixels and split them into a sequence of\\n16x16 patches. The output of ViT is a sequence\\nof 1024-dimension vectors, which are projected\\nto 768-dimension for consistency with T5 model.\\nMuRAG reuses the model as retriever and reader,\\nthus the full model size is 527M parameters.\\nModel #Enc #Dec Hidden Heads Params\\nViT-large 24 0 1024 16 307M\\nT5-base 12 12 768 12 220M\\nTable 2: The model size and conﬁgurations, with\\n#Enc/#Dec denoting encoder/decoder layers.\\nOur model is implemented in JAX (Bradbury\\net al., 2018), based on the T5X codebase (Roberts\\net al., 2022). During pre-training, we ﬁrst train the\\nmodel on LAION for 1M steps, and then continue'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 5, 'page_label': '6'}, page_content='training on CC/PAQ/VQA with 1:1:1 sample ratio\\nfor another 200K steps. We optimize the model\\nwith Adafactor (Shazeer and Stern, 2018). For both\\nstages, we adopt a constant learning rate of 5e-4\\nand a batch size of 4096. The models are trained\\non 64 Cloud v4 TPUs (Jouppi et al., 2020).\\nWe then ﬁne-tune MuRAG on WebQA and Mul-\\ntimodalQA with a constant learning rate of 3e-4\\nfor 20K steps. The checkpoint with the highest\\nvalidation score is run on the test set. We use a\\nbatch size of 64 and set TopK=4 for both in-batch\\ntraining and ﬁxed-retrieval training. We noticed\\nthat increasing Top-K further does not yield further\\nimprovement. We use a beam size of 2 to search\\nfor the best hypothesis for both datasets (increasing\\nit further doesn’t yield better performance).\\n4.2 Datasets\\nFor evaluation, we choose two multimodal QA\\ndatasets: WebQA (Chang et al., 2022) and Mul-\\ntimodalQA (Talmor et al., 2021) and demonstrate\\ntheir statistics in Table 3.\\nWebQA This dataset contains multi-hop, multi-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 5, 'page_label': '6'}, page_content='modal question-answer pairs where all questions\\nare knowledge-seeking queries. The queries re-\\nquire 1-2 images or 1-2 text snippets to answer.\\nDataset Train Dev Test\\nImage/Text Image/Text Image/Text\\nWebQA 18K/17K 2.5K/2.4K 3.4K/4K\\nMultimodalQA 2.1K/7.4K 230/721 -\\nTable 3: Overall Statistics of downstream dataset.\\nEach query in WebQA is associated with a set of\\nvisual/text distractors (hard negatives). The an-\\nswers in WebQA are normally complete sentences\\nto better assess the model’s generation capabil-\\nity. Two evaluation setups are used, namely dis-\\ntractor and full-wiki. Under the distractor setup,\\nthe model needs to retrieve from these hard neg-\\natives + positives to answer the question. Under\\nthe full-wiki setup, the model needs to search over\\n1.1M text and visual sources from Wikipedia to an-\\nswer the question. For evaluation, WebQA uses\\nBARTScore (Yuan et al., 2021) to measure the\\nﬂuency between the generation and the reference,\\nand keyword accuracy score to measure the cor-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 5, 'page_label': '6'}, page_content='rectness/truthfulness of the generation. These two\\nscores are multiplied to calculate the overall score.\\nMultimodalQA-Subset This dataset contains\\nhuman-annotated multimodal questions over differ-\\nent modalities including tables, text, and images.\\nWikipedia tables are used as anchors to connect dif-\\nferent modalities. The authors ﬁrst use the template\\nto generate questions and then ask crowd-workers\\nto ﬁlter and paraphrase the generated questions.\\nSince tables are outside the scope of our paper, we\\nfocus on the subset of queries requiring only text\\nand image information. Speciﬁcally, we choose the\\nquestions with types of ‘TextQ’ and ‘ImageQ’ to\\nconstruct the subset. The query requires 1 image\\nor 1 text snippet to answer. Each query in Multi-\\nmodalQA is also associated with visual and text dis-\\ntractors (hard negatives). Similarly, two evaluation\\nsetups are used as before. Under a full-wiki setup,\\nMultimodalQA uses a database containing 500K'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 5, 'page_label': '6'}, page_content='text and visual sources. The evaluation scores are\\nbased on Exact Match and F1.\\n4.3 Baselines\\nFor WebQA and MultimodalQA, we mainly\\ncompare different variants of pre-trained vision-\\nlanguage models.\\nVLP In WebQA, VLP-like models (Zhou et al.,\\n2020) like Oscar (Li et al., 2020) and VinvL (Zhang\\net al., 2021) are used as the standard baselines.\\nThese models were pre-trained on Conceptual'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 6, 'page_label': '7'}, page_content='3M (Sharma et al., 2018) with a masked language\\nobjective. During ﬁne-tuning, the VLP model takes\\na set of token inputs <[CLS], si, [SEP], Q, [SEP]>\\nﬁrst to select the most plausible source si, and then\\nfeed si in the form of <[CLS], S, Q, A, [SEP]>\\nto autoregressively decode answer A with masked\\nlanguage model prediction.\\nAutoRouting In MultimodalQA, this method\\nﬁrst applies a question type classiﬁer to detect the\\nmodality of the question (either a passage or an\\nimage), and then routes the question to its sub-\\nmodel. The method uses RoBERTa-large (Roberts\\net al., 2022) for text-questions and VilBERT (Lu\\net al., 2019) with features extracted from Faster-\\nRCNN (Ren et al., 2015) for image questions.\\nCLIP (K) CLIP (Radford et al., 2021) is used for\\nfull-wiki retrieval. Speciﬁcally, the baselines sys-\\ntems adopt CLIP to encode queries and all the im-\\nage/text candidates separately into vectors and then\\nrun approximated nearest neighbor searches to ﬁnd'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 6, 'page_label': '7'}, page_content='a set of K potential candidates. After the coarse-\\nlevel retrieval without cross-attention, it adopts a\\nreranker to further narrow down to the 1-2 candi-\\ndates to feed as input S to the QA model.\\n4.4 Experimental Results\\nWe demonstrate WebQA’s results in Table 4. All\\nresults reported are the medium score from three\\nruns with different random seeds, and the variance\\nof the Overall score is within 0.2%. We can observe\\nthat MuRAG can signiﬁcantly outperform VLP\\nwith different backends including Oscar, ResNet,\\nand VinVL. In retrieval performance, our model\\noutperforms VLP by 15% in the full-wiki setting.\\nFor Fluency, our model outperforms VLP by 12%\\nunder the distractor setting and 14% under the full-\\nwiki setting. For Accuracy, our model manages\\nto achieve 16% under the distractor setting and\\neven 20% the under the full-wiki setting. These\\nimprovements reﬂect the high ﬂuency and accuracy\\nof MuRAG’s generation, and the improvement is\\nmore pronounced for full wiki.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 6, 'page_label': '7'}, page_content='We show the MultimodalQA results in Table 5.\\nWe can see that MuRAG is also able to vastly\\noutperform the routing-based multimodality QA\\nmodel. For text questions, our model improves\\nover AutoRouting by 10+% EM under both set-\\ntings. For image questions, the gap becomes more\\nsigniﬁcant, with 20+% improvement under both\\nsettings. Similarly, we ﬁnd that our model is more\\ncapable of handling full-wiki corpus.\\nEvaluation Distractor\\nMetrics Retr FL Accuracy Overall\\nQuestion-Only - 34.9 22.2 13.4\\nVLP (Oscar) 68.9 42.6 36.7 22.6\\nVLP + ResNeXt 69.0 43.0 37.0 23.0\\nVLP + VinVL 70.9 44.2 38.9 24.1\\nMuRAG 74.6 55.7 54.6 36.1\\nEvaluation Full-Wiki\\nCLIP (2) + VLP 11.9 34.2 24.1 14.6\\nCLIP (20) + VLP 24.0 36.1 27.2 16.1\\nMuRAG 39.7 50.7 47.8 31.5\\nTable 4: WebQA ofﬁcial test-set results indicated\\non leaderboard 3 as of May 2022. Retr denotes\\nthe retrieval-F1 score. FL refers to ﬂuency metric\\nBARTSCcore, and Accuracy refers to keyword match-\\ning F1 score, they are combined as Overall.\\nEvaluation Distractor'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 6, 'page_label': '7'}, page_content='Metrics Text Image All\\nEM F1 EM F1 EM\\nQuestion-Only 15.4 18.4 11.0 15.6 13.8\\nAutoRouting 49.5 56.9 37.8 37.8 46.6\\nMuRAG 60.8 67.5 58.2 58.2 60.2\\nEvaluation Full-Wiki\\nMetrics Text Image All\\nEM F1 EM F1 EM\\nCLIP (10) +\\nAutoRouting 35.6 40.2 32.5 32.5 34.7\\nMuRAG 49.7 56.1 56.5 56.5 51.4\\nTable 5: Multimodal dev-set results on the subset.\\n4.5 Ablation Study\\nHere we ablate the properties of MuRAG to better\\nunderstand our experimental results.\\nPre-training Corpus In order to study the contri-\\nbutions of different pre-training corpora, we investi-\\ngated several pre-training corpus combinations. We\\nreport their ﬁne-tuned results on WebQA test set\\nin Table 6. As can be seen, without any pre-training,\\nour model only achieves an overall score of 23.5,\\nwhich lags behind the baseline models. After pre-\\ntraining on different singular datasets, MuRAG is\\nable to achieve better performance than the base-\\nlines. Among the individual datasets, LAION is\\nshown to yield the highest score, and adding CC,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 6, 'page_label': '7'}, page_content='PAQ, and VQA to the pre-training corpus set one\\nby one produces steady improvements.\\nTwo-Stage Fine-tuning In order to study the ne-\\ncessity of the two-stage ﬁne-tuning, we perform an\\nablation study to see the impact of the two stages.\\nWe display our results in Table 7. (Only In-Batch)'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 7, 'page_label': '8'}, page_content='Pre-train Dataset FL Accuracy Overall\\nNone 42.5 36.1 23.5\\nCC 46.4 41.3 25.6\\nLAION 47.8 44.8 28.3\\nVQA 47.0 44.4 27.4\\nPAQ 46.8 42.8 27.0\\nLAION+CC 49.5 47.4 30.7\\nLAION+CC+PAQ 53.7 51.8 34.4\\nLAION+CC+PAQ+VQA 55.7 54.6 36.1\\nTable 6: Ablation Study for different pre-training cor-\\npus, score under distractor setting.\\nModel WebQA Multimodal\\nMuRAG (Only In-Batch) 29.4 49.6\\nMuRAG (Only Fixed-Retrieval) 25.8 40.7\\nMuRAG (Two Stage) 31.5 51.4\\nTable 7: Ablation Study for different ﬁne-tuning stages\\nto see their contributions. WebQA uses the overall\\nscore, and MultimodalQA refers to EM-all score.\\nEvaluation Model Correct Wrong\\nDistractor MuRAG (Text) 80% 20%\\nMuRAG (Image) 64% 36%\\nFull-Wiki MuRAG (Text) 72% 28%\\nMuRAG (Image) 54% 46%\\nTable 8: The human evaluation results on WebQA\\ndataset separately for image/text queries.\\nrefers to the model trained only with in-batch mem-\\nory are directly used to generate outputs by access-\\ning the global memory. Without further tuning,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 7, 'page_label': '8'}, page_content='the performance will drop by roughly 2% on both\\ndatasets. (Only Fixed-Retrieval) refers to using the\\npre-trained retriever directly to obtain Top-K and\\nthen optimize the generative loss. As can be seen,\\nthe performance drop is more severe in this case\\nfor both datasets. This is understandable due the\\nmisalignment between pre-training retrieval is (im-\\nage + text->text) while the ﬁne-tuning retrieval is\\n(text -> image+text). Thus, it is necessary to adapt\\nthe MuRAG’s pre-trained retriever to different use\\ncases depending on the downstream datasets.\\n4.6 Human Analysis\\nIn order to better understand the model’s perfor-\\nmance, we manually study 200 model outputs and\\nclassify them into three categories and show our\\nmanual analysis results in Table 8. As can be seen,\\nimage queries are much harder than text queries.\\nMuRAG only achieves 64% accuracy for the dis-\\ntractor setting and 54% accuracy for the full-wiki\\nsetting, falling signiﬁcantly behind text accuracy.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 7, 'page_label': '8'}, page_content='We further categorize the image-query errors\\nFigure 5: Upper left: correct prediction, Upper Right:\\nerror due to miscounting, Lower: error due to misrecog-\\nnition (multiple image reasoning). Q refers to the ques-\\ntion, P refers to prediction and R refers to the reference.\\nmanually into the categories of Table 9. Counting\\nis the most difﬁcult question type, and constitutes\\n52% of the total errors, while object recognition\\nerrors rank second, constituting 29% of errors. In\\ncontrast, identifying color, shape, and gender is\\ncomparatively easier, with fairly low error rates.\\nWe demonstrate some correct and typical error\\ncases in Figure 5 including miscounting and mis-\\nrecognizing objects. We observe that these errors\\nare mostly due to several reasons: 1) the question\\nis related to infrequent objects, thus making recog-\\nnition errors, 2) the image scene is highly complex\\nwith a large number of objects, thus grounding to a\\nspeciﬁc region is difﬁcult, 3) the questions require'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 7, 'page_label': '8'}, page_content='optical character recognition ability from images.\\nHence, the bottleneck of MuRAG is still in the\\nvisual understanding module.\\nCategory Count Object Color Shape Gender\\nRatio 52% 29.4% 5.8% 5.8% 5.8%\\nTable 9: Error categorization and their ratios on sam-\\npled WebQA-dev image queries.\\n5 Examples\\nWe list more examples in Figure 6 and Figure 7.\\nAs can be seen, in the ﬁrst example, the model is'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 8, 'page_label': '9'}, page_content='grounded on the oracle image-text pair to make the\\ncorrect prediction. However, in the second exam-\\nple, though the model retrieves the wrong image-\\ntext pair, it is able to make the correct prediction of\\n‘the angel is holding a dead body’. We conjecture\\nthat the model utilizes textual clues to make the pre-\\ndiction rather than grounding on the image itself.\\nSuch shortcut learning is concerning and needs to\\nbe addressed through better learning algorithms.\\nFigure 6: Examples: we demonstrate model retrieval\\nvs. groundtruth and model answer vs. reference.\\n6 Conclusion\\nIn this paper, we build the ﬁrst visually-grounded\\nlanguage generator capable of retrieving multi-\\nmodal knowledge from a large-scale corpus. Our\\nexperiments show the promise of this approach, as\\nit outperforms existing baselines by a large margin.\\nAt the same time, the performance on knowledge-\\nseeking queries that require reasoning over images\\nis still signiﬁcantly lower than the performance on'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 8, 'page_label': '9'}, page_content='queries requiring only text. This indicates that there\\nis still ample room for further improvements and\\nwe hope our study can motivate more research on\\nbetter multimodal retrieval-augmented models.\\nLimitations\\nThe current approach has several limitations: 1)\\nsince we do not mine hard negatives during pre-\\ntraining, negatives come from other examples\\nwithin the same batch. This requires that we set the\\nbatch size sufﬁciently large enough to collect hard-\\nenough negatives. This results in the pre-training\\nFigure 7: Examples: we demonstrate model retrieval\\nvs. groundtruth, and model answer vs. reference.\\nrequiring a large number of computation resources\\nto reach competitive retrieval abilities. 2) our pre-\\ntraining corpus’s format (image -> text) is differ-\\nent from ﬁne-tuning (text -> image+text). This\\nmisalignment limits the model’s performance. Fu-\\nture work should consider how to design a better-\\naligned pre-training objective to achieve better per-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 8, 'page_label': '9'}, page_content='formance. 3) Current visual representation in the\\nreader stage is relatively expensive, i.e. 16x16=196\\ntokens per image, which poses great challenges for\\nthe transformer encoder to scale up to large Top-K\\nvalues due to the quadratic attention complexity.\\nEthical Statement\\nOur work uses the LAION dataset, a widely-used\\nand publicly available large-scale visual-language\\ncorpus crawled from the web. The authors have\\nconducted automatic ﬁltering to greatly reduce\\nharmful content. However, it is not possible to\\nfully remove all of the potential risks from the data\\ngiven its tremendous size. Being trained on this\\ndataset, we anticipate our model to contain some\\nbiases (racial, gender, etc.). During our manual\\ninspection, we saw some such biases, for example,\\n5% of errors are caused by misrecognition of gen-\\nder. However, there are other many other forms of\\nbiases that we cannot fully enumerate or observe'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10'}, page_content='explicitly.\\nReferences\\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\\ngaret Mitchell, Dhruv Batra, C Lawrence Zitnick,\\nand Devi Parikh. 2015. Vqa: Visual question an-\\nswering. In Proceedings of the IEEE international\\nconference on computer vision, pages 2425–2433.\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\\nmann, Trevor Cai, Eliza Rutherford, Katie Millican,\\nGeorge van den Driessche, Jean-Baptiste Lespiau,\\nBogdan Damoc, Aidan Clark, et al. 2021. Improv-\\ning language models by retrieving from trillions of\\ntokens. arXiv preprint arXiv:2112.04426.\\nJames Bradbury, Roy Frostig, Peter Hawkins,\\nMatthew James Johnson, Chris Leary, Dougal\\nMaclaurin, George Necula, Adam Paszke, Jake\\nVanderPlas, Skye Wanderman-Milne, and Qiao\\nZhang. 2018. JAX: composable transformations of\\nPython+NumPy programs.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10'}, page_content='learners. Advances in neural information processing\\nsystems, 33:1877–1901.\\nYingshan Chang, Mridu Narang, Hisami Suzuki, Gui-\\nhong Cao, Jianfeng Gao, and Yonatan Bisk. 2022.\\nWebqa: Multihop and multimodal qa. The Confer-\\nence on Computer Vision and Pattern Recognition.\\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and\\nRadu Soricut. 2021. Conceptual 12m: Pushing web-\\nscale image-text pre-training to recognize long-tail\\nvisual concepts. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recog-\\nnition, pages 3558–3568.\\nWenhu Chen, Pat Verga, Michiel de Jong, John Wi-\\neting, and William Cohen. 2022. Augmenting\\npre-trained language models with qa-memory for\\nopen-domain question answering. arXiv preprint\\narXiv:2204.04581.\\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-\\nishna Vedantam, Saurabh Gupta, Piotr Dollár, and\\nC Lawrence Zitnick. 2015. Microsoft coco captions:\\nData collection and evaluation server.arXiv preprint\\narXiv:1504.00325.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10'}, page_content='arXiv:1504.00325.\\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed\\nEl Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\\nJingjing Liu. 2020. Uniter: Universal image-text\\nrepresentation learning. In European conference on\\ncomputer vision, pages 104–120. Springer.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\\nPaul Barham, Hyung Won Chung, Charles Sutton,\\nSebastian Gehrmann, et al. 2022. Palm: Scaling\\nlanguage modeling with pathways. arXiv preprint\\narXiv:2204.02311.\\nMichiel de Jong, Yury Zemlyanskiy, Nicholas FitzGer-\\nald, Fei Sha, and William Cohen. 2022. Mention\\nmemory: incorporating textual knowledge into trans-\\nformers through entity mention attention. ICLR.\\nAlexey Dosovitskiy, Lucas Beyer, Alexander\\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\\nThomas Unterthiner, Mostafa Dehghani, Matthias\\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2020.\\nAn image is worth 16x16 words: Transformers\\nfor image recognition at scale. In International'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10'}, page_content='Conference on Learning Representations.\\nRuiqi Guo, Philip Sun, Erik Lindgren, Quan Geng,\\nDavid Simcha, Felix Chern, and Sanjiv Kumar. 2020.\\nAccelerating large-scale inference with anisotropic\\nvector quantization. In International Conference on\\nMachine Learning, pages 3887–3896. PMLR.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-\\nsupat, and Mingwei Chang. 2020. Retrieval aug-\\nmented language model pre-training. In Proceed-\\nings of the 37th International Conference on Ma-\\nchine Learning, volume 119 of Proceedings of Ma-\\nchine Learning Research, pages 3929–3938. PMLR.\\nDarryl Hannan, Akshay Jain, and Mohit Bansal. 2020.\\nManymodalqa: Modality disambiguation and qa\\nover diverse inputs. In Proceedings of the AAAI Con-\\nference on Artiﬁcial Intelligence , volume 34, pages\\n7879–7886.\\nGautier Izacard and Édouard Grave. 2021. Leveraging\\npassage retrieval with generative models for open\\ndomain question answering. In Proceedings of the\\n16th Conference of the European Chapter of the As-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10'}, page_content='sociation for Computational Linguistics: Main Vol-\\nume, pages 874–880.\\nNorman P Jouppi, Doe Hyun Yoon, George Kurian,\\nSheng Li, Nishant Patil, James Laudon, Cliff Young,\\nand David Patterson. 2020. A domain-speciﬁc\\nsupercomputer for training deep neural networks.\\nCommunications of the ACM, 63(7):67–78.\\nAndrej Karpathy and Li Fei-Fei. 2015. Deep visual-\\nsemantic alignments for generating image descrip-\\ntions. In Proceedings of the IEEE conference\\non computer vision and pattern recognition , pages\\n3128–3137.\\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\\nZettlemoyer, and Mike Lewis. 2019. Generalization\\nthrough memorization: Nearest neighbor language\\nmodels. In International Conference on Learning\\nRepresentations.\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\\ntäschel, et al. 2020. Retrieval-augmented generation\\nfor knowledge-intensive nlp tasks. Advances in Neu-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 9, 'page_label': '10'}, page_content='ral Information Processing Systems, 33:9459–9474.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11'}, page_content='Patrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale\\nMinervini, Heinrich Küttler, Aleksandra Piktus, Pon-\\ntus Stenetorp, and Sebastian Riedel. 2021. Paq: 65\\nmillion probably-asked questions and what you can\\ndo with them. Transactions of the Association for\\nComputational Linguistics, 9:1098–1115.\\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xi-\\naowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu,\\nLi Dong, Furu Wei, et al. 2020. Oscar: Object-\\nsemantics aligned pre-training for vision-language\\ntasks. In European Conference on Computer Vision,\\npages 121–137. Springer.\\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\\nand C Lawrence Zitnick. 2014. Microsoft coco:\\nCommon objects in context. In European confer-\\nence on computer vision, pages 740–755. Springer.\\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\\nLee. 2019. Vilbert: Pretraining task-agnostic visi-\\nolinguistic representations for vision-and-language'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11'}, page_content='tasks. Advances in neural information processing\\nsystems, 32.\\nKenneth Marino, Mohammad Rastegari, Ali Farhadi,\\nand Roozbeh Mottaghi. 2019. Ok-vqa: A visual\\nquestion answering benchmark requiring external\\nknowledge. In Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recogni-\\ntion, pages 3195–3204.\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish\\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark,\\net al. 2021. Learning transferable visual models\\nfrom natural language supervision. In International\\nConference on Machine Learning, pages 8748–8763.\\nPMLR.\\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\\nine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\\nthe limits of transfer learning with a uniﬁed text-to-\\ntext transformer. Journal of Machine Learning Re-\\nsearch, 21(140):1–67.\\nRevanth Gangi Reddy, Xilin Rui, Manling Li, Xudong\\nLin, Haoyang Wen, Jaemin Cho, Lifu Huang, Mo-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11'}, page_content='hit Bansal, Avirup Sil, Shih-Fu Chang, et al. 2021.\\nMumuqa: Multimedia multi-hop news question an-\\nswering via cross-media knowledge extraction and\\ngrounding. arXiv preprint arXiv:2112.10728.\\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian\\nSun. 2015. Faster r-cnn: Towards real-time object\\ndetection with region proposal networks. Advances\\nin neural information processing systems, 28.\\nAdam Roberts, Hyung Won Chung, Anselm Lev-\\nskaya, Gaurav Mishra, James Bradbury, Daniel An-\\ndor, Sharan Narang, Brian Lester, Colin Gaffney,\\nAfroz Mohiuddin, et al. 2022. Scaling up mod-\\nels and data with t5x and seqio. arXiv preprint\\narXiv:2203.17189.\\nChristoph Schuhmann, Richard Vencu, Romain Beau-\\nmont, Robert Kaczmarczyk, Clayton Mullis, Aarush\\nKatta, Theo Coombes, Jenia Jitsev, and Aran Komat-\\nsuzaki. 2021. Laion-400m: Open dataset of clip-\\nﬁltered 400 million image-text pairs. arXiv preprint\\narXiv:2111.02114.\\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\\nRadu Soricut. 2018. Conceptual captions: A'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11'}, page_content='cleaned, hypernymed, image alt-text dataset for au-\\ntomatic image captioning. In Proceedings of the\\n56th Annual Meeting of the Association for Compu-\\ntational Linguistics (Volume 1: Long Papers), pages\\n2556–2565.\\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\\nAdaptive learning rates with sublinear memory cost.\\nIn International Conference on Machine Learning ,\\npages 4596–4604. PMLR.\\nHrituraj Singh, Anshul Nasery, Denil Mehta, Aish-\\nwarya Agarwal, Jatin Lamba, and Balaji Vasan Srini-\\nvasan. 2021. Mimoqa: Multimodal input multi-\\nmodal output question answering. In Proceedings\\nof the 2021 Conference of the North American Chap-\\nter of the Association for Computational Linguistics:\\nHuman Language Technologies, pages 5317–5332.\\nAlon Talmor, Ori Yoran, Amnon Catav, Dan Lahav,\\nYizhong Wang, Akari Asai, Gabriel Ilharco, Han-\\nnaneh Hajishirzi, and Jonathan Berant. 2021. Multi-\\nmodalqa: complex question answering over text, ta-\\nbles and images. In ICLR.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11'}, page_content='Pat Verga, Haitian Sun, Livio Baldini Soares, and\\nWilliam Weston Cohen. 2021. Adaptable and inter-\\npretable neural memory over symbolic knowledge.\\nIn Proceedings of NAACL-HLT, pages 3678–3691.\\nZirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yu-\\nlia Tsvetkov, and Yuan Cao. 2022. Simvlm: Simple\\nvisual language model pretraining with weak super-\\nvision. ICLR.\\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Ye-\\nung, Mojtaba Seyedhosseini, and Yonghui Wu. 2022.\\nCoca: Contrastive captioners are image-text founda-\\ntion models. arXiv preprint arXiv:2205.01917.\\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\\nBartscore: Evaluating generated text as text gener-\\nation. Advances in Neural Information Processing\\nSystems, 34.\\nPengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei\\nYang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jian-\\nfeng Gao. 2021. Vinvl: Revisiting visual representa-\\ntions in vision-language models. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 10, 'page_label': '11'}, page_content='Pattern Recognition, pages 5579–5588.\\nLuowei Zhou, Hamid Palangi, Lei Zhang, Houdong\\nHu, Jason Corso, and Jianfeng Gao. 2020. Uni-\\nﬁed vision-language pre-training for image caption-\\ning and vqa. In Proceedings of the AAAI Conference\\non Artiﬁcial Intelligence , volume 34, pages 13041–\\n13049.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 11, 'page_label': '12'}, page_content='A Pre-training\\nDuring Pre-trainnig, we found that directly train-\\ning with a mixture of all four datasets will lead to\\ninstability. We experimented with different vari-\\nants and found that a scheduled pre-training can\\nlead to a stable solution. We propose to ﬁrst pre-\\ntrain the model on the largest LAION dataset for\\n1M steps, and then continue training on the other\\nthree datasets with a ﬁxed sample ratio. We plot\\nthe ﬁrst stage of LAION training in Figure 8. We\\nmonitor the generation quality (LAION image ->\\ntext captioning), and the retrieval quality (image ->\\n4096 in-batch caption retrieval). As can be seen,\\nthe LAION pre-training converges after 1M steps,\\nwhere we ﬁrst warm up and then decrease the learn-\\ning rate using a scheduler.\\nFigure 8: LAION Pre-training, validation accuracy,\\ngeneration Cider score and retrieval recall score from\\nthe in-batch memory.\\nWe further the pre-training on a mixture of the\\nother three datasets. We plot their inference eval-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 11, 'page_label': '12'}, page_content='uation scores in Figure 9. We can see that the\\nmodel is able to achieve very strong performance\\non these datasets, i.e. higher than 1.2 CiDEr\\non CC12M+3M validation set. The model also\\nachieves strong performance on text-only reading\\ncomprehension on PAQ (similar to NQ), i.e. higher\\nthan 55% EM score. On the VQA dataset, the\\nmodel is able to achieve higher than 72% VQA ac-\\ncuracy on the validation set. These results demon-\\nstrate the efﬁciency and multi-tasking capabilities\\nof the pre-trained model. The overall retrieval\\naccuracy from the multimodal memory consist-\\ning of captions, and passages are plotted in Fig-\\nure 10, where the model is able to achieve 85%\\nRECALL@1 from a 4K memory.\\nB Model Conﬁguration\\nWe demonstrate the ViT conﬁguration as follows:\\n\" v i t _ c o n f i g \" : {\\n\" model \" : \" ViT \" ,\\n\" p a t c h e s \" : {\\n\" s i z e \" : [ 1 6 , 16]\\n} ,\\n\" h i d d e n _ s i z e \" : 1024 ,\\n\" i m a g e _ s i z e \" : [ 2 2 4 , 2 2 4 ] ,\\n\" num_heads \" : 16 ,\\n\" n u m _ l a y e r s \" : 24 ,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 11, 'page_label': '12'}, page_content='\" mlp_dim \" : 4096 ,\\n\" r e t u r n _ p o o l e d _ o u t p u t \" : f a l s e ,\\n\" d r o p o u t _ r a t e \" : 0 . 1\\n} ,\\nWe demonstrate the T5-EncDec conﬁguration as\\nfollows:\\n\" m o d e l _ c o n f i g \" : {\\n\" v o c a b _ s i z e \" : 32128 ,\\n\" h i d d e n _ s i z e \" : 768 ,\\n\" i n t e r m e d i a t e _ d i m \" : 2048 ,\\n\" n u m _ a t t e n t i o n _ h e a d s \" : 12 ,\\n\" memory_key_dim \" : 768 ,\\n\" e n c o d e r _ l a y e r s \" : 12 ,\\n\" d e c o d e r _ l a y e r s \" : 12 ,\\n\" d r o p o u t _ r a t e \" : 0 . 1 ,\\n\" m a x _ d i s t a n c e \" : 128 ,\\n\" num_buckets \" : 32 ,\\n\" s c a l e \" : 1 . 0 ,\\n\" r e t r i e v a l _ w e i g h t \" : 0 . 5 ,\\n}'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-10-21T00:59:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-10-21T00:59:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-5.pdf', 'total_pages': 13, 'page': 12, 'page_label': '13'}, page_content='Figure 9: Mixture Pre-training, CiDEr, EM, and VQA\\naccuracy for CC, PAQ, and VQA datasets.\\nFigure 10: Mixture Pre-training retrieval accuracy over\\nCC, PAQ, and VQA datasets.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1'}, page_content='Ragas: Automated Evaluation of Retrieval Augmented Generation\\nShahul Es†, Jithin James†, Luis Espinosa-Anke∗♢, Steven Schockaert∗\\n†Exploding Gradients\\n∗CardiffNLP, Cardiff University, United Kingdom\\n♢AMPLYFI, United Kingdom\\nshahules786@gmail.com,jamesjithin97@gmail.com\\n{espinosa-ankel,schockaerts1}@cardiff.ac.uk\\nAbstract\\nWe introduce Ragas (Retrieval Augmented\\nGeneration Assessment), a framework for\\nreference-free evaluation of Retrieval Aug-\\nmented Generation (RAG) pipelines. RAG\\nsystems are composed of a retrieval and an\\nLLM based generation module, and provide\\nLLMs with knowledge from a reference textual\\ndatabase, which enables them to act as a natu-\\nral language layer between a user and textual\\ndatabases, reducing the risk of hallucinations.\\nEvaluating RAG architectures is, however, chal-\\nlenging because there are several dimensions to\\nconsider: the ability of the retrieval system to\\nidentify relevant and focused context passages,\\nthe ability of the LLM to exploit such passages'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1'}, page_content='in a faithful way, or the quality of the genera-\\ntion itself. With Ragas, we put forward a suite\\nof metrics which can be used to evaluate these\\ndifferent dimensions without having to rely on\\nground truth human annotations. We posit that\\nsuch a framework can crucially contribute to\\nfaster evaluation cycles of RAG architectures,\\nwhich is especially important given the fast\\nadoption of LLMs.\\n1 Introduction\\nLanguage Models (LMs) capture a vast amount\\nof knowledge about the world, which allows them\\nto answer questions without accessing any exter-\\nnal sources. This idea of LMs as repositories of\\nknowledge emerged shortly after the introduction\\nof BERT (Devlin et al., 2019) and became more\\nfirmly established with the introduction of ever\\nlarger LMs (Roberts et al., 2020). While the most\\nrecent Large Language Models (LLMs) capture\\nenough knowledge to rival human performance\\nacross a wide variety of question answering bench-\\nmarks (Bubeck et al., 2023), the idea of using'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1'}, page_content='LLMs as knowledge bases still has two fundamen-\\ntal limitations. First, LLMs are not able to answer\\nquestions about events that have happened after\\nthey were trained. Second, even the largest models\\nstruggle to memorise knowledge that is only rarely\\nmentioned in the training corpus (Kandpal et al.,\\n2022; Mallen et al., 2023). The standard solution\\nto these issues is to rely on Retrieval Augmented\\nGeneration (RAG) (Lee et al., 2019; Lewis et al.,\\n2020; Guu et al., 2020). Answering a question\\nthen essentially involves retrieving relevant pas-\\nsages from a corpus and feeding these passages,\\nalong with the original question, to the LM. While\\ninitial approaches relied on specialised LMs for\\nretrieval-augmented language modelling (Khandel-\\nwal et al., 2020; Borgeaud et al., 2022), recent work\\nhas suggested that simply adding retrieved docu-\\nments to the input of a standard LM can also work\\nwell (Khattab et al., 2022; Ram et al., 2023; Shi'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1'}, page_content='et al., 2023), thus making it possible to use retrieval-\\naugmented strategies in combination with LLMs\\nthat are only available through APIs.\\nWhile the usefulness of retrieval-augmented\\nstrategies is clear, their implementation requires\\na significant amount of tuning, as the overall per-\\nformance will be affected by the retrieval model,\\nthe considered corpus, the LM, or the prompt for-\\nmulation, among others. Automated evaluation of\\nretrieval-augmented systems is thus paramount. In\\npractice, RAG systems are often evaluated in terms\\nof the language modelling task itself, i.e. by mea-\\nsuring perplexity on some reference corpus. How-\\never, such evaluations are not always predictive\\nof downstream performance (Wang et al., 2023c).\\nMoreover, this evaluation strategy relies on the LM\\nprobabilities, which are not accessible for some\\nclosed models (e.g. ChatGPT and GPT-4). Ques-\\ntion answering is another common evaluation task,\\nbut usually only datasets with short extractive an-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1'}, page_content='swers are considered, which may not be represen-\\ntative of how the system will be used.\\nTo address these issues, in this paper we present\\nRagas1, a framework for the automated assessment\\n1Ragas is available at https://github.com/\\nexplodinggradients/ragas.\\narXiv:2309.15217v2  [cs.CL]  28 Apr 2025'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2'}, page_content='of retrieval augmented generation systems. We fo-\\ncus on settings where reference answers may not be\\navailable, and where we want to estimate different\\nproxies for correctness, in addition to the useful-\\nness of the retrieved passages. The Ragas frame-\\nwork provides an integration with both llama-index\\nand Langchain, the most widely used frameworks\\nfor building RAG solutions, thus enabling devel-\\nopers to easily integrate Ragas into their standard\\nworkflow.\\n2 Related Work\\nEstimating faithfulness using LLMsThe prob-\\nlem of detecting hallucinations in LLM generated\\nresponses has been extensively studied (Ji et al.,\\n2023). Several authors have suggested the idea\\nof predicting factuality using a few-shot prompt-\\ning strategy (Zhang et al., 2023). Recent analy-\\nses, however, suggest that existing models struggle\\nwith detecting hallucination when using standard\\nprompting strategies (Li et al., 2023; Azaria and\\nMitchell, 2023). Other approaches rely on linking'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2'}, page_content='the generated responses to facts from an external\\nknowledge base (Min et al., 2023), but this is not\\nalways possible.\\nYet another strategy is to inspect the probabili-\\nties assigned to individual tokens, where we would\\nexpect the model to be less confident in halluci-\\nnated answers than in factual ones. For instance,\\nBARTScore (Yuan et al., 2021) estimates factuality\\nby looking at the conditional probability of the gen-\\nerated text given the input. Kadavath et al. (2022)\\nuse a variation of this idea. Starting from the ob-\\nservation that LLMs provide well-calibrated proba-\\nbilities when answering multiple-choice questions,\\nthey essentially convert the problem of validating\\nmodel generated answers into a multiple-choice\\nquestion which asks whether the answer is true or\\nfalse. Rather than looking at the output probabil-\\nities, Azaria and Mitchell (2023) propose to train\\na supervised classifier on the weights from one of\\nthe hidden layers of the LLM, to predict whether a'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2'}, page_content='given statement is true or not. While the approach\\nperforms well, the need to access the hidden states\\nof the model makes it unsuitable for systems that\\naccess LLMs through an API.\\nFor models that do not provide access to token\\nprobabilities, such as ChatGPT and GPT-4, differ-\\nent methods are needed. SelfCheckGPT (Manakul\\net al., 2023) addresses this problem by instead sam-\\npling multiple answers. Their core idea is that\\nfactual answers are more stable: when an answer is\\nfactual, we can expect that different samples will\\ntend to be semantically similar, whereas this is less\\nlikely to be the case for hallucinated answers.\\nAutomated evaluation of text generation systems\\nLLMs have also been leveraged to automatically\\nevaluate other aspects of generated text fragments,\\nbeyond factuality. For instance, GPTScore (Fu\\net al., 2023) uses a prompt that specifies the consid-\\nered aspect (e.g. fluency) and then scores passages\\nbased on the average probability of the generated'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2'}, page_content='tokens, according to a given autoregressive LM.\\nThis idea of using prompts was previously also\\nconsidered by Yuan et al. (2021), although they\\nused a smaller fine-tuned LM (i.e. BART) and did\\nnot observe a clear benefit from using prompts. An-\\nother approach directly asks ChatGPT to evaluate\\na particular aspect of the given answer by provid-\\ning a score between 0 and 100, or by providing a\\nrating on a 5-star scale (Wang et al., 2023a). Re-\\nmarkably, strong results can be obtained in this\\nway, although it comes with the limitation of being\\nsensitive to the design of the prompt. Rather than\\nscoring individual answers, some authors have also\\nfocused on using an LLM to select the best answer\\namong a number of candidates (Wang et al., 2023b),\\ntypically to compare the performance of different\\nLLMs. However, care is needed with this approach,\\nas the order in which the answers is presented can\\ninfluence the result (Wang et al., 2023b).\\nIn terms of how ground truth answers or, more'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2'}, page_content='generally, generations, have been typically used\\nin the literature, most approaches have relied on\\nthe availability of one or more reference answers.\\nFor instance, BERTScore (Zhang et al., 2020)\\nand MoverScore (Zhao et al., 2019) use contex-\\ntualised embeddings, produced by a pre-trained\\nBERT model, to compare the similarity between\\nthe generated answer and the reference answers.\\nBARTScore (Yuan et al., 2021) similarly uses refer-\\nence answers to compute aspects such as precision\\n(estimated as the probability of generating the gen-\\nerated answer given the reference) and recall (esti-\\nmated as the probability of generating the reference\\ngiven the generated answer).\\n3 Evaluation Strategies\\nWe consider a standard RAG setting, where given a\\nquestion q, the system first retrieves some context\\nc(q) and then uses the retrieved context to generate\\nan answer as(q). When building a RAG system,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3'}, page_content='we usually do not have access to human-annotated\\ndatasets or reference answers. We therefore fo-\\ncus on metrics that are fully self-contained and\\nreference-free. We focus in particular three quality\\naspects, which we argue are of central importance.\\nFirst, Faithfulness refers to the idea that the an-\\nswer should be grounded in the given context. This\\nis important to avoid hallucinations, and to ensure\\nthat the retrieved context can act as a justification\\nfor the generated answer. Indeed, RAG systems are\\noften used in applications where the factual con-\\nsistency of the generated text w.r.t. the grounded\\nsources is highly important, e.g. in domains such as\\nlaw, where information is constantly evolving. Sec-\\nond, Answer Relevancerefers to the idea that the\\ngenerated answer should address the actual ques-\\ntion that was provided. Finally,Context Relevance\\nrefers to the idea that the retrieved context should\\nbe focused, containing as little irrelevant informa-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3'}, page_content='tion as possible. This is important given the cost\\nassociated with feeding long context passages to\\nLLMs. Moreover, when context passages are too\\nlong, LLMs are often less effective in exploiting\\nthat context, especially for information that is pro-\\nvided in the middle of the context passage (Liu\\net al., 2023).\\nWe now explain how these three quality aspects\\ncan be measured in a fully automated way, by\\nprompting an LLM. In our implementation and\\nexperiments, all prompts are evaluated using the\\ngpt-3.5-turbo-16k model, which is available\\nthrough the OpenAI API2.\\nFaithfulness We say that the answer as(q) is\\nfaithful to the context c(q) if the claims that are\\nmade in the answer can be inferred from the con-\\ntext. To estimate faithfulness, we first use an LLM\\nto extract a set of statements, S(as(q)). The aim\\nof this step is to decompose longer sentences into\\nshorter and more focused assertions. We use the\\nfollowing prompt for this step3:\\nGiven a question and answer, create one'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3'}, page_content='or more statements from each sentence\\nin the given answer.\\nquestion: [question]\\nanswer: [answer]\\nwhere [question] and [answer] refer to the\\ngiven question and answer. For each statement si\\n2https://platform.openai.com\\n3To help clarify the task, we include a demonstration as\\npart of the prompt. This demonstration is not explicitly shown\\nin the listing of the prompts throughout this paper.\\nin S, the LLM determines ifsi can be inferred from\\nc(q) using a verification function v(si, c(q)). This\\nverification step is carried out using the following\\nprompt:\\nConsider the given context and following\\nstatements, then determine whether they\\nare supported by the information present\\nin the context. Provide a brief explana-\\ntion for each statement before arriving\\nat the verdict (Yes/No). Provide a final\\nverdict for each statement in order at the\\nend in the given format. Do not deviate\\nfrom the specified format.\\nstatement: [statement 1]\\n...\\nstatement: [statement n]'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3'}, page_content='The final faithfulness score, F, is then computed\\nas F = |V |\\n|S| , where |V | is the number of statements\\nthat were supported according to the LLM and |S|\\nis the total number of statements.\\nAnswer relevance We say that the answer as(q)\\nis relevant if it directly addresses the question in\\nan appropriate way. In particular, our assessment\\nof answer relevance does not take into account fac-\\ntuality, but penalises cases where the answer is\\nincomplete or where it contains redundant informa-\\ntion. To estimate answer relevance, for the given\\nanswer as(q), we prompt the LLM to generate n\\npotential questions qi based on as(q), as follows:\\nGenerate a question for the given answer.\\nanswer: [answer]\\nWe then obtain embeddings for all questions us-\\ning the text-embedding-ada-002 model, avail-\\nable from the OpenAI API. For each qi, we cal-\\nculate the similarity sim(q, qi) with the original\\nquestion q, as the cosine between the correspond-\\ning embeddings. The answer relevance score, AR,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3'}, page_content='for question q is then computed as:\\nAR = 1\\nn\\nnX\\ni=1\\nsim(q, qi) (1)\\nThis metric evaluates how closely the generated\\nanswer aligns with the initial question or instruc-\\ntion.\\nContext relevance The context c(q) is consid-\\nered relevant to the extent that it exclusively con-\\ntains information that is needed to answer the ques-\\ntion. In particular, this metric aims to penalise the'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4'}, page_content='inclusion of redundant information. To estimate\\ncontext relevance, given a question q and its con-\\ntext c(q), the LLM extracts a subset of sentences,\\nSext, from c(q) that are crucial to answer q, using\\nthe following prompt:\\nPlease extract relevant sentences from\\nthe provided context that can potentially\\nhelp answer the following question. If no\\nrelevant sentences are found, or if you\\nbelieve the question cannot be answered\\nfrom the given context, return the phrase\\n\"Insufficient Information\". While extract-\\ning candidate sentences you’re not al-\\nlowed to make any changes to sentences\\nfrom given context.\\nThe context relevance score is then computed as:\\nCR = number of extracted sentences\\ntotal number of sentences in c(q) (2)\\n4 The WikiEval Dataset\\nTo evaluate the proposed framework, we ideally\\nneed examples of question-context-answer triples\\nwhich are annotated with human judgments. We\\ncan then verify to what extent our metrics agree\\nwith human assessments of faithfulness, answer'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4'}, page_content='relevance and context relevance. Since we are not\\naware of any publicly available datasets that could\\nbe used for this purpose, we created a new dataset,\\nwhich we refer to as WikiEval4. To construct the\\ndataset, we first selected 50 Wikipedia pages cov-\\nering events that have happened since the start of\\n20225. In selecting these pages, we prioritised\\nthose with recent edits. For each of the 50 pages,\\nwe then asked ChatGPT to suggest a question that\\ncan be answered based on the introductory section\\nof the page, using the following prompt:\\nYour task is to formulate a question from\\ngiven context satisfying the rules given\\nbelow:\\n1. The question should be fully answered\\nfrom the given context.\\n2. The question should be framed from\\na part that contains non-trivial informa-\\ntion.\\n3. The answer should not contain any\\n4https://huggingface.co/datasets/\\nexplodinggradients/WikiEval\\n5That is, beyond the reported training cutoff of the model\\nwe used in our experiments.\\nlinks.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4'}, page_content='links.\\n4. The question should be of moderate\\ndifficulty.\\n5. The question must be reasonable and\\nmust be understood and responded to by\\nhumans.\\n6. Do not use phrases that ’provided con-\\ntext’, etc in the question\\ncontext:\\nWe also used ChatGPT to answer the generated\\nquestion, when given the corresponding introduc-\\ntory section as context, using the following prompt:\\nAnswer the question using the informa-\\ntion from the given context.\\nquestion: [question]\\ncontext: [context]\\nAll questions were annotated along the three con-\\nsidered quality dimensions by two annotators. Both\\nannotators were fluent in English and were given\\nclear instructions about the meaning of the three\\nconsidered quality dimensions. For faithfulness\\nand context relevance, the two annotators agreed in\\naround 95% of cases. For answer relevance, they\\nagreed in around 90% of the cases. Disagreements\\nwere resolved after a discussion between the anno-\\ntators.\\nFaithfulness To obtain human judgements about'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4'}, page_content='faithfulness, we first used ChatGPT to answer the\\nquestion without access to any additional context.\\nWe then asked the annotators to judge which of the\\ntwo answers was the most faithful (i.e. the standard\\none or the one generated without context), given\\nthe question and corresponding Wikipedia page.\\nAnswer relevance We first used ChatGPT to\\nobtain candidate answers with lower answer rel-\\nevance, using the following prompt:\\nAnswer the given question in an incom-\\nplete manner.\\nquestion: [question]\\nWe then asked human annotators to compare this\\nanswer, and indicate which of the two answers had\\nthe highest answer relevance.\\nContext relevance To measure this aspect, we\\nfirst added additional sentences to the context by\\nscraping back-links to the corresponding Wikipedia\\npage. In this way, we were able to add information\\nto the context that was related but less relevant for'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5'}, page_content='Faith. Ans. Rel. Cont. Rel.\\nRagas 0.95 0.78 0.70\\nGPT Score 0.72 0.52 0.63\\nGPT Ranking 0.54 0.40 0.52\\nTable 1: Agreement with human annotators in pairwise\\ncomparisons of faithfulness, answer relevance and con-\\ntext relevance, using the WikEval dataset (accuracy).\\nanswering the question. For the few pages with-\\nout any back-links, we instead used ChatGPT to\\ncomplete the given context.\\n5 Experiments\\nTable 1 analyses the agreement between the met-\\nrics proposed in Section 3 and the human assess-\\nments from the proposed WikiEval dataset. Each\\nWikiEval instance requires the model to compare\\ntwo answers or two context fragments. We count\\nhow often the answer/context preferred by the\\nmodel (i.e. with highest estimated faithfulness, an-\\nswer relevance, or context relevance) coincides\\nwith the answer/context preferred by the human\\nannotators. We report the results in terms of ac-\\ncuracy (i.e. the fraction of instances on which the\\nmodel agrees with the annotators).'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5'}, page_content='To put the results in context, we compare our\\nproposed metrics (shown as Ragas in Table 1) with\\ntwo baseline methods. For the first method, shown\\nas GPT Score, we ask ChatGPT to assign a score\\nbetween 0 and 10 for the three quality dimensions.\\nTo this end, we use a prompt that describes the\\nmeaning of the quality metric and then asks to\\nscore the given answer/context in line with that\\ndefinition. For instance, for evaluating faithfulness,\\nwe used the following prompt:\\nFaithfulness measures the information\\nconsistency of the answer against the\\ngiven context. Any claims that are made\\nin the answer that cannot be deduced\\nfrom context should be penalized.\\nGiven an answer and context, assign a\\nscore for faithfulness in the range 0-10.\\ncontext: [context]\\nanswer: [answer]\\nTies, where the same score is assigned by the LLM\\nto both answer candidates, were broken randomly.\\nThe second baseline, shown as GPT Ranking, in-\\nstead asks ChatGPT to select the preferred answer/-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5'}, page_content='context. In this case, the prompt again includes\\na definition of the considered quality metric. For\\ninstance, for evaluating answer relevance, we used\\nthe following prompt:\\nAnswer Relevancy measures the degree\\nto which a response directly addresses\\nand is appropriate for a given question.\\nIt penalizes the present of redundant in-\\nformation or incomplete answers given a\\nquestion. Given an question and answer,\\nrank each answer based on Answer Rele-\\nvancy.\\nquestion: [question]\\nanswer 1: [answer 1]\\nanswer 2: [answer 2]\\nThe results in Table 1 show that our proposed\\nmetrics are much closer aligned with the human\\njudgements than the predictions from the two base-\\nlines. For faithfulness, the Ragas prediction are in\\ngeneral highly accurate. For answer relevance, the\\nagreement is lower, but this is largely due to the\\nfact that the differences between the two candidate\\nanswers are often very subtle. We found context\\nrelevance to be the hardest quality dimension to'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5'}, page_content='evaluate. In particular, we observed that ChatGPT\\noften struggles with the task of selecting the sen-\\ntences from the context that are crucial, especially\\nfor longer contexts.\\n6 Conclusions\\nWe have highlighted the need for automated\\nreference-free evaluation of RAG systems. In par-\\nticular, we have argued the need for an evaluation\\nframework that can assess faithfulness (i.e. is the\\nanswer grounded in the retrieved context), answer\\nrelevance (i.e. does the answer address the ques-\\ntion) and context relevance (i.e. is the retrieved\\ncontext sufficiently focused). To support the devel-\\nopment of such a framework, we have introduced\\nWikiEval, a dataset which human judgements of\\nthese three different aspects. Finally, we have also\\ndescribed Ragas, our implementation of the three\\nconsidered quality aspects. This framework is easy\\nto use and can provide deverlopers of RAG sys-\\ntems with valuable insights, even in the absence\\nof any ground truth. Our evaluation on WikiEval'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5'}, page_content='has shown that the predictions from Ragas are\\nclosely aligned with human predictions, especially\\nfor faithfulness and answer relevance.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6'}, page_content='References\\nAmos Azaria and Tom M. Mitchell. 2023. The inter-\\nnal state of an LLM knows when its lying. CoRR,\\nabs/2304.13734.\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\\nTrevor Cai, Eliza Rutherford, Katie Millican, George\\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia\\nGuy, Jacob Menick, Roman Ring, Tom Hennigan,\\nSaffron Huang, Loren Maggiore, Chris Jones, Albin\\nCassirer, Andy Brock, Michela Paganini, Geoffrey\\nIrving, Oriol Vinyals, Simon Osindero, Karen Si-\\nmonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.\\n2022. Improving language models by retrieving from\\ntrillions of tokens. In International Conference on\\nMachine Learning, ICML 2022, 17-23 July 2022, Bal-\\ntimore, Maryland, USA, volume 162 of Proceedings\\nof Machine Learning Research , pages 2206–2240.\\nPMLR.\\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6'}, page_content='berg, et al. 2023. Sparks of artificial general intelli-\\ngence: Early experiments with gpt-4. arXiv preprint\\narXiv:2303.12712.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In Proceedings of the 2019 Conference of\\nthe North American Chapter of the Association for\\nComputational Linguistics: Human Language Tech-\\nnologies, Volume 1 (Long and Short Papers), pages\\n4171–4186, Minneapolis, Minnesota. Association for\\nComputational Linguistics.\\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\\nLiu. 2023. Gptscore: Evaluate as you desire. CoRR,\\nabs/2302.04166.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\\npat, and Mingwei Chang. 2020. Retrieval augmented\\nlanguage model pre-training. In International confer-\\nence on machine learning, pages 3929–3938. PMLR.\\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6'}, page_content='Madotto, and Pascale Fung. 2023. Survey of halluci-\\nnation in natural language generation. ACM Comput-\\ning Surveys, 55(12):1–38.\\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\\nHenighan, Dawn Drain, Ethan Perez, Nicholas\\nSchiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli\\nTran-Johnson, Scott Johnston, Sheer El Showk, Andy\\nJones, Nelson Elhage, Tristan Hume, Anna Chen,\\nYuntao Bai, Sam Bowman, Stanislav Fort, Deep\\nGanguli, Danny Hernandez, Josh Jacobson, Jack-\\nson Kernion, Shauna Kravec, Liane Lovitt, Ka-\\nmal Ndousse, Catherine Olsson, Sam Ringer, Dario\\nAmodei, Tom Brown, Jack Clark, Nicholas Joseph,\\nBen Mann, Sam McCandlish, Chris Olah, and Jared\\nKaplan. 2022. Language models (mostly) know what\\nthey know. CoRR, abs/2207.05221.\\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric\\nWallace, and Colin Raffel. 2022. Large language\\nmodels struggle to learn long-tail knowledge. CoRR,\\nabs/2211.08411.\\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\\nZettlemoyer, and Mike Lewis. 2020. Generalization'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6'}, page_content='through memorization: Nearest neighbor language\\nmodels. In 8th International Conference on Learning\\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia,\\nApril 26-30, 2020. OpenReview.net.\\nOmar Khattab, Keshav Santhanam, Xiang Lisa Li,\\nDavid Hall, Percy Liang, Christopher Potts, and\\nMatei Zaharia. 2022. Demonstrate-search-predict:\\nComposing retrieval and language models for\\nknowledge-intensive NLP. CoRR, abs/2212.14024.\\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\\n2019. Latent retrieval for weakly supervised open do-\\nmain question answering. In Proceedings of the 57th\\nAnnual Meeting of the Association for Computational\\nLinguistics, pages 6086–6096.\\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,\\nTim Rocktäschel, Sebastian Riedel, and Douwe\\nKiela. 2020. Retrieval-augmented generation for\\nknowledge-intensive NLP tasks. In Advances in Neu-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6'}, page_content='ral Information Processing Systems 33: Annual Con-\\nference on Neural Information Processing Systems\\n2020, NeurIPS 2020, December 6-12, 2020, virtual.\\nJunyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun\\nNie, and Ji-Rong Wen. 2023. Halueval: A large-\\nscale hallucination evaluation benchmark for large\\nlanguage models. CoRR, abs/2305.11747.\\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-\\njape, Michele Bevilacqua, Fabio Petroni, and Percy\\nLiang. 2023. Lost in the middle: How language\\nmodels use long contexts.\\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\\nDaniel Khashabi, and Hannaneh Hajishirzi. 2023.\\nWhen not to trust language models: Investigating\\neffectiveness of parametric and non-parametric mem-\\nories. In Proceedings of the 61st Annual Meeting of\\nthe Association for Computational Linguistics (Vol-\\nume 1: Long Papers) , pages 9802–9822, Toronto,\\nCanada. Association for Computational Linguistics.\\nPotsawee Manakul, Adian Liusie, and Mark J. F. Gales.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6'}, page_content='2023. Selfcheckgpt: Zero-resource black-box hal-\\nlucination detection for generative large language\\nmodels. CoRR, abs/2303.08896.\\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike\\nLewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,\\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2023.\\nFactscore: Fine-grained atomic evaluation of fac-\\ntual precision in long form text generation. CoRR,\\nabs/2305.14251.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7'}, page_content='Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\\nShoham. 2023. In-context retrieval-augmented lan-\\nguage models. CoRR, abs/2302.00083.\\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\\nHow much knowledge can you pack into the param-\\neters of a language model? In Proceedings of the\\n2020 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP), pages 5418–5426,\\nOnline. Association for Computational Linguistics.\\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon\\nSeo, Rich James, Mike Lewis, Luke Zettlemoyer, and\\nWen-tau Yih. 2023. REPLUG: retrieval-augmented\\nblack-box language models. CoRR, abs/2301.12652.\\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxi-\\nang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie\\nZhou. 2023a. Is chatgpt a good NLG evaluator? A\\npreliminary study. CoRR, abs/2303.04048.\\nPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai\\nLin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7'}, page_content='2023b. Large language models are not fair evaluators.\\nCoRR, abs/2305.17926.\\nShufan Wang, Yixiao Song, Andrew Drozdov, Aparna\\nGarimella, Varun Manjunatha, and Mohit Iyyer.\\n2023c. KNN-LM does not improve open-ended text\\ngeneration. CoRR, abs/2305.14625.\\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\\nBartscore: Evaluating generated text as text genera-\\ntion. In Advances in Neural Information Processing\\nSystems 34: Annual Conference on Neural Informa-\\ntion Processing Systems 2021, NeurIPS 2021, De-\\ncember 6-14, 2021, virtual, pages 27263–27277.\\nTianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei\\nFang, Luc Gaitskell, Thomas Hartvigsen, Xixin Wu,\\nDanny Fox, Helen Meng, and James R. Glass. 2023.\\nInterpretable unified language checking. CoRR,\\nabs/2304.03728.\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\\nWeinberger, and Yoav Artzi. 2020. Bertscore: Evalu-\\nating text generation with BERT. In8th International\\nConference on Learning Representations, ICLR 2020,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7'}, page_content='Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe-\\nview.net.\\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-\\ntian M. Meyer, and Steffen Eger. 2019. MoverScore:\\nText generation evaluating with contextualized em-\\nbeddings and earth mover distance. In Proceedings\\nof the 2019 Conference on Empirical Methods in\\nNatural Language Processing and the 9th Interna-\\ntional Joint Conference on Natural Language Pro-\\ncessing (EMNLP-IJCNLP), pages 563–578, Hong\\nKong, China. Association for Computational Lin-\\nguistics.\\nA Examples from WikiEval\\nTables 2, 3 and 4 show examples from the WikiEval\\ndataset, focusing in particular on answers with high\\nand low faithfulness (Table 2), high and low answer\\nrelevance (Table 3), and high and low context rele-\\nvance (Table 4).'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8'}, page_content='Question Context Answer\\nWho directed the film Op-\\npenheimer and who stars\\nas J. Robert Oppenheimer\\nin the film?\\nOppenheimer is a 2023 biographical thriller film written\\nand directed by Christopher Nolan. Based on the 2005\\nbiography American Prometheus by Kai Bird and Mar-\\ntin J. Sherwin, the film chronicles the life of J. Robert\\nOppenheimer, a theoretical physicist who was pivotal in\\ndeveloping the first nuclear weapons as part of the Man-\\nhattan Project, and thereby ushering in the Atomic Age.\\nCillian Murphy stars as Oppenheimer, with Emily Blunt\\nas Oppenheimer’s wife Katherine \"Kitty\" Oppenheimer.\\nHigh Faithfulness : Christopher\\nNolan directed the film Oppen-\\nheimer. Cillian Murphy stars as J.\\nRobert Oppenheimer in the film.\\nLow Faithfulness : James\\nCameron directed the film Op-\\npenheimer. Tom Cruise stars as J.\\nRobert Oppenheimer in the film.\\nTable 2: Example from WikiEval, showing answers with high and low faithfulness.\\nQuestion Answer\\nWhen is the scheduled\\nlaunch date and time for'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8'}, page_content='the PSLV-C56 mission,\\nand where will it be\\nlaunched from?\\nHigh answer relevance: The PSLV-C56 mission is scheduled to be launched on Sunday, 30\\nJuly 2023 at 06:30 IST / 01:00 UTC. It will be launched from the Satish Dhawan Space\\nCentre, Sriharikota, Andhra Pradesh, India.\\nLow answer relevance: The scheduled launch date and time for the PSLV-C56 mission have\\nnot been provided.The PSLV-C56 mission is an important space mission for India. It aims to\\nlaunch a satellite into orbit to study weather patterns.\\nTable 3: Example from WikiEval, showing answers with high and low answer relevance.\\nQuestion Context\\nWhen was the Chimnabai\\nClock Tower completed,\\nand who was it named af-\\nter?\\nHigh context relevance: The Chimnabai Clock Tower, also known as the Raopura Tower, is\\na clock tower situated in the Raopura area of Vadodara, Gujarat, India. It was completed\\nin 1896 and named in memory of Chimnabai I (1864–1885), a queen and the first wife of\\nSayajirao Gaekwad III of Baroda State.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-29T01:18:28+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-29T01:18:28+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-4.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8'}, page_content='Low context relevance: The Chimnabai Clock Tower, also known as the Raopura Tower, is\\na clock tower situated in the Raopura area of Vadodara, Gujarat, India. It was completed\\nin 1896 and named in memory of Chimnabai I (1864–1885), a queen and the first wife of\\nSayajirao Gaekwad III of Baroda State. It was built in Indo-Saracenic architecture style.\\nHistory. Chimnabai Clock Tower was built in 1896. The tower was named after Chimnabai\\nI (1864–1885), a queen and the first wife of Sayajirao Gaekwad III of Baroda State. It was\\ninaugurated by Mir Kamaluddin Hussainkhan, the last Nawab of Baroda. During the rule of\\nGaekwad, it was a stoppage for horse drawn trams. The clock tower was erected at the cost\\nof 25,000 (equivalent to 9.2 million or USD 120,000 in 2023).\\nTable 4: Example from WikiEval, showing answers with high and low context relevance.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 0, 'page_label': '1'}, page_content='Active Retrieval Augmented Generation\\nZhengbao Jiang1∗ Frank F. Xu1∗ Luyu Gao1∗ Zhiqing Sun1∗ Qian Liu2\\nJane Dwivedi-Yu3 Yiming Yang1 Jamie Callan1 Graham Neubig1\\n1Language Technologies Institute, Carnegie Mellon University\\n2Sea AI Lab 3FAIR, Meta\\n{zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu\\nAbstract\\nDespite the remarkable ability of large lan-\\nguage models (LMs) to comprehend and gen-\\nerate language, they have a tendency to hal-\\nlucinate and create factually inaccurate out-\\nput. Augmenting LMs by retrieving informa-\\ntion from external knowledge resources is one\\npromising solution. Most existing retrieval aug-\\nmented LMs employ a retrieve-and-generate\\nsetup that only retrieves information once based\\non the input. This is limiting, however, in\\nmore general scenarios involving generation\\nof long texts, where continually gathering in-\\nformation throughout generation is essential. In\\nthis work, we provide a generalized view of ac-\\ntive retrieval augmented generation, methods'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 0, 'page_label': '1'}, page_content='that actively decide when and what to retrieve\\nacross the course of the generation. We propose\\nForward-Looking Active REtrieval augmented\\ngeneration (FLARE), a generic method which\\niteratively uses a prediction of the upcoming\\nsentence to anticipate future content, which is\\nthen utilized as a query to retrieve relevant doc-\\numents to regenerate the sentence if it contains\\nlow-confidence tokens. We test FLARE along\\nwith baselines comprehensively over 4 long-\\nform knowledge-intensive generation tasks/-\\ndatasets. FLARE achieves superior or compet-\\nitive performance on all tasks, demonstrating\\nthe effectiveness of our method.1\\n1 Introduction\\nGenerative language models (LMs) (Brown et al.,\\n2020; Ouyang et al., 2022; OpenAI, 2023; Chowd-\\nhery et al., 2022; Zhang et al., 2022; Touvron et al.,\\n2023; Zhao et al., 2023) have become a founda-\\ntional component in natural language processing\\n(NLP) systems with their remarkable abilities. Al-\\nthough LMs have memorized some world knowl-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 0, 'page_label': '1'}, page_content='edge during training (Petroni et al., 2019; Roberts\\net al., 2020; Jiang et al., 2020), they still tend to\\n∗Lead contributors.\\n1Code and datasets are available athttps://github.com/\\njzbjyb/FLARE.\\nhallucinate and create imaginary content (Maynez\\net al., 2020; Zhou et al., 2021). Augmenting LMs\\nwith retrieval components that look up relevant in-\\nformation from external knowledge resources is a\\npromising direction to address hallucination (Khan-\\ndelwal et al., 2020; Izacard et al., 2022).\\nRetrieval augmented LMs commonly use a\\nretrieve-and-generate setup where they retrieve doc-\\numents based on the user’s input, and then generate\\na complete answer conditioning on the retrieved\\ndocuments (Chen et al., 2017; Guu et al., 2020;\\nLewis et al., 2020; Izacard and Grave, 2021; Sachan\\net al., 2021; Lee et al., 2021; Jiang et al., 2022;\\nIzacard et al., 2022; Nakano et al., 2021; Qian\\net al., 2023; Lazaridou et al., 2022; Shi et al., 2023).\\nThese single-time retrieval augmented LMs outper-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 0, 'page_label': '1'}, page_content='form purely parametric LMs, particularly for short-\\nform knowledge-intensive generation tasks such\\nas factoid question answering (QA) (Kwiatkowski\\net al., 2019; Joshi et al., 2017), where the informa-\\ntion needs are clear in the user’s input, and it is\\nsufficient to retrieve relevant knowledge once solely\\nbased on the input.\\nIncreasingly powerful large LMs have also\\ndemonstrated abilities in more complex tasks that\\ninvolve generating long-form output, such as long-\\nform QA (Fan et al., 2019; Stelmakh et al., 2022),\\nopen-domain summarization (Cohen et al., 2021;\\nHayashi et al., 2021; Giorgi et al., 2022), and\\n(chain-of-thought; CoT) reasoning (Wei et al.,\\n2022; Ho et al., 2020; Geva et al., 2021; Hendrycks\\net al., 2020). In contrast to short-form generation,\\nlong-form generation presents complex informa-\\ntion needs that are not always evident from the in-\\nput alone. Similar to how humans gradually gather\\ninformation as we create content such as papers,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 0, 'page_label': '1'}, page_content='essays, or books, long-form generation with LMs\\nwould require gathering multiple pieces of knowl-\\nedge throughout the generation process . For ex-\\nample, to generate a summary about a particular\\ntopic, the initial retrieval based on the topic name\\narXiv:2305.06983v2  [cs.CL]  22 Oct 2023'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 1, 'page_label': '2'}, page_content='Generate a summary about Joe Biden.\\nSearch results:   !![1]: …[2]: …\\nJoe Biden (born November 20, 1942) is the 46th president of the United States.Joe Biden (born November 20, 1942) is the 46th president of the United States.\\nHe graduated from the University of Delaware in 1965 with a Bachelor of Arts in history and political science.\\nJoe Biden attended the University of Pennsylvania, where he earned a law degree.\\nRetriever\\nInputStep 1\\nSearch results:   !\"![1]: …[2]: …\\n\"####\\n\"#$\\n#$\\n$\\nStep 2\\nJoe Biden announced his candidacy for the 2020 presidential election on April 25, 2019.\\nJoe Biden announced his candidacy for the 2020 presidential election on August 18, 2019.\"#%\\n#%\\nStep 3\\nSearch results:   !\"\"[1]: …[2]: …Retrieveddocuments\\nLM\\nGeneration\\n$\\n%$\\n%%\\nFigure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user\\ninput x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 1, 'page_label': '2'}, page_content='italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the\\nsystem retrieves relevant documents and regenerates the sentence.\\n(e.g., Joe Biden) may not cover all aspects and de-\\ntails. It is crucial to retrieve extra information as\\nneeded during generation, such as when generat-\\ning a certain aspect (e.g., Joe Biden’s education\\nhistory) or a specific detail (e.g., the date of Joe\\nBiden’s presidential campaign announcement).\\nSeveral attempts have been made to retrieve mul-\\ntiple times throughout generation. These attempts\\ninclude methods that passively use the past context\\nto retrieve additional information at a fixed interval\\n(Khandelwal et al., 2020; Borgeaud et al., 2022;\\nRam et al., 2023; Trivedi et al., 2022) which might\\nnot accurately reflect what LMs intend to gener-\\nate in the future or retrieve at inappropriate points.\\nSome works in multihop QA decompose the full\\nquestion into sub-questions, each of which is used'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 1, 'page_label': '2'}, page_content='to retrieve extra information (Press et al., 2022; Yao\\net al., 2022; Khot et al., 2022; Khattab et al., 2022).\\nWe ask the following question: can we create a\\nsimple and generic retrieval augmented LM thatac-\\ntively decides when and what to retrievethroughout\\nthe generation process, and are applicable to a va-\\nriety of long-form generation tasks? We provide a\\ngeneralized view of active retrieval augmented gen-\\neration. Our hypothesis regarding when to retrieve\\nis that LMs should retrieve information only when\\nthey lack the required knowledge to avoid unneces-\\nsary or inappropriate retrieval that occurs in passive\\nretrieval augmented LMs (Khandelwal et al., 2020;\\nBorgeaud et al., 2022; Ram et al., 2023; Trivedi\\net al., 2022). Given the observation that large LMs\\ntend to be well-calibrated and low probability/con-\\nfidence often indicates a lack of knowledge (Ka-\\ndavath et al., 2022), we adopt an active retrieval\\nstrategy that only retrieves when LMs generate low-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 1, 'page_label': '2'}, page_content='probability tokens. When deciding what to retrieve,\\nit is important to consider what LMs intend to gen-\\nerate in the future, as the goal of active retrieval is to\\nbenefit future generations. Therefore, we propose\\nanticipating the future by generating a temporary\\nnext sentence, using it as a query to retrieve rel-\\nevant documents, and then regenerating the next\\nsentence conditioning on the retrieved documents.\\nCombining the two aspects, we propose Forward-\\nLooking Active REtrieval augmented generation\\n(FLARE), as illustrated in Figure 1. FLARE iter-\\natively generates a temporary next sentence, use\\nit as the query to retrieve relevant documents if it\\ncontains low-probability tokens and regenerate the\\nnext sentence until reaches the end.\\nFLARE is applicable to any existing LMs at\\ninference time without additional training. Con-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 2, 'page_label': '3'}, page_content='sidering the impressive performance achieved by\\nGPT-3.5 (Ouyang et al., 2022) on a variety of\\ntasks, we examine the effectiveness of our meth-\\nods on text-davinci-003. We evaluate FLARE\\non 4 diverse tasks/datasets involving generating\\nlong outputs, including multihop QA (2WikiMul-\\ntihopQA), commonsense reasoning (StrategyQA),\\nlong-form QA (ASQA), and open-domain summa-\\nrization (WikiAsp) (Ho et al., 2020; Geva et al.,\\n2021; Stelmakh et al., 2022; Hayashi et al., 2021).\\nOver all tasks, FLARE achieves superior or com-\\npetitive performance compared to single-time and\\nmulti-time retrieval baselines, demonstrating the\\neffectiveness and generalizability of our method.\\n2 Retrieval Augmented Generation\\nWe formally define single-time retrieval augmented\\ngeneration and propose the framework of active\\nretrieval augmented generation.\\n2.1 Notations and Definitions\\nGiven a user input x and a document corpus D =\\n{di}|D|\\ni=1 (such as all Wikipedia articles), the goal of'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 2, 'page_label': '3'}, page_content='retrieval augmented LMs is to generate the answer\\ny = [s1, s2, ...,sm] = [w1, w2, ..., wn] containing\\nm sentences or n tokens leveraging information\\nretrieved from the corpus.\\nIn retrieval augmented LM, the LM typically\\npairs with a retriever that can retrieve a list of\\ndocuments Dq = ret(q) for a query q; the LM\\nconditions on both the user input x and retrieved\\ndocuments Dq to generate the answer. Since we\\nfocus on examining various methods of determin-\\ning when and what to retrieve, we follow exist-\\ning methods (Ram et al., 2023; Trivedi et al.,\\n2022) to prepend the retrieved documents before\\nthe user input to aid future generation for both\\nbaselines and our method for fair comparisons:\\ny = LM([Dq, x]), where [·, ·] is concatenation fol-\\nlowing the specified order.\\n2.2 Single-time Retrieval Augmented\\nGeneration\\nThe most common choice is to directly use the user\\ninput as the query for retrieval and generate the\\ncomplete answer at once y = LM([Dx, x]).'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 2, 'page_label': '3'}, page_content='2.3 Active Retrieval Augmented Generation\\nTo aid long-form generation with retrieval, we pro-\\npose active retrieval augmented generation. It is a\\ngeneric framework that actively decides when and\\nwhat to retrieve through the generation process,\\nresulting in the interleaving of retrieval and genera-\\ntion. Formally, at step t(t ≥ 1), the retrieval query\\nqt is formulated based on both the user input x and\\npreviously generated output y<t = [y0, ...,yt−1]:\\nqt = qry(x, y<t),\\nwhere qry(·) is the query formulation function. At\\nthe beginning (t = 1), the previous generation is\\nempty (y<1 = ∅), and the user input is used as the\\ninitial query (q1 = x). Given retrieved documents\\nDqt, LMs continually generate the answer until the\\nnext retrieval is triggered or reaches the end:\\nyt = LM([Dqt, x, y<t]),\\nwhere yt represents the generated tokens at the cur-\\nrent step t, and the input to LMs is the concatena-\\ntion of the retrieved documents Dqt, the user input'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 2, 'page_label': '3'}, page_content='x, and the previous generation y<t. We discard\\npreviously retrieved documents ∪t′<tDqt′ and only\\nuse the retrieved documents from the current step\\nto condition the next generation to prevent reaching\\nthe input length limit of LMs.\\n3 FLARE: Forward-Looking Active\\nREtrieval Augmented Generation\\nOur intuition is that (1) LMs should only retrieve\\ninformation when they do not have the necessary\\nknowledge to avoid unnecessary or inappropriate\\nretrieval, and (2) the retrieval queries should reflect\\nthe intents of future generations. We propose two\\nforward-looking active retrieval augmented gener-\\nation (FLARE) methods to implement the active\\nretrieval augmented generation framework. The\\nfirst method prompts the LM to generate retrieval\\nqueries when necessary while generating the an-\\nswer using retrieval-encouraging instructions, de-\\nnoted as FLAREinstruct. The second method directly\\nuses the LM’s generation as search queries, denoted\\nas FLAREdirect, which iteratively generates the next'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 2, 'page_label': '3'}, page_content='sentence to gain insight into the future topic, and\\nif uncertain tokens are present, retrieves relevant\\ndocuments to regenerate the next sentence.\\n3.1 FLARE with Retrieval Instructions\\nInspired by Toolformer (Schick et al., 2023), a\\nstraightforward way of expressing information\\nneeds for retrieval is to generate “[Search(query)]”\\nwhen additional information is needed (Schick\\net al., 2023), e.g., “The colors on the flag of\\nGhana have the following meanings. Red is for\\n[Search(Ghana flag red meaning)] the blood of mar-\\ntyrs, ...” When working with GPT-3.5 models that'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 3, 'page_label': '4'}, page_content='Search results:   !![1]: …[2]: …\\nJoe Biden attended\\nSearch results:   !\"![1]: …[2]: …Search results:   !\"\"[1]: …[2]: …\\n[Search(Joe Biden University)]\\n[Search(Joe Biden degree)]the University of Pennsylvania, where he earned\\na law degree.\\nGenerate a summary about Joe Biden.Input$\\n&$\\n&#%$\\n&%%%Generation\\nRetriever\\n$\\n%$\\n%%\\nFigure 2: An illustration of forward-looking active re-\\ntrieval augmented generation with retrieval instructions\\n(FLAREinstruct). It iteratively generates search queries\\n(shown in gray italic) to retrieve relevant information to\\naid future generations.\\noffer only API access, we elicit such behavior by\\nfew-shot prompting (Brown et al., 2020).\\nSpecifically, for a downstream task, we place\\nthe search-related instruction and exemplars at the\\nbeginning as skill 1, followed by the instruction and\\nexemplars of the downstream task as skill 2. Given\\na test case, we ask LMs to combine skills 1 and 2 to\\ngenerate search queries while performing the task.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 3, 'page_label': '4'}, page_content='The structure of the prompt is shown in Prompt 3.1,\\nand full details can be found in Prompt D.3.\\nPrompt 3.1: retrieval instructions\\nSkill 1. An instruction to guide LMs to generate search\\nqueries.\\nSeveral search-related exemplars.\\nSkill 2. An instruction to guide LMs to perform a\\nspecific downstream task (e.g., multihop QA).\\nSeveral task-related exemplars.\\nAn instruction to guide LMs to combine skills 1\\nand 2 for the test case.\\nThe input of the test case.\\nAs shown in Figure 2, when the LM generates\\n“[Search(query)]” (shown in gray italic), we stop\\nthe generation and use the query terms to retrieve\\nrelevant documents, which are prepended before\\nthe user input to aid future generation until the\\nnext search query is generated or reaches the end.\\nAdditional implementation details are included in\\nAppendix A.\\n3.2 Direct FLARE\\nSince we cannot fine-tune black-box LMs, we\\nfound queries generated by FLAREinstruct through\\nretrieval instructions might not be reliable. There-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 3, 'page_label': '4'}, page_content='fore, we propose a more direct way of forward-\\nlooking active retrieval that uses the next sentence\\nto decide when and what to retrieve.\\n3.2.1 Confidence-based Active Retrieval\\nAs shown in Figure 1, at step t, we first generate a\\ntemporary next sentence ˆst = LM([x, y<t]) with-\\nout conditioning on retrieved documents. Then we\\ndecide whether to trigger retrieval and formulate\\nqueries based on ˆst. If the LM is confident aboutˆst,\\nwe accept it without retrieving additional informa-\\ntion; if not, we use ˆst to formulate search queries\\nqt to retrieve relevant documents, and then regen-\\nerate the next sentence st. The reason we utilize\\nsentences as the basis of our iteration is due to their\\nsignificance as semantic units that are neither too\\nshort nor too lengthy like phrases and paragraphs.\\nHowever, our approach can also utilize phrases or\\nparagraphs as the basis.\\nSince LMs tend to be well-calibrated that low\\nprobability/confidence often indicates a lack of'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 3, 'page_label': '4'}, page_content='knowledge (Jiang et al., 2021; Kadavath et al.,\\n2022; Varshney et al., 2022), we actively trigger\\nretrieval if any token of ˆst has a probability lower\\nthan a threshold θ ∈ [0, 1]. θ = 0means retrieval\\nis never triggered, while θ = 1triggers retrieval\\nevery sentence.\\nyt =\\n(\\nˆst if all tokens of ˆst have probs ≥ θ\\nst = LM([Dqt, x, y<t]) otherwise\\nwhere the query qt is formulated based on ˆst.\\n3.2.2 Confidence-based Query Formulation\\nOne way to perform retrieval is to directly use the\\nnext sentence ˆst as the query qt. This shares a sim-\\nilar spirit with methods that use generated hypo-\\nthetical titles or paragraphs from LMs as retrieval\\nqueries or evidences (Gao et al., 2022; Sun et al.,\\n2022; Yu et al., 2022; Mao et al., 2021). We gen-\\neralize such techniques to long-form generation\\nwhere active information access is essential.\\nWe found retrieving with the next sentence\\nachieves significantly better results than with the\\nprevious context, as shown later in subsection 6.2.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 3, 'page_label': '4'}, page_content='However, it has a risk of perpetuating errors con-\\ntained in it. For example, if the LM produces the\\nsentence “Joe Biden attended the University of\\nPennsylvania” instead of the correct fact that he\\nattended the University of Delaware, using this er-\\nroneous sentence as a query might retrieve mislead-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 4, 'page_label': '5'}, page_content='Joe Biden attended the University of Pennsylvania, where he earned a law degree.\\nAsk a question to which the answer is “the University of Pennsylvania”Ask a question to which the answer is “a law degree”\\nWhat university did Joe Biden attend?What degree did Joe Biden earn?\\nimplicit query by maskingexplicit query by question generationJoe Biden attended  , where he earned  .\\nLM such as ChatGPT\\nFigure 3: Implicit and explicit query formulation. To-\\nkens with low probabilities are marked with underlines.\\ning information. We propose two simple methods\\nto overcome this issue as illustrated in Figure 3.\\nMasked sentences as implicit queries. The first\\nmethod masks out low-confidence tokens inˆst with\\nprobabilities below a threshold β ∈ [0, 1], where a\\nhigher β results in more aggressive masking. This\\nremoves potential distractions from the sentence to\\nimprove retrieval accuracy.\\nGenerated questions as explicit queries. An-\\nother method is to generate explicit questions that'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 4, 'page_label': '5'}, page_content='target the low-confident span in ˆst. For example, if\\nthe LM is uncertain about “the University of Penn-\\nsylvania”, a question like “Which university did\\nJoe Biden attend?” can help retrieve relevant in-\\nformation. Self-ask (Press et al., 2022) achieved\\nthis by manually inserting follow-up questions\\ninto downstream task exemplars as shown later\\nin Prompt D.2, which requires task-specific annota-\\ntion efforts. Instead, we developed a universal ap-\\nproach that generates questions for low-confidence\\nspans without additional annotation. Specifically,\\nWe first extract all spans from ˆst with probabilities\\nbelow β. For each extracted span z, we prompt\\ngpt-3.5-turbo to generate a question qt,z that\\ncan be answered with the span:\\nPrompt 3.2: zero-shot question generation\\nUser input x.\\nGenerated output so far y≤t.\\nGiven the above passage, ask a question to which\\nthe answer is the term/entity/phrase “z”.\\nWe retrieve using each generated question and'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 4, 'page_label': '5'}, page_content='interleave the returned documents into a single\\nranking list to aid future generations. In summary,\\nqueries qt are formulated based on ˆst as follows:\\nqt =\\n(\\n∅ if all tokens of ˆst have probs ≥ θ\\nmask(ˆst) or qgen(ˆst) otherwise\\n3.3 Implementation Details\\nBase LM We validate our method on one of the\\nmost advanced GPT-3.5 LMs text-davinci-003\\nby iteratively querying their API.2\\nDocument corpus and retrievers. Since we fo-\\ncus on the integration of retrieval and generation,\\nwe use off-the-shelf retrievers that take queries\\nas inputs and return a list of relevant documents.\\nFor datasets that mainly rely on knowledge from\\nWikipedia, we use the Wikipedia dump from\\nKarpukhin et al. (2020) and employ BM25 (Robert-\\nson and Zaragoza, 2009) as the retriever. For\\ndatasets that rely on knowledge from the open web,\\nwe use the Bing search engine as our retriever.3\\nRetrieved document formatting. Multiple re-\\ntrieved documents are linearized according to their'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 4, 'page_label': '5'}, page_content='ranking and then added to the beginning of the user\\ninput using Prompt D.1.\\nOther implementation details such as sentence to-\\nkenization and efficiency are included Appendix A.\\n4 Multi-time Retrieval Baselines\\nExisting passive multi-time retrieval augmented\\nLMs can also be formulated using our framework\\n(subsection 2.3). In this section, we formally in-\\ntroduce three baseline categories based on when\\nand what to retrieve. These baselines are not exact\\nreproductions of the corresponding paper because\\nmany design choices differ which makes direct\\ncomparisons impossible. We implemented them\\nusing the same settings, with the only variation\\nbeing when and what to retrieve.\\nPrevious-window approaches trigger retrieval\\nevery l tokens, where l represents the window size.\\nGenerated tokens from the previous window are\\nused as the query:\\nqt = yt−1 (t ≥ 2),\\nyt = [w(t−1)l+1, ..., wtl].\\nSome existing methods in this category are RETRO\\n(Borgeaud et al., 2022), IC-RALM (Ram et al.,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 4, 'page_label': '5'}, page_content='2https://api.openai.com/v1/completions April 23.\\n3https://www.microsoft.com/en-us/bing/apis/\\nbing-web-search-api'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 5, 'page_label': '6'}, page_content='2023), which retrieve every few tokens, and KNN-\\nLM (Khandelwal et al., 2020), which retrieves ev-\\nery token.4 We follow Ram et al. (2023) to use a\\nwindow size of l = 16.\\nPrevious-sentence approaches trigger retrieval\\nevery sentence and use the previous sentence as the\\nquery, and IRCoT (Trivedi et al., 2022) belongs to\\nthis category:\\nqt = yt−1 (t ≥ 2),\\nyt = st.\\nQuestion decomposition approaches manually\\nannotated task-specific exemplars to guide LMs\\nto generate decomposed sub-questions while pro-\\nducing outputs. For example, self-ask (Press et al.,\\n2022), a method in this category, manually inserts\\nsub-questions in exemplars using Prompt D.2. For\\nthe test case, retrieval is triggered dynamically\\nwhenever the model generates a sub-question.\\nThe aforementioned approaches can retrieve ad-\\nditional information while generating. However,\\nthey have notable drawbacks: (1) Using previously\\ngenerated tokens as queries might not reflect what\\nLMs intend to generate in the future. (2) Retriev-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 5, 'page_label': '6'}, page_content='ing information at a fixed interval can be inefficient\\nbecause it might occur at inappropriate points. (3)\\nQuestion decomposition approaches require task-\\nspecific prompt engineering, which restricts their\\ngeneralizability in new tasks.\\n5 Experimental Setup\\nWe evaluate the effectiveness of FLARE on 4 di-\\nverse knowledge-intensive tasks using few-shot in-\\ncontext learning (Radford et al., 2019; Brown et al.,\\n2020; Liu et al., 2023). We follow previous works\\n(Trivedi et al., 2022) to sub-sample at most 500\\nexamples from each dataset due to the cost of run-\\nning experiments. Datasets, metrics, and settings\\nare summarized in Table 7 of Appendix B. The\\nhyperparameters of FLARE are selected based on\\nthe development set and listed in Table 9. FLARE\\nrefers to FLAREdirect if not specifically stated.\\nMultihop QA The goal of multihop QA is to\\nanswer complex questions through information re-\\ntrieval and reasoning. We use 2WikiMultihopQA\\n(Ho et al., 2020) which contains 2-hop complex'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 5, 'page_label': '6'}, page_content='4Since KNN-LM uses the contextualized representation\\ncorresponding to the current decoding position to retrieve rel-\\nevant information which encodes all previous tokens. Strictly\\nspeaking, qt should be y<t.\\nquestions sourced from Wikipedia articles that re-\\nquire composition, comparison, or inference, e.g.,\\n“Why did the founder of Versus die?” We follow\\nWang et al. (2022) to generate both the chain-of-\\nthought and the final answer. Experimental setting\\ndetails are included in Appendix B.\\nWe use regular expressions to extract the final\\nanswer from the output and compare it with the ref-\\nerence answer using exact match (EM), and token-\\nlevel F1, precision, and recall.\\nCommonsense reasoning Commonsense reason-\\ning requires world and commonsense knowledge\\nto generate answers. We use StrategyQA (Geva\\net al., 2021) which is a collection of crowdsourced\\nyes/no questions, e.g., “Would a pear sink in wa-\\nter?” We follow Wei et al. (2022) to generate both'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 5, 'page_label': '6'}, page_content='the chain-of-thought and the final yes/no answer.\\nDetails are included in Appendix B.\\nWe extract the final answer and match it against\\nthe gold answer using exact match.\\nLong-form QA Long-form QA aims to generate\\ncomprehensive answers to questions seeking com-\\nplex information (Fan et al., 2019; Stelmakh et al.,\\n2022). We use ASQA (Stelmakh et al., 2022) as our\\ntestbed where inputs are ambiguous questions with\\nmultiple interpretations, and outputs should cover\\nall of them. For example, “Where do the Philadel-\\nphia Eagles play their home games?” could be\\nasking about the city, sports complex, or stadium.\\nWe found in many cases it is challenging even for\\nhumans to identify which aspect of the question\\nis ambiguous. Therefore, we created another set-\\nting (ASQA-hint) where we provide a brief hint\\nto guide LMs to stay on track when generating an-\\nswers. The hint for the above case is “This question\\nis ambiguous in terms of which specific location or'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 5, 'page_label': '6'}, page_content='venue is being referred to.” Experimental setting\\ndetails are included in Appendix B.\\nWe use metrics from Stelmakh et al. (2022), in-\\ncluding EM, RoBERTa-based QA score (Disambig-\\nF1), ROUGE (Lin, 2004), and an overall score com-\\nbining Disambig-F1 and ROUGE (DR).\\nOpen-domain summarization The goal of open-\\ndomain summarization is to generate a comprehen-\\nsive summary about a topic by gathering informa-\\ntion from open web (Giorgi et al., 2022). We use\\nWikiAsp (Hayashi et al., 2021) which aims to gen-\\nerate aspect-based summaries about entities from\\n20 domains in Wikipedia, e.g., “Generate a sum-\\nmary about Echo School (Oregon) including the'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 6, 'page_label': '7'}, page_content='0.0\\n20.0\\n40.0\\n60.0\\n80.0\\n2WikiMultihopQA StrategyQA ASQA ASQA-hint WikiAsp\\nNo ret. Single-time ret. Previous-window ret. Forward-Looking Active REtrieval augmented generation (FLARE)\\nFigure 4: Comparision between FLARE and baselines across all tasks/datasets. We report the primary metric for\\neach dataset: EM for 2WikiMultihopQA, StrategyQA, and ASQA, and UniEval for WikiAsp.\\nfollowing aspects: academics, history.” Experimen-\\ntal setting details are included in Appendix B.\\nMetrics include ROUGE, named entity-based F1,\\nand UniEval (Zhong et al., 2022) which measures\\nfactual consistency.\\n6 Experimental Results\\nWe first report overall results across 4 tasks/datasets\\nand compare the performance of FLARE with all\\nthe baselines introduced in section 4. We then\\nrun ablation experiments to study the efficacy of\\nvarious design choices of our method.\\n6.1 Comparison with Baselines\\nOverall results. The overall performance of\\nFLARE and baseline across all tasks/datasets are'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 6, 'page_label': '7'}, page_content='reported in Figure 4. FLARE outperforms all base-\\nline on all tasks/datasets, indicating that FLARE\\nis a generic method that can effectively retrieve\\nadditional information throughout the generation.\\nAmong various tasks, multihop QA shows the\\nmost significant improvement. This is largely due\\nto the task’s clear definition and specific objective\\nof producing the final answer through a 2-hop rea-\\nsoning process, which makes it easier for LMs to\\ngenerate on-topic output. In contrast, ASQA and\\nWikiAsp are more open-ended, which increases the\\ndifficulty of both generation and evaluation. The\\nimprovement on ASQA-hint is larger than that of\\nASQA because identifying ambiguous aspects is\\nchallenging even for humans in many cases, and\\nproviding a generic hint helps LMs to stay on topic.\\nThorough comparisons with baselines. The per-\\nformance of all baselines on 2WikiMultihopQA\\nare reported in Table 1. FLARE outperforms all\\nbaselines by a large margin, which confirms that'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 6, 'page_label': '7'}, page_content='forward-looking active retrieval is highly effective.\\nMost multi-time retrieval augmented approaches\\noutperform single-time retrieval but with different\\nMethods EM F 1 Prec. Rec.\\nNo retrieval 28.2 36.8 36.5 38.6\\nSingle-time retrieval 39.4 48.8 48.6 51.5\\nMulti-time retrieval\\nPrevious-window 43.2 52.3 51.7 54.5\\nPrevious-sentence 39.0 49.2 48.9 51.8\\nQuestion decomposition 47.8 56.4 56.1 58.6\\nFLAREinstruct (ours) 42.4 49.8 49.1 52.5\\nFLAREdirect (ours) 51.0 59.7 59.1 62.6\\nTable 1: FLARE and baselines on 2WikiMultihopQA.\\nPrevious-window (Borgeaud et al., 2022; Ram et al.,\\n2023), previous-sentence (Trivedi et al., 2022), and ques-\\ntion decomposition (Press et al., 2022; Yao et al., 2022)\\nmethods are reimplemented for fair comparisons.\\nmargins. The improvement of retrieving using the\\nprevious sentence is relatively small which we hy-\\npothesize is mainly because the previous sentence\\noften describes entities or relations different from\\nthose in the next sentence in 2WikiMultihopQA.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 6, 'page_label': '7'}, page_content='While the previous-window approach might use\\nthe first half of a sentence to retrieve information\\npotentially helpful for generating the second half.\\nAmong all baselines, the question decomposition\\napproach (Press et al., 2022) achieves the best per-\\nformance. which is not surprising since the in-\\ncontext exemplars manually annotated with decom-\\nposed sub-questions (Prompt D.2) guide LMs to\\ngenerate sub-questions that align with the topic/in-\\ntent of future generations. FLARE outperforms\\nthis baseline, indicating that manual exemplar an-\\nnotation is not necessary for effective future-aware\\nretrieval. The gap between FLAREinstruct and ques-\\ntion decomposition is large, indicating that teaching\\nLMs to generate search queries using task-generic\\nretrieval instructions and exemplars is challenging.\\nWe report all metrics for the other datasets in\\nTable 2. FLARE outperforms baselines with re-\\nspect to all metrics. Retrieval using the previ-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 7, 'page_label': '8'}, page_content='Datasets StrategyQA ASQA ASQA-hint WikiAsp\\nMetrics EM EM D-F 1 R-L DR EM D-F 1 R-L DR UniEval E-F1 R-L\\nNo retrieval 72.9 33.8 24.2 33.3 28.4 40.1 32.5 36.4 34.4 47.1 14.1 26.4\\nSingle-time retrieval 68.6 40.0 27.1 34.0 30.4 43.2 34.8 37.4 36.0 52.4 17.4 26.9\\nMulti-time retrieval\\nPrevious-window 71.2 39.9 27.0 34.3 30.4 43.7 35.7 37.5 36.6 51.8 18.1 27.3\\nPrevious-sentence 71.0 39.9 27.9 34.3 30.9 44.7 35.9 37.5 36.7 52.6 17.8 27.2\\nFLARE (ours) 77.3 41.3 28.2 34.3 31.1 46.2 36.7 37.7 37.2 53.4 18.9 27.6\\nTable 2: Comparison between FLARE and baselines on StrategyQA, ASQA, ASQA-hint, and WikiAsp. D-F1 is\\nDisambig-F1, R-L is ROUGE-L, and E-F1 is named entity-based F1.\\n2WikiMultihopQA ASQA-hint\\nEM F 1 Prec. Rec. EM D-F 1 R-L DR\\nPrevious 39.0 49.2 48.9 51.8 42.5 34.1 36.9 35.5\\nNext 48.8 57.6 57.1 60.5 45.9 35.7 37.5 36.6\\nTable 3: A head-to-head comparison between using the\\nprevious sentence and the next sentence for retrieval.\\n#Tokens EM F 1 Prec. Rec.\\n16 43.2 52.3 51.7 54.5'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 7, 'page_label': '8'}, page_content='32 43.6 52.4 52.0 55.0\\n48 40.0 49.3 49.0 52.0\\nAll 39.0 48.5 48.2 51.1\\nTable 4: Previous-window approaches using different\\nnumbers of tokens as queries.\\nous window underperforms single-time retrieval\\non ASQA, which we hypothesize is because the\\nprevious window does not accurately reflect future\\nintent. Since we focus on evaluating factuality, met-\\nrics with an emphasis on factual content (such as\\nEM, Disambig-F1, UniEval) are more reliable than\\nmetrics computed over all tokens (ROUGE-L).\\n6.2 Ablation Study\\nImportance of forward-looking retrieval. We\\nfirst validate that forward-looking retrieval is more\\neffective than past-context-based retrieval. We run\\nablation experiments on 2WikiMultihopQA and\\nASQA-hint comparing retrieval using the previ-\\nous versus the next sentence. Specifically, both\\nmethods retrieve every sentence and directly use\\nthe complete previous/next sentence as queries. As\\nshown in Table 3, using the next sentence to retrieve'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 7, 'page_label': '8'}, page_content='is clearly better than using the previous sentence,\\nconfirming our hypothesis.\\nWe also run previous-window approaches using\\ndifferent numbers of past tokens as queries. As\\nshown in Table 4, using too many tokens (> 32) in\\n%steps/sentences with retrieval\\n0.0\\n20.0\\n40.0\\n60.0\\n80.0\\n0.0 25.0 50.0 75.0 100.0\\n2WikiMultihopQA StrategyQA\\nFigure 5: Performance (EM) of FLARE with respect\\nto the percentage of steps/sentences with retrieval on\\n2WikiMultihopQA and StrategyQA.\\nthe past hurts the performance, further confirming\\nour hypothesis that previous context might not be\\nrelevant to intent of future generations.\\nImportance of active retrieval. Next, we inves-\\ntigate how active retrieval threshold θ affects per-\\nformance. To alter our method from not retrieving\\nto retrieving every sentence, we adjust the confi-\\ndence threshold θ that determines when to trigger\\nretrieval from 0 to 1. We then calculate the pro-\\nportion of steps/sentences where retrieval is acti-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 7, 'page_label': '8'}, page_content='vated, and present the performance based on it. As\\nshown in Figure 5, on 2WikiMultihopQA, the per-\\nformance plateaus when the retrieval percentage\\nexceeds 60%, indicating that retrieval when LMs\\nare confident is not necessary. On StrategyQA, the\\nperformance drops when the retrieval percentage\\nexceeds 50%, indicating that unnecessary retrieval\\ncan introduce noise and impede the original gen-\\neration process. We found triggering retrieval for\\n40%-80% of sentences usually leads to a good per-\\nformance across tasks/datasets.\\nEffectiveness of different query formulation\\nmethods We study implicit query formation by\\nmasking and explicit query formulation through\\nquestion generation. In Table 5, we compare the\\nperformance of FLARE with different masking'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 8, 'page_label': '9'}, page_content='β EM F 1 Prec. Rec.\\n0.0 0.488 0.576 0.571 0.605\\n0.2 0.498 0.588 0.582 0.616\\n0.4 0.510 0.597 0.591 0.627\\n0.6 0.506 0.593 0.586 0.622\\nTable 5: Performance of FLARE with respect to the\\nmasking threshold β on 2WikiMultihopQA.\\nASQA-hint WikiAsp\\nEM D-F 1 R-L DR UniEval E-F 1 R-L\\nImplicit 45.7 36.9 37.7 37.3 53.4 18.8 27.7\\nExplicit 46.2 36.7 37.7 37.2 53.4 18.9 27.6\\nTable 6: A comparison between implicit and explicit\\nquery formulation methods in FLARE.\\nthresholds β. Retrieving directly with the complete\\nsentence (β = 0) is worse than masking tokens\\nwith low probabilities, confirming our hypothesis\\nthat low-confidence erroneous tokens can distract\\nretrievers. We compare implicit and explicit query\\nformulation methods in Table 6. Performances of\\nboth methods are similar, indicating that both meth-\\nods can effectively reflect information needs.\\n7 Related Work\\nWe refer to subsection 2.2 and section 4 for ex-\\ntensively discussion on single-time and multi-time'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 8, 'page_label': '9'}, page_content='retrieval augmented LMs, which is the most rele-\\nvant area to this paper.\\nIterative and adaptive retrieval Iterative re-\\ntrieval and refinement has been studied in both\\ntext and code generation tasks (Peng et al., 2023;\\nZhang et al., 2023; Zemlyanskiy et al., 2022; Yu\\net al., 2023). FLARE differs from these methods in\\nthe granularity of generation and retrieval strategies.\\nAdaptive retrieval has been studied in single-time\\nretrieval scenarios based on either question pop-\\nularity or generation probabilities (Mallen et al.,\\n2022; Li et al., 2023), while we focus on long-form\\ngeneration requiring active information access.\\nBrowser-enhanced LMs WebGPT (Nakano\\net al., 2021) and WebCPM (Qin et al., 2023) train\\nLMs to interact with browser to enhance factuality\\nusing reinforcement learning or supervised train-\\ning where multiple queries can be triggered before\\ngeneration. FLARE is built on text-based retrievers\\nbut can be combined with a browser to potentially\\nimprove retrieval quality.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 8, 'page_label': '9'}, page_content='8 Conclusion\\nTo aid long-form generation with retrieval aug-\\nmentation, we propose an active retrieval aug-\\nmented generation framework that decides when\\nand what to retrieve during generation. We imple-\\nment this framework with forward-looking active\\nretrieval that iteratively uses the upcoming sentence\\nto retrieve relevant information if it contains low-\\nconfidence tokens and regenerates the next sen-\\ntence. Experimental results on 4 tasks/datasets\\ndemonstrate the effectiveness of our methods. Fu-\\nture directions include better strategies for active\\nretrieval and developing efficient LM architectures\\nfor active information integration.\\n9 Limitations\\nWe also conduct experiments on Wizard of\\nWikipedia (Dinan et al., 2019) and ELI5 (Fan et al.,\\n2019), and found that FLARE did not provide sig-\\nnificant gains. Wizard of Wikipedia is a knowledge-\\nintensive dialogue generation dataset where the out-\\nput is relatively short (∼20 tokens on average) so'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 8, 'page_label': '9'}, page_content='retrieving multiple disparate pieces of information\\nmight not be necessary. ELI5 (Fan et al., 2019)\\nis a long-form QA dataset requiring in-depth an-\\nswers to open-ended questions. Due to issues men-\\ntioned in Krishna et al. (2021) such as difficulties\\nof grounding generation in retrieval and evalua-\\ntion, both single-time retrieval and FLARE did not\\nprovide significant gains over not using retrieval.\\nFrom an engineering perspective, interleaving gen-\\neration and retrieval with a naive implementation\\nincreases both overheads and the cost of generation.\\nLMs need to be activated multiple times (once for\\neach retrieval) and a caching-free implementation\\nalso requires recomputing the previous activation\\neach time after retrieval. This issue can be poten-\\ntially alleviated with special architectural designs\\nthat encode the retrieved documents Dqt and the\\ninput/generation (x/y<t) independently.\\nAcknowledgements\\nThis work was supported in part by a grant from'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 8, 'page_label': '9'}, page_content='the Singapore Defence Science and Technology\\nAgency and the IBM PhD Fellowship. We thank\\nChunting Zhou, Amanda Bertsch, Uri Alon, Hi-\\nroaki Hayashi, Harsh Trivedi, Patrick Lewis, Timo\\nSchick, Kaixin Ma, Shuyan Zhou, and Songwei Ge\\nfor their insightful discussions and help with the\\nexperiments.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 9, 'page_label': '10'}, page_content='References\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\\nTrevor Cai, Eliza Rutherford, Katie Millican, George\\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia\\nGuy, Jacob Menick, Roman Ring, Tom Hennigan,\\nSaffron Huang, Loren Maggiore, Chris Jones, Albin\\nCassirer, Andy Brock, Michela Paganini, Geoffrey\\nIrving, Oriol Vinyals, Simon Osindero, Karen Si-\\nmonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.\\n2022. Improving language models by retrieving from\\ntrillions of tokens. In International Conference on\\nMachine Learning, ICML 2022, 17-23 July 2022, Bal-\\ntimore, Maryland, USA, volume 162 of Proceedings\\nof Machine Learning Research , pages 2206–2240.\\nPMLR.\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\\nGretchen Krueger, Tom Henighan, Rewon Child,\\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 9, 'page_label': '10'}, page_content='Clemens Winter, Christopher Hesse, Mark Chen, Eric\\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\\nJack Clark, Christopher Berner, Sam McCandlish,\\nAlec Radford, Ilya Sutskever, and Dario Amodei.\\n2020. Language models are few-shot learners. In Ad-\\nvances in Neural Information Processing Systems 33:\\nAnnual Conference on Neural Information Process-\\ning Systems 2020, NeurIPS 2020, December 6-12,\\n2020, virtual.\\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\\nBordes. 2017. Reading wikipedia to answer open-\\ndomain questions. In Proceedings of the 55th Annual\\nMeeting of the Association for Computational Lin-\\nguistics, ACL 2017, Vancouver, Canada, July 30 -\\nAugust 4, Volume 1: Long Papers, pages 1870–1879.\\nAssociation for Computational Linguistics.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\\nPaul Barham, Hyung Won Chung, Charles Sutton,\\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\\nSasha Tsvyashchenko, Joshua Maynez, Abhishek'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 9, 'page_label': '10'}, page_content='Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\\nHutchinson, Reiner Pope, James Bradbury, Jacob\\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\\nRewon Child, Oleksandr Polozov, Katherine Lee,\\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\\nand Noah Fiedel. 2022. Palm: Scaling language mod-\\neling with pathways. CoRR, abs/2204.02311.\\nNachshon Cohen, Oren Kalinsky, Yftah Ziser, and\\nAlessandro Moschitti. 2021. Wikisum: Coherent'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 9, 'page_label': '10'}, page_content='summarization dataset for efficient human-evaluation.\\nIn Proceedings of the 59th Annual Meeting of the As-\\nsociation for Computational Linguistics and the 11th\\nInternational Joint Conference on Natural Language\\nProcessing, ACL/IJCNLP 2021, (Volume 2: Short\\nPapers), Virtual Event, August 1-6, 2021, pages 212–\\n219. Association for Computational Linguistics.\\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\\nFan, Michael Auli, and Jason Weston. 2019. Wizard\\nof wikipedia: Knowledge-powered conversational\\nagents. In 7th International Conference on Learning\\nRepresentations, ICLR 2019, New Orleans, LA, USA,\\nMay 6-9, 2019. OpenReview.net.\\nAngela Fan, Yacine Jernite, Ethan Perez, David Grang-\\nier, Jason Weston, and Michael Auli. 2019. ELI5:\\nlong form question answering. In Proceedings of\\nthe 57th Conference of the Association for Compu-\\ntational Linguistics, ACL 2019, Florence, Italy, July\\n28- August 2, 2019, Volume 1: Long Papers, pages\\n3558–3567. Association for Computational Linguis-\\ntics.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 9, 'page_label': '10'}, page_content='tics.\\nLuyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan.\\n2022. Precise zero-shot dense retrieval without rele-\\nvance labels. CoRR, abs/2212.10496.\\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\\nDan Roth, and Jonathan Berant. 2021. Did aristotle\\nuse a laptop? a question answering benchmark with\\nimplicit reasoning strategies. Transactions of the\\nAssociation for Computational Linguistics , 9:346–\\n361.\\nJohn M. Giorgi, Luca Soldaini, Bo Wang, Gary D.\\nBader, Kyle Lo, Lucy Lu Wang, and Arman Co-\\nhan. 2022. Exploring the challenges of open\\ndomain multi-document summarization. CoRR,\\nabs/2212.10526.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\\npat, and Ming-Wei Chang. 2020. REALM: retrieval-\\naugmented language model pre-training. CoRR,\\nabs/2002.08909.\\nHiroaki Hayashi, Prashant Budania, Peng Wang, Chris\\nAckerson, Raj Neervannan, and Graham Neubig.\\n2021. Wikiasp: A dataset for multi-domain aspect-\\nbased summarization. Trans. Assoc. Comput. Lin-\\nguistics, 9:211–225.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 9, 'page_label': '10'}, page_content='Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\\n2020. Measuring massive multitask language under-\\nstanding. CoRR, abs/2009.03300.\\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,\\nand Akiko Aizawa. 2020. Constructing A multi-hop\\nQA dataset for comprehensive evaluation of reason-\\ning steps. In Proceedings of the 28th International\\nConference on Computational Linguistics, COLING\\n2020, Barcelona, Spain (Online), December 8-13,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 10, 'page_label': '11'}, page_content='2020, pages 6609–6625. International Committee on\\nComputational Linguistics.\\nGautier Izacard and Edouard Grave. 2021. Leveraging\\npassage retrieval with generative models for open do-\\nmain question answering. In Proceedings of the 16th\\nConference of the European Chapter of the Associ-\\nation for Computational Linguistics: Main Volume,\\nEACL 2021, Online, April 19 - 23, 2021, pages 874–\\n880. Association for Computational Linguistics.\\nGautier Izacard, Patrick S. H. Lewis, Maria Lomeli,\\nLucas Hosseini, Fabio Petroni, Timo Schick, Jane\\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\\nEdouard Grave. 2022. Few-shot learning with\\nretrieval augmented language models. CoRR,\\nabs/2208.03299.\\nZhengbao Jiang, Jun Araki, Haibo Ding, and Graham\\nNeubig. 2021. How can we know When language\\nmodels know? on the calibration of language mod-\\nels for question answering. Trans. Assoc. Comput.\\nLinguistics, 9:962–977.\\nZhengbao Jiang, Luyu Gao, Jun Araki, Haibo Ding,\\nZhiruo Wang, Jamie Callan, and Graham Neubig.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 10, 'page_label': '11'}, page_content='2022. Retrieval as attention: End-to-end learning\\nof retrieval and reading within a single transformer.\\nCoRR, abs/2212.02027.\\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\\nNeubig. 2020. How can we know what language\\nmodels know. Trans. Assoc. Comput. Linguistics ,\\n8:423–438.\\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In Proceedings of the 55th Annual Meeting of\\nthe Association for Computational Linguistics, ACL\\n2017, Vancouver, Canada, July 30 - August 4, Volume\\n1: Long Papers, pages 1601–1611. Association for\\nComputational Linguistics.\\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\\nHenighan, Dawn Drain, Ethan Perez, Nicholas\\nSchiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli\\nTran-Johnson, Scott Johnston, Sheer El Showk, Andy\\nJones, Nelson Elhage, Tristan Hume, Anna Chen,\\nYuntao Bai, Sam Bowman, Stanislav Fort, Deep\\nGanguli, Danny Hernandez, Josh Jacobson, Jack-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 10, 'page_label': '11'}, page_content='son Kernion, Shauna Kravec, Liane Lovitt, Ka-\\nmal Ndousse, Catherine Olsson, Sam Ringer, Dario\\nAmodei, Tom Brown, Jack Clark, Nicholas Joseph,\\nBen Mann, Sam McCandlish, Chris Olah, and Jared\\nKaplan. 2022. Language models (mostly) know what\\nthey know. CoRR, abs/2207.05221.\\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\\nS. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,\\nand Wen-tau Yih. 2020. Dense passage retrieval for\\nopen-domain question answering. In Proceedings of\\nthe 2020 Conference on Empirical Methods in Nat-\\nural Language Processing, EMNLP 2020, Online,\\nNovember 16-20, 2020, pages 6769–6781. Associa-\\ntion for Computational Linguistics.\\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\\nZettlemoyer, and Mike Lewis. 2020. Generalization\\nthrough memorization: Nearest neighbor language\\nmodels. In 8th International Conference on Learning\\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia,\\nApril 26-30, 2020. OpenReview.net.\\nOmar Khattab, Keshav Santhanam, Xiang Lisa Li,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 10, 'page_label': '11'}, page_content='David Hall, Percy Liang, Christopher Potts, and\\nMatei Zaharia. 2022. Demonstrate-search-predict:\\nComposing retrieval and language models for\\nknowledge-intensive NLP. CoRR, abs/2212.14024.\\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu,\\nKyle Richardson, Peter Clark, and Ashish Sabharwal.\\n2022. Decomposed prompting: A modular approach\\nfor solving complex tasks. CoRR, abs/2210.02406.\\nKalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021.\\nHurdles to progress in long-form question answering.\\nIn North American Association for Computational\\nLinguistics.\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\\nfield, Michael Collins, Ankur P. Parikh, Chris Alberti,\\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\\nton Lee, Kristina Toutanova, Llion Jones, Matthew\\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\\nral questions: a benchmark for question answering\\nresearch. Trans. Assoc. Comput. Linguistics, 7:452–\\n466.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 10, 'page_label': '11'}, page_content='466.\\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech\\nStokowiec, and Nikolai Grigorev. 2022. Internet-\\naugmented language models through few-shot\\nprompting for open-domain question answering.\\nCoRR, abs/2203.05115.\\nHaejun Lee, Akhil Kedia, Jongwon Lee, Ashwin Paran-\\njape, Christopher D. Manning, and Kyoung-Gu Woo.\\n2021. You only need one model for open-domain\\nquestion answering. CoRR, abs/2112.07381.\\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,\\nTim Rocktäschel, Sebastian Riedel, and Douwe\\nKiela. 2020. Retrieval-augmented generation for\\nknowledge-intensive NLP tasks. In Advances in Neu-\\nral Information Processing Systems 33: Annual Con-\\nference on Neural Information Processing Systems\\n2020, NeurIPS 2020, December 6-12, 2020, virtual.\\nJunyi Li, Tianyi Tang, Wayne Xin Zhao, Jingyuan Wang,\\nJian-Yun Nie, and Ji-Rong Wen. 2023. The web can\\nbe your oyster for improving large language models.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 10, 'page_label': '11'}, page_content='CoRR, abs/2305.10998.\\nChin-Yew Lin. 2004. ROUGE: A package for auto-\\nmatic evaluation of summaries. In Text Summariza-\\ntion Branches Out, pages 74–81, Barcelona, Spain.\\nAssociation for Computational Linguistics.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 11, 'page_label': '12'}, page_content='Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\\ntrain, prompt, and predict: A systematic survey of\\nprompting methods in natural language processing.\\nACM Comput. Surv., 55(9):195:1–195:35.\\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\\nHannaneh Hajishirzi, and Daniel Khashabi. 2022.\\nWhen not to trust language models: Investigating\\neffectiveness and limitations of parametric and non-\\nparametric memories. CoRR, abs/2212.10511.\\nYuning Mao, Pengcheng He, Xiaodong Liu, Yelong\\nShen, Jianfeng Gao, Jiawei Han, and Weizhu Chen.\\n2021. Generation-augmented retrieval for open-\\ndomain question answering. In Proceedings of the\\n59th Annual Meeting of the Association for Compu-\\ntational Linguistics and the 11th International Joint\\nConference on Natural Language Processing, ACL/I-\\nJCNLP 2021, (Volume 1: Long Papers), Virtual Event,\\nAugust 1-6, 2021, pages 4089–4100. Association for\\nComputational Linguistics.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 11, 'page_label': '12'}, page_content='Joshua Maynez, Shashi Narayan, Bernd Bohnet, and\\nRyan McDonald. 2020. On faithfulness and factu-\\nality in abstractive summarization. In Proceedings\\nof the 58th Annual Meeting of the Association for\\nComputational Linguistics, pages 1906–1919, On-\\nline. Association for Computational Linguistics.\\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\\nLong Ouyang, Christina Kim, Christopher Hesse,\\nShantanu Jain, Vineet Kosaraju, William Saunders,\\nXu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen\\nKrueger, Kevin Button, Matthew Knight, Benjamin\\nChess, and John Schulman. 2021. Webgpt: Browser-\\nassisted question-answering with human feedback.\\nCoRR, abs/2112.09332.\\nOpenAI. 2023. GPT-4 technical report. CoRR,\\nabs/2303.08774.\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\\nSandhini Agarwal, Katarina Slama, Alex Ray, John\\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\\nMaddie Simens, Amanda Askell, Peter Welinder,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 11, 'page_label': '12'}, page_content='Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022.\\nTraining language models to follow instructions with\\nhuman feedback. CoRR, abs/2203.02155.\\nBaolin Peng, Michel Galley, Pengcheng He, Hao Cheng,\\nYujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou\\nYu, Weizhu Chen, and Jianfeng Gao. 2023. Check\\nyour facts and try again: Improving large language\\nmodels with external knowledge and automated feed-\\nback. CoRR, abs/2302.12813.\\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\\nPatrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu,\\nand Alexander H. Miller. 2019. Language mod-\\nels as knowledge bases? In Proceedings of the\\n2019 Conference on Empirical Methods in Natu-\\nral Language Processing and the 9th International\\nJoint Conference on Natural Language Processing,\\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\\nber 3-7, 2019 , pages 2463–2473. Association for\\nComputational Linguistics.\\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\\nNoah A Smith, and Mike Lewis. 2022. Measuring'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 11, 'page_label': '12'}, page_content='and narrowing the compositionality gap in language\\nmodels. arXiv preprint arXiv:2210.03350.\\nHongjing Qian, Yutao Zhu, Zhicheng Dou, Haoqi Gu,\\nXinyu Zhang, Zheng Liu, Ruofei Lai, Zhao Cao,\\nJian-Yun Nie, and Ji-Rong Wen. 2023. Webbrain:\\nLearning to generate factually correct articles for\\nqueries by grounding on large web corpus. CoRR,\\nabs/2304.04358.\\nYujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao\\nLiang, Kunlun Zhu, Yankai Lin, Xu Han, Ning Ding,\\nHuadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan\\nLiu, Maosong Sun, and Jie Zhou. 2023. Webcpm: In-\\nteractive web search for chinese long-form question\\nanswering. CoRR, abs/2305.06849.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\\nDario Amodei, and Ilya Sutskever. 2019. Language\\nmodels are unsupervised multitask learners. OpenAI\\nBlog, 1(8).\\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\\nShoham. 2023. In-context retrieval-augmented lan-\\nguage models. arXiv preprint arXiv:2302.00083.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 11, 'page_label': '12'}, page_content='Adam Roberts, Colin Raffel, and Noam Shazeer. 2020.\\nHow much knowledge can you pack into the param-\\neters of a language model? In Proceedings of the\\n2020 Conference on Empirical Methods in Natural\\nLanguage Processing, EMNLP 2020, Online, Novem-\\nber 16-20, 2020, pages 5418–5426. Association for\\nComputational Linguistics.\\nStephen E. Robertson and Hugo Zaragoza. 2009. The\\nprobabilistic relevance framework: BM25 and be-\\nyond. Found. Trends Inf. Retr., 3(4):333–389.\\nDevendra Singh Sachan, Siva Reddy, William L. Hamil-\\nton, Chris Dyer, and Dani Yogatama. 2021. End-to-\\nend training of multi-document reader and retriever\\nfor open-domain question answering. In Advances\\nin Neural Information Processing Systems 34: An-\\nnual Conference on Neural Information Processing\\nSystems 2021, NeurIPS 2021, December 6-14, 2021,\\nvirtual, pages 25968–25981.\\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\\nCancedda, and Thomas Scialom. 2023. Toolformer:'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 11, 'page_label': '12'}, page_content='Language models can teach themselves to use tools.\\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon\\nSeo, Rich James, Mike Lewis, Luke Zettlemoyer, and\\nWen-tau Yih. 2023. REPLUG: retrieval-augmented\\nblack-box language models. CoRR, abs/2301.12652.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 12, 'page_label': '13'}, page_content='Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-\\nWei Chang. 2022. ASQA: factoid questions meet\\nlong-form answers. In Proceedings of the 2022 Con-\\nference on Empirical Methods in Natural Language\\nProcessing, EMNLP 2022, Abu Dhabi, United Arab\\nEmirates, December 7-11, 2022, pages 8273–8288.\\nAssociation for Computational Linguistics.\\nZhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and\\nDenny Zhou. 2022. Recitation-augmented language\\nmodels. CoRR, abs/2210.01296.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\\nAzhar, Aurélien Rodriguez, Armand Joulin, Edouard\\nGrave, and Guillaume Lample. 2023. Llama: Open\\nand efficient foundation language models. CoRR,\\nabs/2302.13971.\\nHarsh Trivedi, Niranjan Balasubramanian, Tushar\\nKhot, and Ashish Sabharwal. 2022. Interleav-\\ning retrieval with chain-of-thought reasoning for\\nknowledge-intensive multi-step questions. CoRR,\\nabs/2212.10509.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 12, 'page_label': '13'}, page_content='abs/2212.10509.\\nNeeraj Varshney, Man Luo, and Chitta Baral. 2022. Can\\nopen-domain QA reader utilize external knowledge\\nefficiently like humans? CoRR, abs/2211.12707.\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V .\\nLe, Ed H. Chi, and Denny Zhou. 2022. Self-\\nconsistency improves chain of thought reasoning in\\nlanguage models. CoRR, abs/2203.11171.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022.\\nChain of thought prompting elicits reasoning in large\\nlanguage models. CoRR, abs/2201.11903.\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\\nReact: Synergizing reasoning and acting in language\\nmodels. CoRR, abs/2210.03629.\\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,\\nMingxuan Ju, Soumya Sanyal, Chenguang Zhu,\\nMichael Zeng, and Meng Jiang. 2022. Generate\\nrather than retrieve: Large language models are\\nstrong context generators. CoRR, abs/2209.10063.\\nWenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 12, 'page_label': '13'}, page_content='Jiang, and Ashish Sabharwal. 2023. Improving lan-\\nguage models via plug-and-play retrieval feedback.\\nCoRR, abs/2305.14002.\\nYury Zemlyanskiy, Michiel de Jong, Joshua Ainslie,\\nPanupong Pasupat, Peter Shaw, Linlu Qiu, Sumit\\nSanghai, and Fei Sha. 2022. Generate-and-retrieve:\\nUse your predictions to improve retrieval for seman-\\ntic parsing. In Proceedings of the 29th International\\nConference on Computational Linguistics, COLING\\n2022, Gyeongju, Republic of Korea, October 12-17,\\n2022, pages 4946–4951. International Committee on\\nComputational Linguistics.\\nFengji Zhang, Bei Chen, Yue Zhang, Jin Liu, Daoguang\\nZan, Yi Mao, Jian-Guang Lou, and Weizhu Chen.\\n2023. Repocoder: Repository-level code completion\\nthrough iterative retrieval and generation. CoRR,\\nabs/2303.12570.\\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 12, 'page_label': '13'}, page_content='Simig, Punit Singh Koura, Anjali Sridhar, Tianlu\\nWang, and Luke Zettlemoyer. 2022. Opt: Open\\npre-trained transformer language models. ArXiv,\\nabs/2205.01068.\\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\\nXiaolei Wang, Yupeng Hou, Yingqian Min, Be-\\nichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,\\nChen Yang, Yushuo Chen, Zhipeng Chen, Jinhao\\nJiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang\\nLiu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.\\n2023. A survey of large language models. CoRR,\\nabs/2303.18223.\\nMing Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu\\nJiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and\\nJiawei Han. 2022. Towards a unified multi-\\ndimensional evaluator for text generation. In Pro-\\nceedings of the 2022 Conference on Empirical Meth-\\nods in Natural Language Processing, EMNLP 2022,\\nAbu Dhabi, United Arab Emirates, December 7-11,\\n2022, pages 2023–2038. Association for Computa-\\ntional Linguistics.\\nChunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 12, 'page_label': '13'}, page_content='Francisco Guzmán, Luke Zettlemoyer, and Marjan\\nGhazvininejad. 2021. Detecting hallucinated content\\nin conditional neural sequence generation. In Find-\\nings of the Association for Computational Linguis-\\ntics: ACL-IJCNLP 2021, pages 1393–1404, Online.\\nAssociation for Computational Linguistics.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 13, 'page_label': '14'}, page_content='A FLARE Implementation Details\\nFLAREinstruct implementation details We\\nfound that LMs can effectively combine retrieval\\nand downstream task-related skills and generate\\nmeaningful search queries while performing the\\ntask. However, there are two issues: (1) LMs tend\\nto generate fewer search queries than necessary.\\n(2) Generating excessive search queries can\\ndisrupt answer generation and adversely affect\\nperformance. We address these issues using two\\nmethods respectively. First, we increase the logit\\nof the token “[” by 2.0 to improve the chances\\nof LMs generating “[Search(query)]”. Second,\\nwhenever LMs generate a search query, we use it\\nto retrieve relevant information, promptly remove\\nit from the generation, and generate the next few\\ntokens while forbidding “[” by adding a large\\nnegative value to the logit of “[”.\\nThe initial query of FLARE. FLARE starts\\nwith the user input x as the initial query to re-\\ntrieve documents to generate the first sentence'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 13, 'page_label': '14'}, page_content='ˆs1 = LM([Dx, x]) to bootstrap the iterative gener-\\nation process. For the following steps, the tempo-\\nrary forward-looking sentence is generated without\\nretrieved documents.\\nSentence tokenization. For each step t, we gen-\\nerate 64 tokens which are longer than most sen-\\ntences, and use NLTK sentence tokenizer 5 to ex-\\ntract the first sentence and discard the rest.\\nEfficiency As shown in subsection 6.2, on aver-\\nage retrieval is triggered for 30% ∼ 60% of sen-\\ntences depending on downstream tasks. In compar-\\nision, KNN-LM (Khandelwal et al., 2020) retrieves\\nevery token, RETRO or IC-RALM (Borgeaud et al.,\\n2022; Ram et al., 2023) retrievers every 4∼32 to-\\nkens, and IRCoT (Trivedi et al., 2022) retrieves\\nevery sentence. Compared to single-time retrieval,\\nhowever, interleaving retrieval and generation with\\na naive implementation indeed increases overheads,\\nwhich we discuss in the limitation section (sec-\\ntion 9).\\nB Datasets and Settings\\nDatasets, metrics, and experimental settings are'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 13, 'page_label': '14'}, page_content='summarized in Table 7.\\n5https://www.nltk.org/api/nltk.tokenize.\\nPunktSentenceTokenizer.html\\nMultihop QA For “Why did the founder of Ver-\\nsus die?”, the output we aim to generate is “The\\nfounder of Versus was Gianni Versace. Gianni Ver-\\nsace was shot and killed on the steps of his Miami\\nBeach mansion on July 15, 1997. So the answer\\nis shot.” We use 8 exemplars from Trivedi et al.\\n(2022) listed in Prompt D.4 for in-context learn-\\ning, BM25 as the retriever, and Wikipedia articles\\nas the retrieval corpus. Similar to the observation\\nin Trivedi et al. (2022), we found incorporating\\nretrieval results for exemplars improves the per-\\nformance, we use the input x of each exemplar to\\nretrieve several documents and then add them using\\nthe format in Prompt D.1. We found increasing the\\nnumber of retrieval documents often increases per-\\nformance. Therefore, we use the maximum number\\nof documents that can fit within the input length\\nlimit of text-davinci-003, which is 2 for 2Wiki-\\nMultihopQA.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 13, 'page_label': '14'}, page_content='MultihopQA.\\nCommonsense Reasoning For “Would a pear\\nsink in water?”, the output we aim to generate is\\n“The density of a pear is about 0.6g/cm3, which is\\nless than water. Objects less dense than water float.\\nThus, a pear would float. So the final answer is no.”\\nWe use 6 exemplars from Wei et al. (2022) listed in\\nPrompt D.5, BM25 on the Wikipedia corpus, and 3\\nretrieved documents to run experiments.\\nLong-form QA For “Where do the Philadelphia\\nEagles play their home games?”, the output we\\naim to generate is “We need to consider the dif-\\nferent possible locations or venues that could be\\nconsidered the home field of the Philadelphia Ea-\\ngles. These include the city, the sports complex,\\nor the stadium. Therefore, this question has 3 in-\\nterpretations and the answers are: (1) The city is\\nPhiladelphia. (2) The sports complex is the South\\nPhiladelphia Sports Complex. (3) The stadium is\\nthe Lincoln Financial Field stadium.” For both the\\noriginal setting (ASQA) and the setting with hints'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 13, 'page_label': '14'}, page_content='(ASQA-hint), we manually annotate 8 exemplars\\n(Prompt D.6 and D.8), use BM25 on the Wikipedia\\ncorpus, and 3 retrieved documents to run experi-\\nments.\\nOpen-domain Summarization The original\\nWikiAsp dataset is designed for multi-document\\nsummarization and provides a list of references to\\nsystems. We converted it into the open-domain\\nsetting by removing the associated references and\\ninstead gathering information from the open web.\\nFor “Generate a summary about Echo School (Ore-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 14, 'page_label': '15'}, page_content='gon) including the following aspects: academics,\\nhistory.”, the output we aim to generate is “# Aca-\\ndemics. In 2008, 91% of the school’s seniors re-\\nceived their high school diploma... # History. The\\nclass of 2008 was the 100th class in the school’s\\nhistory.” where # is used to indicate aspects. We\\nmanually annotate 4 exemplars (Prompt D.10), and\\nuse the Bing search engine to retrieve 5 documents\\nfrom the open web. To avoid leaking, we exclude\\nseveral Wikipedia-related domains listed in Table 8\\nfrom Bing’s search results.\\nC Hyperparameters\\nHyperparameters of FLARE on different datasets\\nare listed in Table 9.\\nD Prompts and Few-shot exemplars\\nThe prompt used to linearize multiple documents\\nis shown in Prompt D.1. The prompt used in self-\\nask (Press et al., 2022) is shown in Prompt D.2.\\nPrompts and exemplars of different tasks/datasets\\nare shown in Prompt D.3, D.4, D.5, D.6, D.8, and\\nD.10, respectively.\\nPrompt D.1: document formatting\\nSearch results:\\n[1] Document 1\\n[2] Document 2\\n...'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 14, 'page_label': '15'}, page_content='[2] Document 2\\n...\\nThe user input x\\nPrompt D.2: multihop QA with self-ask\\nQuestion: Who lived longer, Theodor Haecker or Harry\\nVaughan Watkins?\\nAre follow up questions needed here: Yes.\\nFollow up: How old was Theodor Haecker when he died?\\nIntermediate answer: Theodor Haecker was 65 years old\\nwhen he died.\\nFollow up: How old was Harry Vaughan Watkins when he\\ndied?\\nIntermediate answer: Harry Vaughan Watkins was 69 years\\nold when he died.\\nSo the final answer is: Harry Vaughan Watkins.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 15, 'page_label': '16'}, page_content='Settings 2WikiMultihopQA StrategyQA ASQA WikiAsp\\n(Ho et al., 2020) (Geva et al., 2021) (Stelmakh et al., 2022) (Hayashi et al., 2021)\\nDataset statistics\\nTask multihop QA commonsense QA long-form QA open-domain summarization\\n#Examples 500 229 500 500\\nEvaluation settings\\nMetrics EM, F 1, Prec., Rec. EM EM, Disambig-F 1, ROUGE, DR UniEval, entity-F1, ROUGE\\nRetrieval settings\\nCorpus Wikipedia Wikipedia Wikipedia open web\\nRetriever BM25 BM25 BM25 Bing\\nTop-k 2 3 3 5\\nPrompt format\\n#Exemplars 8 6 8 4\\nRet. for exemplars ✓ ✗ ✗ ✗\\nTable 7: Dataset statistics and experimental settings of different tasks.\\nwikipedia.org, wikiwand.com, wiki2.org, wikimedia.org\\nTable 8: Wikipedia-related domains excluded from Bing’s search results.\\nDataset θ β Query formulation Combine single- & multi-time retrieval\\n2WikiMultihopQA 0.8 0.4 implicit ✗\\nStrategyQA 0.4 0.4 implicit ✗\\nASQA & ASQA-hint 0.8 0.4 explicit ✓\\nWikiAsp 0.8 0.4 explicit ✓\\nTable 9: Hyperparameters of FLARE on different datasets.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 16, 'page_label': '17'}, page_content='Prompt D.3: retrieval instructions for 2WikiMultihopQA\\nSkill 1. Use the Search API to look up relevant information by writing “[Search(term)]” where “term” is the search term you\\nwant to look up. For example:\\nQuestion: But what are the risks during production of nanomaterials?\\nAnswer (with Search): [Search(nanomaterial production risks)] Some nanomaterials may give rise to various kinds of lung\\ndamage.\\nQuestion: The colors on the flag of Ghana have the following meanings.\\nAnswer (with Search): Red is for [Search(Ghana flag red meaning)] the blood of martyrs, green for forests, and gold for\\nmineral wealth.\\nQuestion: Metformin is the first-line drug for what?\\nAnswer (with Search): [Search(Metformin first-line drug)] patients with type 2 diabetes and obesity.\\nSkill 2. Answer questions by thinking step-by-step. First, write out the reasoning steps, then draw the conclu-\\nsion. For example:\\nQuestion: When did the director of film Hypocrite (Film) die?'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 16, 'page_label': '17'}, page_content='Answer (with step-by-step): The film Hypocrite was directed by Miguel Morayta. Miguel Morayta died on 19 June 2013. So\\nthe answer is 19 June 2013.\\nQuestion: Are both Kurram Garhi and Trojkrsti located in the same country?\\nAnswer (with step-by-step): Kurram Garhi is located in the country of Pakistan. Trojkrsti is located in the country of\\nRepublic of Macedonia. Thus, they are not in the same country. So the answer is no.\\nQuestion: Do director of film Coolie No. 1 (1995 Film) and director of film The Sensational Trial have the same\\nnationality?\\nAnswer (with step-by-step): Coolie No. 1 (1995 film) was directed by David Dhawan. The Sensational Trial was directed by\\nKarl Freund. David Dhawan’s nationality is India. Karl Freund’s nationality is Germany. Thus, they do not have the same\\nnationality. So the answer is no.\\nQuestion: Who is Boraqchin (Wife Of Ögedei)’s father-in-law?'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 16, 'page_label': '17'}, page_content='Answer (with step-by-step): Boraqchin is married to Ögedei Khan. Ögedei Khan’s father is Genghis Khan. Thus, Boraqchin’s\\nfather-in-law is Genghis Khan. So the answer is Genghis Khan.\\nQuestion: Who was born first out of Martin Hodge and Ivania Martinich?\\nAnswer (with step-by-step): Martin Hodge was born on 4 February 1959. Ivania Martinich was born on 25 July 1995. Thus,\\nMartin Hodge was born first. So the answer is Martin Hodge.\\nQuestion: When did the director of film Laughter In Hell die?\\nAnswer (with step-by-step): The film Laughter In Hell was directed by Edward L. Cahn. Edward L. Cahn died on August 25,\\n1963. So the answer is August 25, 1963.\\nQuestion: Which film has the director died later, The Gal Who Took the West or Twenty Plus Two?\\nAnswer (with step-by-step): The film Twenty Plus Two was directed by Joseph M. Newman. The Gal Who Took\\nthe West was directed by Frederick de Cordova. Joseph M. Newman died on January 23, 2006. Fred de Cordova'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 16, 'page_label': '17'}, page_content='died on September 15, 2001. Thus, the person to die later from the two is Twenty Plus Two. So the answer is Twenty Plus Two.\\nQuestion: Who is the grandchild of Krishna Shah (Nepalese Royal)?\\nAnswer (with step-by-step): Krishna Shah has a child named Rudra Shah. Rudra Shah has a child named Prithvipati Shah.\\nThus, Krishna Shah has a grandchild named Prithvipati Shah. So the answer is Prithvipati Shah.\\nNow, combine the aforementioned two skills. First, write out the reasoning steps, then draw the conclusion,\\nwhere the reasoning steps should also utilize the Search API “[Search(term)]” whenever possible.\\nQuestion: Where did Minbyauk Thihapate’s wife die?\\nAnswer (with step-by-step & Search):'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 17, 'page_label': '18'}, page_content='Prompt D.4: exemplars of 2WikiMultihopQA\\nQuestion: When did the director of film Hypocrite (Film) die?\\nAnswer: The film Hypocrite was directed by Miguel Morayta. Miguel Morayta died on 19 June 2013. So the answer is 19\\nJune 2013.\\nQuestion: Are both Kurram Garhi and Trojkrsti located in the same country?\\nAnswer: Kurram Garhi is located in the country of Pakistan. Trojkrsti is located in the country of Republic of Macedonia.\\nThus, they are not in the same country. So the answer is no.\\nQuestion: Do director of film Coolie No. 1 (1995 Film) and director of film The Sensational Trial have the same\\nnationality?\\nAnswer: Coolie No. 1 (1995 film) was directed by David Dhawan. The Sensational Trial was directed by Karl Freund. David\\nDhawan’s nationality is India. Karl Freund’s nationality is Germany. Thus, they do not have the same nationality. So the\\nanswer is no.\\nQuestion: Who is Boraqchin (Wife Of Ögedei)’s father-in-law?'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 17, 'page_label': '18'}, page_content='Answer: Boraqchin is married to Ögedei Khan. Ögedei Khan’s father is Genghis Khan. Thus, Boraqchin’s father-in-law is\\nGenghis Khan. So the answer is Genghis Khan.\\nQuestion: Who was born first out of Martin Hodge and Ivania Martinich?\\nAnswer: Martin Hodge was born on 4 February 1959. Ivania Martinich was born on 25 July 1995. Thus, Martin Hodge was\\nborn first. So the answer is Martin Hodge.\\nQuestion: When did the director of film Laughter In Hell die?\\nAnswer: The film Laughter In Hell was directed by Edward L. Cahn. Edward L. Cahn died on August 25, 1963. So the\\nanswer is August 25, 1963.\\nQuestion: Which film has the director died later, The Gal Who Took the West or Twenty Plus Two?\\nAnswer: The film Twenty Plus Two was directed by Joseph M. Newman. The Gal Who Took the West was directed by\\nFrederick de Cordova. Joseph M. Newman died on January 23, 2006. Fred de Cordova died on September 15, 2001. Thus,'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 17, 'page_label': '18'}, page_content='the person to die later from the two is Twenty Plus Two. So the answer is Twenty Plus Two.\\nQuestion: Who is the grandchild of Krishna Shah (Nepalese Royal)?\\nAnswer: Krishna Shah has a child named Rudra Shah. Rudra Shah has a child named Prithvipati Shah. Thus, Krishna Shah\\nhas a grandchild named Prithvipati Shah. So the answer is Prithvipati Shah.\\nQuestion: Which country the director of film Citizen Mavzik is from?\\nAnswer:'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 18, 'page_label': '19'}, page_content='Prompt D.5: exemplars of StrategyQA\\nGenerate a yes or no answer to the following question.\\nQuestion: Do hamsters provide food for any animals?\\nAnswer: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So the final\\nanswer is yes.\\nGenerate a yes or no answer to the following question.\\nQuestion: Could Brooke Shields succeed at University of Pennsylvania?\\nAnswer: Brooke Shields went to Princeton University. Princeton University is about as academically rigorous as the Uni-\\nversity of Pennsylvania. Thus, Brooke Shields could also succeed at the University of Pennsylvania. So the final answer is yes.\\nGenerate a yes or no answer to the following question.\\nQuestion: Hydrogen’s atomic number squared exceeds number of Spice Girls?\\nAnswer: Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen’s atomic number\\nsquared is less than 5. So the final answer is no.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 18, 'page_label': '19'}, page_content='Generate a yes or no answer to the following question.\\nQuestion: Is it common to see frost during some college commencements?\\nAnswer: College commencement ceremonies can happen in December, May, and June. December is in the winter, so there\\ncan be frost. Thus, there could be frost at some commencements. So the final answer is yes.\\nGenerate a yes or no answer to the following question.\\nQuestion: Could a llama birth twice during War in Vietnam (1945-46)?\\nAnswer: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6 months.\\nThus, a llama could not give birth twice during the War in Vietnam. So the final answer is no.\\nGenerate a yes or no answer to the following question.\\nQuestion: Would a pear sink in water?\\nAnswer: The density of a pear is about 0.6g/cm3, which is less than water. Objects less dense than water float. Thus, a pear\\nwould float. So the final answer is no.\\nGenerate a yes or no answer to the following question.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 18, 'page_label': '19'}, page_content='Question: Did Spiderman fight against Falcon in the MCU?\\nAnswer:'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 19, 'page_label': '20'}, page_content='Prompt D.6: exemplars of ASQA\\nGiven an ambiguous question, figure out its interpretations and answer them one by one.\\nQuestion: Who played bonnie in gone with the wind?\\nAnswer: This question is ambiguous in terms of which version or adaptation of Gone with the Wind is being referred to. In\\norder to figure out its interpretations, we need to consider different versions or adaptations of Gone with the Wind. Gone with\\nthe Wind has two versions or adaptations: the 1939 film Gone with the Wind or the 2008 musical Gone with the Wind.\\nTherefore, this question has 2 interpretations: (1) Who played Bonnie in the 1939 film Gone with the Wind? (2) Who played\\nBonnie in the 2008 musical Gone with the Wind? The answers to all interpretations are: (1) The 1939 film Gone with the\\nWind’s character Bonnie was played by Eleanore Cammack \"Cammie\" King. (2) The 2008 musical Gone with the Wind’s\\ncharacter Bonnie was played by Leilah de Meza.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 19, 'page_label': '20'}, page_content='Given an ambiguous question, figure out its interpretations and answer them one by one.\\nQuestion: What is the second largest city in the usa?\\nAnswer: This question is ambiguous in terms of the criteria being used to determine the second largest city in the USA.\\nIn order to figure out its interpretations, we need to consider different criteria to determine a city’s size. City size can be\\nmeasured by two criteria: population or area. Therefore, this question has 2 interpretations: (1) What is the second largest\\ncity in the USA by population? (2) What is the second largest city in the USA by area? The answers to all interpretations are:\\n(1) The second largest city in the USA by population is Los Angeles, California. (2) The second largest city in the USA by\\narea is Juneau, Alaska.\\nGiven an ambiguous question, figure out its interpretations and answer them one by one.\\nQuestion: When was bohemian rhapsody released as a single?'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 19, 'page_label': '20'}, page_content='Answer: This question is ambiguous in terms of which country’s release of the single is being referred to. In order to figure\\nout its interpretations, we need to consider different countries where Bohemian Rhapsody is released. Bohemian Rhapsody\\nwas released in the United Kingdom and in the United States on different dates. Therefore, this question has 2 interpretations:\\n(1) When was Bohemian Rhapsody released as a single in the United Kingdom? (2) When was Bohemian Rhapsody released\\nas a single in the United States? The answers to all interpretations are: (1) Bohemian Rhapsody was released as a single in the\\nUnited Kingdom on 31 October 1975. (2) Bohemian Rhapsody was released as a single in the United States on December 1975.\\nGiven an ambiguous question, figure out its interpretations and answer them one by one.\\nQuestion: Where do the philadelphia eagles play their home games?'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 19, 'page_label': '20'}, page_content='Answer: This question is ambiguous in terms of which specific location or venue is being referred to. In order to figure out its\\ninterpretations, we need to consider the different possible locations or venues that could be considered the home field of the\\nPhiladelphia Eagles. These include the city, the sports complex, or the stadium. Therefore, this question has 3 interpretations:\\n(1) What city do the Philadelphia Eagles play their home games? (2) In what sports complex do the Philadelphia Eagles play\\ntheir home games? (3) What stadium do the Philadelphia Eagles play their home games? The answers to all interpretations\\nare: (1) Philadelphia Eagles play their home games in the city Philadelphia. (2) Philadelphia Eagles play their home games in\\nthe South Philadelphia Sports Complex. (3) Philadelphia Eagles play their home games in the Lincoln Financial Field stadium.\\nGiven an ambiguous question, figure out its interpretations and answer them one by one.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 19, 'page_label': '20'}, page_content='Question: When did xbox one come out in australia?\\nAnswer: This question is ambiguous in terms of which specific version of the Xbox One is being referred to. In order to\\nfigure out its interpretations, we need to consider the different versions of the Xbox One that have been released. Xbox One\\nhas two versions: the Xbox One video game console or the Xbox One X high-end model. Therefore, this question has 2\\ninterpretations: (1) When did the Xbox One release in Australia? (2) When did the Xbox One X release in Australia? The\\nanswers to all interpretations are: (1) The Xbox One video game console was released in Australia on November 22, 2013.\\n(2) The Xbox One X video game console was released in Australia on November 7, 2017.\\nGiven an ambiguous question, figure out its interpretations and answer them one by one.\\nQuestion: When does the movie summer of 84 come out?\\nAnswer: This question is ambiguous in terms of which release of the movie is being referred to. In order to figure out its'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 19, 'page_label': '20'}, page_content='interpretations, we need to consider different releases of the movie Summer of ’84. The movie Summer of ’84 is first released\\nat the Sundance Festival before it’s released throughout the US. Therefore, this question has 2 interpretations: (1) When\\ndid the movie Summer of ’84 first release at the Sundance Festival? (2) When did the movie Summer of ’84 first release\\nthroughout the US? The answers to all interpretations are: (1) Summer of ’84 was released at the Sundance Festival on\\nJanuary 22, 2018. (2) Summer of ’84 was released throughout the US on August 10, 2018.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 20, 'page_label': '21'}, page_content='Prompt D.7: exemplars of ASQA (cont.)\\nGiven an ambiguous question, figure out its interpretations and answer them one by one.\\nQuestion: What was roy orbison’s first number one hit?\\nAnswer: This question is ambiguous in terms of which specific chart or region is being referred to. In order to figure out its\\ninterpretations, we need to consider the different charts and regions where Roy Orbison’s music was popular. Roy Orbison is\\npopular in both the US Hot 100 and Canada, and the UK and Ireland. Therefore, this question has 2 interpretations: (1) What\\nwas Roy Orbison’s first number one hit in the US Hot 100 and Canada? (2) What was Roy Orbison’s first number one hit in\\nthe UK and Ireland? The answers to all interpretations are: (1) Running Scared was the first number one hit for Roy Orbison\\nin the US Hot 100 and Canada. (2) Only the Lonely (Know the Way I Feel) was the first number one hit for Roy Orbison in\\nthe UK and Ireland.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 20, 'page_label': '21'}, page_content='the UK and Ireland.\\nGiven an ambiguous question, figure out its interpretations and answer them one by one.\\nQuestion: What is the criminal’s name in the breakfast club?\\nAnswer: This question is ambiguous in terms of which specific name is being referred to - the character’s name or the actor’s\\nname. In order to figure out its interpretations, we need to consider both possibilities: the character’s name or the actor’s\\nname. Therefore, this question has 2 interpretations: (1) What is the criminal’s character name in The Breakfast Club? (2)\\nWhat is the the name of the actor who played the criminal in The Breakfast Club? The answers to all interpretations are: (1)\\nJohn Bender was the name of the criminal’s character in The Breakfast Club. (2) Judd Nelson was the actor of the criminal in\\nThe Breakfast Club.\\nGiven an ambiguous question, figure out its interpretations and answer them one by one.\\nQuestion: How many state parks are there in virginia?\\nAnswer:'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 21, 'page_label': '22'}, page_content='Prompt D.8: exemplars of ASQA-hint\\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpretations and\\nanswer them one by one.\\nQuestion: Who played bonnie in gone with the wind?\\nHint: This question is ambiguous in terms of which version or adaptation of Gone with the Wind is being referred to.\\nAnswer: In order to figure out its interpretations, we need to consider different versions or adaptations of Gone with the Wind.\\nGone with the Wind has two versions or adaptations: the 1939 film Gone with the Wind or the 2008 musical Gone with the\\nWind. Therefore, this question has 2 interpretations: (1) Who played Bonnie in the 1939 film Gone with the Wind? (2) Who\\nplayed Bonnie in the 2008 musical Gone with the Wind? The answers to all interpretations are: (1) The 1939 film Gone with\\nthe Wind’s character Bonnie was played by Eleanore Cammack \"Cammie\" King. (2) The 2008 musical Gone with the Wind’s\\ncharacter Bonnie was played by Leilah de Meza.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 21, 'page_label': '22'}, page_content='Given an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-\\ntions and answer them one by one.\\nQuestion: What is the second largest city in the usa?\\nHint: This question is ambiguous in terms of the criteria being used to determine the second largest city in the USA.\\nAnswer: In order to figure out its interpretations, we need to consider different criteria to determine a city’s size. City size can\\nbe measured by two criteria: population or area. Therefore, this question has 2 interpretations: (1) What is the second largest\\ncity in the USA by population? (2) What is the second largest city in the USA by area? The answers to all interpretations are:\\n(1) The second largest city in the USA by population is Los Angeles, California. (2) The second largest city in the USA by\\narea is Juneau, Alaska.\\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-\\ntions and answer them one by one.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 21, 'page_label': '22'}, page_content='Question: When was bohemian rhapsody released as a single?\\nHint: This question is ambiguous in terms of which country’s release of the single is being referred to.\\nAnswer: In order to figure out its interpretations, we need to consider different countries where Bohemian Rhapsody is\\nreleased. Bohemian Rhapsody was released in the United Kingdom and in the United States on different dates. Therefore,\\nthis question has 2 interpretations: (1) When was Bohemian Rhapsody released as a single in the United Kingdom? (2) When\\nwas Bohemian Rhapsody released as a single in the United States? The answers to all interpretations are: (1) Bohemian\\nRhapsody was released as a single in the United Kingdom on 31 October 1975. (2) Bohemian Rhapsody was released as a\\nsingle in the United States on December 1975.\\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-\\ntions and answer them one by one.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 21, 'page_label': '22'}, page_content='Question: Where do the philadelphia eagles play their home games?\\nHint: This question is ambiguous in terms of which specific location or venue is being referred to.\\nAnswer: In order to figure out its interpretations, we need to consider the different possible locations or venues that could be\\nconsidered the home field of the Philadelphia Eagles. These include the city, the sports complex, or the stadium. Therefore,\\nthis question has 3 interpretations: (1) What city do the Philadelphia Eagles play their home games? (2) In what sports\\ncomplex do the Philadelphia Eagles play their home games? (3) What stadium do the Philadelphia Eagles play their home\\ngames? The answers to all interpretations are: (1) Philadelphia Eagles play their home games in the city Philadelphia. (2)\\nPhiladelphia Eagles play their home games in the South Philadelphia Sports Complex. (3) Philadelphia Eagles play their\\nhome games in the Lincoln Financial Field stadium.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 21, 'page_label': '22'}, page_content='Given an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-\\ntions and answer them one by one.\\nQuestion: When did xbox one come out in australia?\\nHint: This question is ambiguous in terms of which specific version of the Xbox One is being referred to.\\nAnswer: In order to figure out its interpretations, we need to consider the different versions of the Xbox One that have been\\nreleased. Xbox One has two versions: the Xbox One video game console or the Xbox One X high-end model. Therefore, this\\nquestion has 2 interpretations: (1) When did the Xbox One release in Australia? (2) When did the Xbox One X release\\nin Australia? The answers to all interpretations are: (1) The Xbox One video game console was released in Australia on\\nNovember 22, 2013. (2) The Xbox One X video game console was released in Australia on November 7, 2017.\\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 21, 'page_label': '22'}, page_content='tions and answer them one by one.\\nQuestion: When does the movie summer of 84 come out?\\nHint: This question is ambiguous in terms of which release of the movie is being referred to.\\nAnswer: In order to figure out its interpretations, we need to consider different releases of the movie Summer of ’84. The\\nmovie Summer of ’84 is first released at the Sundance Festival before it’s released throughout the US. Therefore, this question\\nhas 2 interpretations: (1) When did the movie Summer of ’84 first release at the Sundance Festival? (2) When did the movie\\nSummer of ’84 first release throughout the US? The answers to all interpretations are: (1) Summer of ’84 was released at the\\nSundance Festival on January 22, 2018. (2) Summer of ’84 was released throughout the US on August 10, 2018.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 22, 'page_label': '23'}, page_content='Prompt D.9: exemplars of ASQA-hint (cont.)\\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpretations and\\nanswer them one by one.\\nQuestion: What was roy orbison’s first number one hit?\\nHint: This question is ambiguous in terms of which specific chart or region is being referred to.\\nAnswer: In order to figure out its interpretations, we need to consider the different charts and regions where Roy Orbison’s\\nmusic was popular. Roy Orbison is popular in both the US Hot 100 and Canada, and the UK and Ireland. Therefore, this\\nquestion has 2 interpretations: (1) What was Roy Orbison’s first number one hit in the US Hot 100 and Canada? (2) What\\nwas Roy Orbison’s first number one hit in the UK and Ireland? The answers to all interpretations are: (1) Running Scared\\nwas the first number one hit for Roy Orbison in the US Hot 100 and Canada. (2) Only the Lonely (Know the Way I Feel) was'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 22, 'page_label': '23'}, page_content='the first number one hit for Roy Orbison in the UK and Ireland.\\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-\\ntions and answer them one by one.\\nQuestion: What is the criminal’s name in the breakfast club?\\nHint: This question is ambiguous in terms of which specific name is being referred to - the character’s name or the actor’s\\nname.\\nAnswer: In order to figure out its interpretations, we need to consider both possibilities: the character’s name or the actor’s\\nname. Therefore, this question has 2 interpretations: (1) What is the criminal’s character name in The Breakfast Club? (2)\\nWhat is the the name of the actor who played the criminal in The Breakfast Club? The answers to all interpretations are: (1)\\nJohn Bender was the name of the criminal’s character in The Breakfast Club. (2) Judd Nelson was the actor of the criminal in\\nThe Breakfast Club.'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 22, 'page_label': '23'}, page_content='The Breakfast Club.\\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-\\ntions and answer them one by one.\\nQuestion: How many state parks are there in virginia?\\nHint: This question is ambiguous in terms of the time frame or period being referred to.\\nAnswer:'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 23, 'page_label': '24'}, page_content='Prompt D.10: exemplars of WikiAsp\\nGenerate a summary about Aslanhane Mosque including the following aspects: location, history with one aspect per line.\\n# Location\\nThe mosque is in the old quarter of ankara next to ankara castle. With an altitude of 947 metres (3,107 ft) it overlooks ankara\\nat 39°56’12\"N 32°51’55\"E.\\n# History\\nThe mosque is one of the oldest mosques in Turkey still standing. It was built during the reign of Mesud II of the Anatolian\\nSeljuks in 1290. Its architect was Ebubekir Mehmet. It was commissioned by two Ahi leaders named Hüsamettin and\\nHasaneddin. However, in 1330, it was repaired by another Ahi leader named ¸ Serafettin after whom the mosque was named.\\nAfter several minor repairs the mosque was restored by the directorate general of foundations in 2010-2013 term.\\nGenerate a summary about Untold Legends: The Warrior’s Code including the following aspects: reception,\\ngameplay, development with one aspect per line.\\n# Reception'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 23, 'page_label': '24'}, page_content='# Reception\\nThe game received \"mixed or average reviews\" according to video game review aggregator Metacritic.\\n# Gameplay\\nThe warrior’s code is a hack n’ slash action role-playing game, which concentrates on action-oriented combat.\\n# Development\\nAs a pre-order bonus, the game was shipped with a small action figure of the Guardian class.\\nGenerate a summary about Raid on St. Augustine including the following aspects: aftermath, background with\\none aspect per line.\\n# Aftermath\\nOnce the English had gone Menéndez and the rest of the Spanish settlers returned to find a smoldering ruins and very little\\nleft. He soon and begged for help from the viceroy of Cuba and the settlement took a while to build itself back up. The\\ndestroyed fort was replaced with the present day Castillo de San Marcos.\\n# Background\\nWar had already been unofficially declared by Philip II of Spain after the Treaty of Nonsuch in which Elizabeth I had'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 23, 'page_label': '24'}, page_content='offered her support to the rebellious Protestant Dutch rebels. The Queen through Francis Walsingham ordered Sir Francis\\nDrake to lead an expedition to attack the Spanish New World in a kind of preemptive strike. Sailing from Plymouth,\\nEngland, he struck first at Santiago in November 1585 then across the Atlantic at the Spanish new world city of Santo\\nDomingo of which was captured and ransomed on 1 January 1586 and following that successfully attacked the important\\ncity of Cartagena on 19 February. Drake wanted to strike at another Spanish city on the Main before finally visiting and\\nreplenishing Sir Walter Raleigh’s new colony of Roanoke Colony on the American East Coast. Then after this he hoped\\nto make the Transatlantic crossing back to England. The fleet headed north, and in late April Drake put into the Spanish\\nCuban mainland and his men dug wells in search of fresh water and gathered supplies to help counter an outbreak of'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 23, 'page_label': '24'}, page_content='dysentery after which he moved on. The fleet traveled north within sight of land on the Florida peninsula sailing past\\nthe West coast. On 27 May 1586 as they approached further north a small fort was spotted on the shore, with a small\\ninlet close by. This was the location of St Augustine, the most northerly town in Spain’s New World Empire, and the\\noldest permanent colonial settlement in North America. Drake knew of the place and was also aware of the fact that\\nthe spanish under Pedro Menéndez de Avilés had ordered all of the French Huguenot colonists that had tried to settle\\nin the area executed. Drake decided on one final opportunity to raid and plunder, and a chance to avenge his fellow Protestants.\\nGenerate a summary about Lakewood (Livingston, Alabama) including the following aspects: architecture, his-\\ntory with one aspect per line.\\n# Architecture\\nThe house has a plan that is relatively rare in early Alabama architecture. The plan features a brick ground floor that is topped'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 23, 'page_label': '24'}, page_content='by one-and-a-half-stories of wood-frame construction. The ground floor originally contained domestic spaces, with the\\nformal rooms on the principle floor and bedrooms on the upper floor. A central hallway is present on all levels. The facade is\\nfive bays wide, with central entrance doors on the ground and principle floors. The bays are divided by two-story Doric\\npilasters, with the middle third of the facade occupied by a two-tiered tetrastyle Doric portico. Two curved wrought iron\\nstaircases ascend from ground level to the front center of the upper portico, leading to the formal entrance.\\n# History\\nLakewood was built for Joseph lake, a native of North Carolina, by Hiram W. Bardwell, a master builder. Construction\\nwas completed in 1840. Located adjacent to the University of West Alabama, Julia Strudwick Tutwiler, a Lake relative,\\nperiodically resided in the house from 1881 to 1910 while she served as president of the university. It was then known as'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-10-24T00:50:47+00:00', 'author': '', 'keywords': '', 'moddate': '2023-10-24T00:50:47+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/pdfs/RAG-1.pdf', 'total_pages': 24, 'page': 23, 'page_label': '24'}, page_content='Livingston Normal College. The house was extensively photographed by Alex Bush for the Historic American Buildings\\nSurvey in November and December 1936. Lakewood has continued to be owned by descendants of the Lake family to the\\ncurrent day. The house and its surviving 10 acres (4.0 ha) of grounds were listed on the Places in Peril in 2012 due to the\\nimmediate threat of its acquisition by developers.\\nGenerate a summary about Carlos Moedas including the following aspects: biography, early life, political career\\nwith one aspect per line.'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='arXiv:2505.10792v2  [cs.CL]  19 May 2025\\nFinetune-RAG: Fine-Tuning Language Models to\\nResist Hallucination in Retrieval-Augmented\\nGeneration\\nZhan Peng Lee\\nPints AI Labs\\nzhanpeng.lee@pints.co\\nAndre Lin∗\\nPints AI Labs\\nandrelim444@gmail.com\\nCalvin Tan\\nPints AI Labs\\ncalvin@pints.co\\nAbstract\\nRetrieval-Augmented Generation (RAG) has emerged as a powerful framework to\\nimprove factuality in large language models (LLMs) by grounding their outputs in\\nretrieved documents. However, ensuring perfect retrieval of relevant information\\nremains challenging, and when irrelevant content is passed downstream to an LLM,\\nit can lead to hallucinations. In this work, we propose Finetune-RAG, a simple\\nand effective fine-tuning approach that features the first-of-its-kind RAG training\\ndataset constructed to mimic real-world imperfections. Experimental results show\\nthat Finetune-RAG improves factual accuracy by 21.2% over the base model. We'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='also propose Bench-RAG, an LLM-as-a-judge evaluation pipeline that stress tests\\nmodels under realistic imperfect retrieval scenarios. Our codebase2 and dataset3\\nare fully open sourced for community use.\\n1 Introduction\\nLarge Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of\\nnatural language processing tasks (Wang et al., 2023; Rozière et al., 2024; Cui et al., 2025; Yasunaga\\net al., 2022; Liu et al., 2024). However, their tendency to \"hallucinate\", that is, to produce fluent\\nbut factually incorrect information, remains a persistent challenge (Li et al., 2024a; Duan et al.,\\n2024; Zhang et al., 2023), particularly in high-stakes domains such as healthcare, law, and finance\\n(Agarwal et al., 2024; Dahl et al., 2024; Kang and Liu, 2023). To address this, Retrieval-Augmented\\nGeneration (RAG)has become a popular solution. Instead of relying solely on parametric memory,'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='RAG systems retrieve external documents and condition the model’s response on this evidence.\\nIn practice, retrieval accuracy in RAG is far from flawless. Retrieved documents may be outdated,\\nmisleading, or topically adjacent but factually incorrect. These errors can propagate downstream,\\nleading models to blend inaccurate context into fluent but false answers. This is especially concerning\\nin domains such as law, compliance, financial reporting, or medicine, where mistakes can have\\nwide-ranging repercussions.\\nMost prior work has addressed this issue from the retrieval perspective, focusing on improving\\nretrievers, reranking mechanisms, or applying filtering heuristics (Sawarkar et al., 2024; Dong et al.,\\n2024; Zhou and Chen, 2025). In contrast, relatively little attention has been given to improving the\\nmodel’s ability to resist using the incorrect information.\\nIn this paper, we introduce Finetune-RAG, a method that directly targets hallucination by fine-tuning'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='the model with imperfect RAG samples that mimic real-world retrieval scenarios. We constructed a\\ndiverse dataset covering legal documents, scientific literature, books, and web data, each paired with\\n∗Work was done during an internship at Pints AI\\n2https://github.com/Pints-AI/Finetune-Bench-RAG\\n3https://huggingface.co/datasets/pints-ai/Finetune-RAG'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='a plausible but fictitious counterpart. We then fine-tune instruction-tuned LLMs, specifically Meta’s\\nLlama 3.1-8B-Instruct (Grattafiori et al., 2024), on this dataset using two prompt variants: a Baseline\\nformat and a Structured XMLvariant. This setup allows us to assess generalization and prompt\\nsensitivity. To our knowledge, Finetune-RAG provides the first RAG dataset of its kind, as existing\\nRAG finetuning datasets implicitly assume perfect information retrieval, and mostly focus only the\\nLLM’s ability to extract coherent answers from relevant chunks.\\nOur key insight is that LLMs struggle to identify contextual clues that are obvious to the human eye,\\nsuch as financial reports from a similarly named company or outdated information based on dates\\nindicated by document metadata. Through fine-tuning models with a controlled mixture of true and\\nfalse context placed alongside, we teach them to ground their answers exclusively in the reliable\\ninformation provided.'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='We evaluated the effectiveness of Finetune-RAG usingBench-RAG, a custom benchmarking suite\\nwe have created that leverages GPT-4o (OpenAI, 2024) as an automated judge to assess the accu-\\nracy, relevance, helpfulness and depth of the LLM response. Our results show that Finetune-RAG\\nsubstantially improves factual correctness while maintaining output quality across other dimensions,\\ndemonstrating that generation-time defenses are a viable complement to improved retrieval.\\nOur contributions are as follows:\\n• Fine-tuning Approach. We propose a novel fine-tuning strategy for RAG systems that\\nteaches models to ignore misleading context and generate answers based solely on factual\\ninput.\\n• Training Dataset.We release a curated, multi-domain dataset designed for hallucination\\nresistance training, with both factual and fictitious content.\\n• Evaluation Setup.We benchmark the effectiveness of our approach using GPT-4o-based'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='evaluations and show significant gains in factual accuracy without compromising helpfulness\\nor relevance.\\n• Open-source release. We make our code, models, dataset, and evaluation framework\\npublicly available to facilitate further research. They can be accessed in our open-source\\nrepository4 and dataset5.\\nBy fine-tuning LLMs on RAG examples containing both factual and fictitious documents, we show\\nthat it is possible to build models that can reliably choose truth over noise. Our dataset reflects noisy,\\ndomain-diverse retrieval as encountered in practice, making it a strong foundation for stress-testing\\nhallucination resistance in future RAG systems.\\n2 Background\\n2.1 Retrieval-Augmented Generation\\nRetrieval-Augmented Generation (RAG) augments large language models by incorporating external\\ndocuments into the generation process. Rather than relying solely on the model’s internal parameters,\\nRAG retrieves relevant passages from a knowledge base and feeds them, along with the user query,'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='into the model to guide its response (Zhou et al., 2024).\\nA standard RAG system operates in two phases:\\n• Retrieval. A retriever model selects the top-k most relevant documents for a given query.\\n• Generation. A language model generates a response conditioned on both the query and the\\nretrieved documents.\\nThe appeal of RAG lies in its ability to dynamically access up-to-date or domain-specific information,\\nwhich is especially useful in fast-changing or specialized fields. However, it also introduces new\\nfailure modes, particularly when the retrieval quality is imperfect (Barnett et al., 2024).\\n4https://github.com/Pints-AI/Finetune-Bench-RAG\\n5https://huggingface.co/datasets/pints-ai/Finetune-RAG\\n2'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='2.2 Hallucination in Language Models\\nHallucination refers to the phenomenon where language models produce outputs that are factually\\nincorrect or unsupported by the input, resulting in unfaithful outputs (Rawte et al., 2023). In RAG\\nsystems, hallucination can be especially problematic when the model is presented with a mixture of\\nrelevant and irrelevant (or even misleading) context. Even with carefully worded prompts, models\\ncan inadvertently \"trust\" incorrect sources and generate plausible but wrong answers (Yoran et al.,\\n2024).\\nDespite the presence of external context, most current models lack mechanisms to actively filter or\\nignore misleading information once it is included in the prompt (Shi et al., 2023). Finetune-RAG\\nspecifically targets this weakness by training models to develop this filtering capability.\\n3 Related Works\\n3.1 Mitigating Hallucination with Synthetic Prompt Tuning\\nSYNTRA (Jones et al., 2023) reduces hallucinations in large language models by modifying the'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='model’s instructions rather than adjusting its internal weights. SYNTRA does this by attaching a\\nsmall, trainable embedding vector to the system message, which acts as an additional instruction\\nprefix. This vector is optimized using a synthetic task where hallucinations are easy to measure. For\\nexample, the model is prompted to return names starting with a specific letter from a visible list, and\\nany incorrect or invented names are counted as hallucinations. By learning to avoid such mistakes in\\na controlled setting, the model can generalize to reduce hallucinations in downstream tasks. However,\\nbecause SYNTRA focuses on modifying prompts and not the model’s internal reasoning, it does not\\nenable the model to distinguish between factual and misleading content, failing to address real-world\\nRAG scenarios (Barnett et al., 2024)(Shi et al., 2023).\\n3.2 Refusal-Aware Fine-Tuning\\nZhang et al. (2024) propose a fine-tuning method, R-Tuning, that teaches language models to express'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='uncertainty and decline to answer when a question falls outside their pre-trained knowledge. This is\\nachieved by identifying questions the model answers incorrectly during training and appending an\\nuncertainty statement such as “I am unsure” to those responses. The result is a model that behaves\\nmore conservatively and with improved confidence calibration. However, R-Tuning is designed for\\nclosed-book settings, where the model relies only on its internal knowledge without a RAG system.\\n3.3 Constrained Reasoning with Decompose-and-Query (D&Q)\\nCao et al. (2023) propose the Decompose-and-Query (D&Q) framework, which extends retrieval-\\naugmented generation (RAG) by teaching language models to break down complex queries, retrieve\\nrelevant information using external tools, and generate answers based on a structured knowledge\\nsource. In particular, D&Q introduces a curated question–answer (QA) base, which is a collection of'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='verified QA pairs that the model consults during reasoning. This setup helps reduce hallucinations\\nby constraining the model to reliable content and allowing it to backtrack when inconsistencies are\\ndetected.\\nHowever, the effectiveness of D&Q depends strongly on the quality and coverage of its QA base. In\\npractical RAG applications, where retrieved content can be noisy, ambiguous, or incomplete (Shi\\net al., 2023), relying on a fixed and curated source may become a limitation. Since the framework\\nlacks mechanisms to dynamically assess the reliability of new information, it remains susceptible to\\nhallucinations caused by misleading or inaccurate context.\\n4 Methodology\\nWe introduce Finetune-RAG, a fine-tuning method designed to train large language models (LLMs)\\nto distinguish between correct and fictitious context within a Retrieval-Augmented Generation (RAG)\\nsetup. Unlike prior work that attempts to improve factuality by enhancing the retrieval phase,'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='Finetune-RAG focuses on improving the model’s generation behavior when faced with imperfect or\\n3'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='misleading inputs. Our core idea is to fine-tune the model using examples where both correct and\\nincorrect information are explicitly presented to model, allowing it to learn the ability to sift out the\\ncorrect information to use for its response.\\n4.1 Problem Setup\\nIn a typical RAG system, the model is given a user query q and a set of retrieved documents\\n{d1, d2, ..., dk} (Zhou et al., 2024). When any of the documents is irrelevant or misleading, the model\\nmay generate incorrect responses (Yoran et al., 2024).\\nIn Finetune-RAG, we simulate this scenario during training by constructing prompts that include:\\n• One correct (factual) document chunk dcorrect\\n• One fictitious (misleading) document chunk dfictitious\\n• A corresponding question q\\n• A reference answer a, written using only dcorrect as the reference\\nThe model is then trained using supervised fine-tuning to produce the answer a despite having access\\nto both dcorrect and dfictitious in the input.'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='In Bayesian modeling, we can think of the task as a conditional generation problem where the goal is\\nto maximize the probability of generating a truthful answer a given a question q and a mixed set of\\ncontexts (some correct dcorrect, some fictitious dfictitious).\\nWe aim to model:\\nP(a | q, dcorrect, dfictitious) (1)\\nHowever, this is the observed conditional probability, and what we want the model to learn is to\\nignore dfictitious and generate the answer as if conditioned only on dcorrect. So our training objective is\\nto align to the following idealized posterior:\\nP∗(a | q, dcorrect, dfictitious) → P(a | q, dcorrect) (2)\\nIn other words, even though the model receives both correct and fictitious information, it must assign\\nzero (or negligible) attention/mass to dfictitious during decoding.\\n4.2 Prompt Construction\\nEach training example in Finetune-RAG is processed to include a system message and a user'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='message, following the standard instruction-tuning format (Ouyang et al., 2022) used in chat-style\\nlanguage models. The system message defines the behavior of the assistant, while the user message\\nprovides the question along with correct and fictitious information.\\n4.2.1 System Message\\nThe system message is consistent in all training examples. It instructs the assistant to rely solely on\\nthe provided context and discourages the use of prior knowledge or hallucination:\\n\"Some information is retrieved from the database as provided based on the\\nuser’s question. The assistant is to answer the question to the best of\\nhis/her ability, using only the information provided. The assistant must\\nnot add his/her own knowledge.\"\\n4.2.2 User Message\\nTo help the model distinguish between factual and fictitious context more effectively, we explore the\\nuse of XML-like (Bray et al., 1998) structured input. We hypothesize that introducing a consistent'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='and explicit hierarchy, where document chunks are clearly labeled and separated, can make it easier\\n4'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='for the model to parse and evaluate different sources of information. This is especially important in\\nRAG settings, where hallucinations often result from the model blending or misattributing content\\nacross documents. Our approach aligns with findings from recent work such as StructRAG (Li et al.,\\n2024b) and SRAG (Lin et al., 2025), which demonstrates that task-specific structured representations\\nsuch as tables or graphs can significantly improve the performance of LLMs on knowledge-intensive\\nreasoning tasks. Our use of XML aims to impose syntactic clarity and boundary enforcement at the\\ninput level.\\nTo test this, we compare two user message formats: an unstructuredBaseline Formatand a structured\\nXML Format. Both present a question along with two document chunks, one factual and one\\nfictitious, but differ in how the information is presented. Refer to Section 6.4 for the exact prompt\\nstructure.\\n5 Experimental Setup\\n5.1 Model'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='5.1 Model\\nWe fine-tuned Meta’s Llama 3.1–8B-Instruct (Grattafiori et al., 2024), an instruct-tuned model that\\nsupports chat-style interaction and long context windows. We adapt the system and user message\\nformatting based on the chosen prompt structure described in Section 4.2.2.\\n5.2 Dataset and Preprocessing\\nOur dataset contains a total of 1,653 examples from diverse domains, such as legal documents,\\nscientific papers, news articles, and technical reports. For the complete structure of each example in\\nthe dataset, refer to Annex A.\\nEach example is formatted in both the baseline and XML structures. The dataset is then partitioned\\ninto training (80%), validation (10%), and test (10%) sets.\\n5.3 Hyperparameters\\nWe selected hyperparameter values that balance model performance with computational efficiency.\\nRefer to Table 1 for the complete set of hyperparameters used.\\nTable 1: Fine-tuning hyperparameters used on Llama 3.1-8B-Instruct\\nParameter Value\\nSteps 20\\nBatch size 64'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='Batch size 64\\nLearning rate 2e-5\\nWarmup ratio 0.1\\nLR Scheduler Cosine decay\\nOptimizer AdamW\\nβ1 0.9\\nβ2 0.95\\nWeight decay 0.1\\nMixed precision BF16\\n5.4 Checkpoints and Reproducibility\\nWe have released the model checkpoints fine-tuned with both Baseline6 and XML7 formats on Hug-\\ngingFace. Each prompt structure has two repositories, and each repository contains five checkpoints,\\ntotaling 10 checkpoints each.\\n6https://huggingface.co/pints-ai/Llama-3.1-8B-Instruct-RAG_Baseline_tuned-1\\nhttps://huggingface.co/pints-ai/Llama-3.1-8B-Instruct-RAG_Baseline_tuned-2\\n7https://huggingface.co/pints-ai/Llama-3.1-8B-Instruct-RAG_XML_tuned-1\\nhttps://huggingface.co/pints-ai/Llama-3.1-8B-Instruct-RAG_XML_tuned-2\\n5'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='6 Evaluation\\nWe evaluate Finetune-RAG’s ability to generate factually accurate answers when presented with both\\ncorrect and fictitious context. Our evaluation framework focuses on measuring whether the model\\nis able to selectively use only the correct information, and we assess output quality across four key\\ndimensions.\\n6.1 Bench-RAG\\nWe adopt a custom benchmarking pipeline, namely Bench-RAG, using GPT-4o model (OpenAI,\\n2024) in a LLM-as-a-judge inspired by prior work(Zheng et al., 2023; Gu et al., 2025; Li et al., 2025).\\nUsing structured prompts to elicit consistent evaluations for each model output, we measure:\\n• Accuracy: A binary metric indicating whether the generated answer is factually correct and\\nbased solely on the correct chunk. (True/False)\\n• Helpfulness: A score from 1 to 10 assessing how useful the answer is in addressing the\\nuser’s question.\\n• Relevance: A score from 1 to 10 measuring how relevant the content is to the query.'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='• Depth: A score from 1 to 10 reflecting the level of detail or insight present in the answer.\\nEach generated output is rated using a structured prompt format, which requests scores across these\\ncategories and a brief justification. Refer to Appendix B for the full structure. This methodology\\ndraws from recent research demonstrating that LLMs can align closely with human preferences when\\nprompted properly, achieving high inter-rater agreement, i.e. multiple evaluators provide consistent\\nratings for the same outputs (Gu et al., 2025; Li et al., 2025).\\n6.2 Checkpoints Evaluated\\nFor each prompt structure, we evaluate all 10 model checkpoints saved during training (see Section\\n5.4). These checkpoints represent the model’s learning trajectory over the course of a single fine-\\ntuning epoch. At each checkpoint, we generate answers to the test dataset questions using both the\\ncorrect context dcorrect and the fictitious context dfictitious. The generated answers are then submitted to'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='the evaluator for scoring. Refer to Appendix B.1 and B.2 for the structure of the prompt used for\\nevaluation.\\n6.3 Results\\nWe report quantitative results from our fine-tuning experiments scored across 4 dimensions: factual\\naccuracy, helpfulness, relevance, and depth. Evaluation was performed using GPT-4o (OpenAI,\\n2024) as an LLM judge, as described in Section 6.1. We then aggregate the scores of each sequence\\nin the test dataset to derive the final evaluation result for each checkpoint:\\n¯Accuracy =\\n \\n1\\nntest\\nntestX\\ni=1\\n1[Accuracyi = T rue]\\n!\\n× 100% (3)\\n¯Helpfulness = 1\\nntest\\nntestX\\ni=1\\nHelpfulness i (4)\\n¯Relevance = 1\\nntest\\nntestX\\ni=1\\nRelevancei (5)\\n¯Depth = 1\\nntest\\nntestX\\ni=1\\nDepthi (6)\\nFigures 1 and 2 summarize performance trends across training steps. We observe consistent improve-\\nments in factual accuracy over time, particularly in the Baseline format. In most cases, gains in\\n6'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='accuracy are achieved without sacrificing helpfulness or relevance, and in later checkpoints, all four\\nmetrics reach strong levels of performance.\\nNotably, accuracy rises from 76.97% at step 0 to 98.18% at step 20 in the Baseline format, demon-\\nstrating the model’s increasing ability to ignore fictitious context. Helpfulness and depth also improve\\nsteadily, with a dip at the first generated checkpoint.\\nFigure 1: Evaluation results across training steps (Baseline format). Accuracy is plotted on the right\\ny-axis, and other metrics use the left y-axis.\\nStep Acc. (%) Help Rel. Depth\\n0 76.97 8.81 9.55 8.32\\n2 67.88 7.08 7.48 6.76\\n4 91.52 8.08 8.47 7.15\\n6 93.94 9.58 9.83 8.81\\n8 96.36 9.38 9.61 8.55\\n10 97.58 9.33 9.62 8.51\\n12 96.36 9.52 9.78 8.80\\n14 96.97 9.73 9.91 9.01\\n16 97.58 9.78 9.95 9.06\\n18 97.58 9.77 9.95 9.05\\n20 98.18 9.77 9.95 9.02 0 2 4 6 8 10 12 14 16 18 20\\n5\\n6\\n7\\n8\\n9\\n10\\nStep\\nScore\\n50\\n60\\n70\\n80\\n90\\n100\\nAccuracy (%)\\nHelpfulness Relevance Depth Accuracy'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='Figure 2: Evaluation results across training steps (XML format). Accuracy is plotted on the right\\ny-axis, and other metrics use the left y-axis.\\nStep Acc. Help Rel Depth\\n0 78.79 8.81 9.56 8.19\\n2 52.73 5.79 6.16 5.24\\n4 87.88 6.56 7.09 5.47\\n6 95.76 9.46 9.73 8.75\\n8 94.55 9.09 9.35 8.21\\n10 94.55 8.93 9.32 8.01\\n12 95.76 8.95 9.33 8.05\\n14 95.76 9.28 9.59 8.52\\n16 97.58 9.35 9.61 8.61\\n18 97.58 9.28 9.50 8.50\\n20 96.97 9.40 9.64 8.64 0 2 4 6 8 10 12 14 16 18 20\\n5\\n6\\n7\\n8\\n9\\n10\\nStep\\nScore\\n50\\n60\\n70\\n80\\n90\\n100\\nAccuracy (%)\\nHelpfulness Relevance Depth Accuracy\\n7'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='6.4 Ablation: Effect of Prompt Structure\\nTo assess the impact of prompt formatting on hallucination resistance, we perform an ablation study\\ncomparing two versions of Finetune-RAG: one trained using the Baseline formatand another using\\na more structured XML format. Both models were fine-tuned on the same dataset with identical\\nhyperparameters and evaluated using the same GPT-4o-based benchmarking pipeline.\\nPrompt Format Differences The Baseline format presents context in a flat, unstructured layout,\\nwhile the XML format uses nested tags to explicitly delineate retrieved content blocks (see Section\\n4.2.2). We hypothesized that structured formatting might help the model better separate and reason\\nabout distinct chunks.\\nBaseline Format This format presents the retrieved content in a plain and direct layout:\\nFilename: {filename1}\\nInformation:\\n{content1}\\nFilename: {filename2}\\nInformation:\\n{content2}\\nQuestion: {question}'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='XML Format This version wraps the content in an XML-like structure for clearer boundaries:\\n<Results>\\n<Result>\\n<Filename>{filename1}</Filename>\\n<Information>{content1}</Information>\\n</Result>\\n<Result>\\n<Filename>{filename2}</Filename>\\n<Information>{content2}</Information>\\n</Result>\\n</Results>\\nQuestion: {question}\\nResults As shown in Figures 1 and 2, both models demonstrate strong improvements over time.\\nHowever, the Baseline model consistently achieves higher accuracy and better overall scores in the\\nlater checkpoints:\\n• At step 20, the Baseline-tuned model achieves an accuracy of 98.18%, compared to 96.97%\\nfor the XML-tuned model.\\n• The Baseline-tuned model also maintains slightly higher scores for helpfulness (9.77 vs\\n9.40) and depth (9.02 vs 8.64).\\nInterpretation These results suggest that while XML-style formatting introduces clear structural\\nboundaries that aid human readers, it did not consistently outperform the simpler Baseline prompt. We'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='offer two possible explanations: (1) the model may have developed inductive biases from pretraining\\nthat favor interpreting flat, plain-text layouts, such as those seen in summaries or abstracts, and (2)\\nfine-tuning datasets used in LLaMA or similar models may have predominantly featured unstructured\\nprompts, making the model more adept at handling them.\\nThis suggests that while prompt formatting is an important factor, training data design and supervision\\nsignal play a larger role in hallucination resistance.\\n8'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='7 Discussion\\nOur results show that Finetune-RAG significantly improves a model’s ability to resist hallucinations\\nin a RAG setting, even when the prompt includes both correct and misleading context. Fine-tuning\\nwith dual-context examples leads to consistent improvements in factual accuracy, while preserving\\nhelpfulness, relevance, and depth.\\n7.1 Inductive Bias Emergence in Structure-Agnostic Learning\\nA significant and perhaps unexpected result in our study is that models trained on unstructured\\nprompts (Baseline format) performed better, especially in factual accuracy, compared to those trained\\nwith structured XML prompts. This challenges the common belief that clear structure always aids\\nreasoning. Instead, it suggests a deeper learning process, which involves the development of stronger\\nbuilt-in tendencies for selecting content when structure is absent. This raises a potential area that can\\nbe further researched upon.\\n7.2 Limitations'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='7.2 Limitations\\nDespite promising results, several limitations remain:\\n• Synthetic dataset generation: The fictitious content is generated using GPT-4o (OpenAI,\\n2024), which may introduce distributional artifacts that differ from real-world retrieval\\nerrors. Additionally, the size of the dataset can be further increased for effective fine-tuning\\nin larger models.\\n• Binary supervision: We treat hallucination as a binary decision at the generation level.\\nHowever, hallucination is often more nuanced, involving partial truths, omissions, or subtle\\nphrasing, which our current framework may not sufficiently address.\\n• Controlled context pairing: During training, each example includes exactly one correct\\nand one incorrect document chunk. This creates a simplified binary contrast that may not\\ngeneralize to real-world scenarios where multiple retrieved documents vary in quality. A\\nstronger training approach can be constructed using our existing dataset to create more'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='varied and robust scenarios that the model can train on.\\n• Compute requirements: While our method is simpler and less resource-intensive than\\nalternatives such as full retraining or reinforcement learning, it still requires access to a\\nhigh-memory GPU (e.g., H100) to fine-tune long-context models with large batch sizes.\\nThis may limit accessibility for some users or institutions.\\n7.3 Future Work\\nThere are several promising extensions to Finetune-RAG that could further improve its robustness\\nand applicability:\\n• Training with more in-context RAG: Real-world retrieval often returns more than two\\ndocuments, and the context window of LLMs are increasing rapidly. At the time of our\\nwork, we focused on relatively low context window of 8k, which would realistically be used\\nfor two to three RAG documents using up to 3k context window. With increasing context\\nwindow, future work can explore training with more RAG chunks to optimize LLMs RAG'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='performance even at high level of stresses caused by more retrieved chunks. To support this,\\nwe future-proofed our dataset by including two additional relevant chunks per example to\\nsupport generating more complex multi-document training scenarios.\\n• Joint retrieval-generation optimization: While Finetune-RAG focuses on improving the\\ngeneration component, combining it with learned retrieval mechanisms such as reranker-\\naware retrievers or contrastively trained retrievers could lead to further improvements in\\nfactual accuracy and context filtering.\\n• Multimodal extensions: Hallucination is not limited to text-based models. Ex-\\ntending Finetune-RAG to multimodal settings, such as image-caption retrieval or\\ncode+documentation generation, may help build more robust grounded systems in other\\ndomains.\\n9'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='• Evaluation on downstream tasks: While our benchmarking focuses on controlled hallucina-\\ntion settings, future work should assess Finetune-RAG’s impact on end-to-end performance\\nin downstream RAG applications such as open-domain question answering, legal document\\nsummarization, and domain-specific information retrieval.\\n8 Conclusion\\nIn this work, we present Finetune-RAG, a simple yet effective method for reducing hallucination in\\nRetrieval-Augmented Generation (RAG) through supervised fine-tuning. Rather than focusing on\\nretrieval quality, Finetune-RAG trains the generation model to rely solely on factual context while\\nignoring misleading information, with no architectural changes required.\\nWe constructed a diverse training set and evaluate using Bench-RAG, a technique that leverages\\nGPT-4o as an automatic judge. Results show substantial gains in factual accuracy while preserving\\nhelpfulness, relevance, and depth. Ablation studies further reveal that prompt structure subtly impacts'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='robustness, with less structured formats sometimes aiding discrimination.\\nDespite its simplicity, Finetune-RAG demonstrates that generation-stage fine-tuning can meaningfully\\nimprove hallucination resistance in noisy retrieval environments. We release our code, dataset, and\\ncheckpoints to support further research in this direction, and highlight future extensions including\\nmulti-document training, joint retrieval-generation optimization, and adaptation to multimodal tasks.\\n10'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='References\\nAgarwal, V ., Jin, Y ., Chandra, M., Choudhury, M. D., Kumar, S., and Sastry, N. (2024). Medhalu:\\nHallucinations in responses to healthcare queries by large language models.\\nBarnett, S., Kurniawan, S., Thudumu, S., Brannelly, Z., and Abdelrazek, M. (2024). Seven failure\\npoints when engineering a retrieval augmented generation system.\\nBray, T., Paoli, J., Sperberg-McQueen, C. M., Maler, E., and Yergeau, F. (1998). Extensible markup\\nlanguage (xml) 1.0. https://www.w3.org/TR/REC-xml/. W3C Recommendation.\\nCao, H., An, Z., Feng, J., Xu, K., Chen, L., and Zhao, D. (2023). A step closer to comprehensive\\nanswers: Constrained multi-stage question decomposition with large language models.\\nCui, M., Gao, P., Liu, W., Luan, J., and Wang, B. (2025). Multilingual machine translation with open\\nlarge language models at practical scale: An empirical study.\\nDahl, M., Magesh, V ., Suzgun, M., and Ho, D. E. (2024). Large legal fictions: Profiling legal'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='hallucinations in large language models. Journal of Legal Analysis, 16(1):64–93.\\nDong, J., Fatemi, B., Perozzi, B., Yang, L. F., and Tsitsulin, A. (2024). Don’t forget to connect!\\nimproving rag with graph-based reranking.\\nDuan, H., Yang, Y ., and Tam, K. Y . (2024). Do llms know about hallucination? an empirical\\ninvestigation of llm’s hidden states.\\nGrattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., and et al.\\n(2024). The llama 3 herd of models.\\nGu, J., Jiang, X., Shi, Z., Tan, H., Zhai, X., Xu, C., Li, W., Shen, Y ., Ma, S., Liu, H., Wang, S., Zhang,\\nK., Wang, Y ., Gao, W., Ni, L., and Guo, J. (2025). A survey on llm-as-a-judge.\\nJones, E., Palangi, H., Simões, C., Chandrasekaran, V ., Mukherjee, S., Mitra, A., Awadallah, A., and\\nKamar, E. (2023). Teaching language models to hallucinate less with synthetic tasks.\\nKang, H. and Liu, X.-Y . (2023). Deficiency of large language models in finance: An empirical\\nexamination of hallucination.'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='Li, D., Jiang, B., Huang, L., Beigi, A., Zhao, C., Tan, Z., Bhattacharjee, A., Jiang, Y ., Chen, C.,\\nWu, T., Shu, K., Cheng, L., and Liu, H. (2025). From generation to judgment: Opportunities and\\nchallenges of llm-as-a-judge.\\nLi, J., Chen, J., Ren, R., Cheng, X., Zhao, W. X., Nie, J.-Y ., and Wen, J.-R. (2024a). The dawn after\\nthe dark: An empirical study on factuality hallucination in large language models.\\nLi, Z., Chen, X., Yu, H., Lin, H., Lu, Y ., Tang, Q., Huang, F., Han, X., Sun, L., and Li, Y . (2024b).\\nStructrag: Boosting knowledge intensive reasoning of llms via inference-time hybrid information\\nstructurization.\\nLin, T., Zhu, Y ., Luo, Y ., and Tang, N. (2025). Srag: Structured retrieval-augmented generation for\\nmulti-entity question answering over wikipedia graph.\\nLiu, Y ., Shi, K., He, K., Ye, L., Fabbri, A., Liu, P., Radev, D., and Cohan, A. (2024). On learning to\\nsummarize with large language models as references. In Duh, K., Gomez, H., and Bethard, S.,'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='editors, Proceedings of the 2024 Conference of the North American Chapter of the Association\\nfor Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages\\n8647–8664, Mexico City, Mexico. Association for Computational Linguistics.\\nOpenAI (2024). Gpt-4o system card.\\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal,\\nS., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A.,\\nWelinder, P., Christiano, P., Leike, J., and Lowe, R. (2022). Training language models to follow\\ninstructions with human feedback.\\n11'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='Rawte, V ., Chakraborty, S., Pathak, A., Sarkar, A., Tonmoy, S. M. T. I., Chadha, A., Sheth, A. P., and\\nDas, A. (2023). The troubling emergence of hallucination in large language models – an extensive\\ndefinition, quantification, and prescriptive remediations.\\nRozière, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y ., Liu, J., Sauvestre, R.,\\nRemez, T., Rapin, J., Kozhevnikov, A., Evtimov, I., Bitton, J., Bhatt, M., Ferrer, C. C., Grattafiori,\\nA., Xiong, W., Défossez, A., Copet, J., Azhar, F., Touvron, H., Martin, L., Usunier, N., Scialom, T.,\\nand Synnaeve, G. (2024). Code llama: Open foundation models for code.\\nSawarkar, K., Mangal, A., and Solanki, S. R. (2024). Blended rag: Improving rag (retriever-\\naugmented generation) accuracy with semantic search and hybrid query-based retrievers. In 2024\\nIEEE 7th International Conference on Multimedia Information Processing and Retrieval (MIPR),\\nvolume 24, page 155–161. IEEE.'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='Shi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, E., Schärli, N., and Zhou, D. (2023). Large\\nlanguage models can be easily distracted by irrelevant context.\\nWang, Y ., Le, H., Gotmare, A. D., Bui, N. D. Q., Li, J., and Hoi, S. C. H. (2023). Codet5+: Open\\ncode large language models for code understanding and generation.\\nYasunaga, M., Ren, H., Bosselut, A., Liang, P., and Leskovec, J. (2022). Qa-gnn: Reasoning with\\nlanguage models and knowledge graphs for question answering.\\nYoran, O., Wolfson, T., Ram, O., and Berant, J. (2024). Making retrieval-augmented language models\\nrobust to irrelevant context.\\nZhang, H., Diao, S., Lin, Y ., Fung, Y . R., Lian, Q., Wang, X., Chen, Y ., Ji, H., and Zhang, T. (2024).\\nR-tuning: Instructing large language models to say ‘i don’t know’.\\nZhang, Y ., Li, Y ., Cui, L., Cai, D., Liu, L., Fu, T., Huang, X., Zhao, E., Zhang, Y ., Chen, Y ., Wang,\\nL., Luu, A. T., Bi, W., Shi, F., and Shi, S. (2023). Siren’s song in the ai ocean: A survey on'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='hallucination in large language models.\\nZheng, L., Chiang, W.-L., Sheng, Y ., Zhuang, S., Wu, Z., Zhuang, Y ., Lin, Z., Li, Z., Li, D., Xing,\\nE. P., Zhang, H., Gonzalez, J. E., and Stoica, I. (2023). Judging llm-as-a-judge with mt-bench and\\nchatbot arena.\\nZhou, J. and Chen, L. (2025). Openrag: Optimizing rag end-to-end via in-context retrieval learning.\\nZhou, Y ., Liu, Y ., Li, X., Jin, J., Qian, H., Liu, Z., Li, C., Dou, Z., Ho, T.-Y ., and Yu, P. S. (2024).\\nTrustworthiness in retrieval-augmented generation systems: A survey.\\n12'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}, page_content='A Dataset Example Format\\nAligning with Section 4.1, each example in the dataset is structured as follows:\\n{\\n\"content\": <factual chunk>,\\n\"filename\": <original document filename>,\\n\"fictitious_content\": <misleading chunk>,\\n\"fictitious_filename\": <filename of misleading chunk>,\\n\"question\": <user query>,\\n\"answer\": <GPT-4o generated answer based only on correct content>,\\n}\\nB Bench-RAG Prompt Structure\\nGiven both the correct and fictitious document chunks, the fine-tuned model checkpoints are used\\nto generate answers for questions on the test dataset. The outputs are stored in a structured jsonl\\nformat, with each entry containing the following fields:\\n{\\n\"filename\": <original document filename>,\\n\"content\": <factual chunk>,\\n\"question\": <user query>,\\n\"response\": <model’s generated answer>\\n}\\nWith these output, we curate a prompt for the four measurements derived from our evaluation.\\nB.1 System Message for Evaluation\\nAccuracy\\n\"Please act as an impartial judge and evaluate the quality of the'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}, page_content='response provided by an AI assistant to the user question displayed\\nbelow, based solely on a piece of information extracted from a file\\nprovided below. Your evaluation should consider the accuracy of the\\nresponse.\\nYou will check whether the response contains extra details not found\\nin the piece of information provided. If extra details are found,\\naccuracy is false. Otherwise, accuracy is true. Take note that if the\\nresponse partially addresses the question, but did not provide extra\\ndetails not found in the piece of information provided, the response\\nwill still be considered accurate (hence accuracy = true).\\nBegin your evaluation by providing a short explanation. Be as objective\\nas possible. After providing your explanation, you must rate the accuracy\\nwith true or false by strictly following this JSON format:\\n{\\n\"accuracy_explanation\":\\n<provide an explanation on accuracy, whether extra details\\noutside the content were found.>,\\n\"accuracy\": <true/false>\\n}\"\\nHelpfulness'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}, page_content='}\"\\nHelpfulness\\n\"Please act as an impartial judge and evaluate the quality of the\\nresponse provided by an AI assistant to the user question displayed\\n13'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}, page_content='below, based solely on a piece of information extracted from a file\\nprovided below. Your evaluation should consider the helpfulness of the\\nresponse.\\nYou will check whether the AI assistant is helpful in answering the\\nquestion based on the response.\\nBegin your evaluation by providing a short explanation. Be as objective\\nas possible. After providing your explanation, you must rate the\\nhelpfulness on a scale of 1 to 10 by strictly following this JSON format:\\n{\\n\"helpfulness_explanation\": <provide an explanation on helpfulness>,\\n\"helpfulness\": <score>\\n}\"\\nRelevance\\n\"Please act as an impartial judge and evaluate the quality of the\\nresponse provided by an AI assistant to the user question displayed\\nbelow, based solely on a piece of information extracted from a file\\nprovided below. Your evaluation should consider the relevance of the\\nresponse.\\nYou will check the relevance of the response by evaluating whether the\\nresponse fully addresses the question.'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}, page_content='Begin your evaluation by providing a short explanation. Be as objective\\nas possible. After providing your explanation, you must rate the\\nrelevance on a scale of 1 to 10 by strictly following this JSON format:\\n{\\n\"relevance_explanation\": <provide an explanation on relevance>,\\n\"relevance\": <score>\\n}\"\\nDepth\\n\"Please act as an impartial judge and evaluate the quality of the\\nresponse provided by an AI assistant to the user question displayed\\nbelow, based solely on a piece of information extracted from a file\\nprovided below. Your evaluation should consider the depth of the\\nresponse.\\nYou will check the depth of the response by evaluating the level of\\ndetail of the response in answering the question.\\nBegin your evaluation by providing a short explanation. Be as objective\\nas possible. After providing your explanation, you must rate the\\ndepth on a scale of 1 to 10 by strictly following this JSON format:\\n{\\n\"depth_explanation\": <provide an explanation on depth>,\\n\"depth\": <score>\\n}\"'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}, page_content='\"depth\": <score>\\n}\"\\nB.2 User Message for Evaluation\\nAll measurements utilizes the same user message structure for evaluation. Note that the content used\\nis the correct content, rather than the fictitious one:\\n[The Start of Provided Information Extracted from a File]\\n14'),\n",
              " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Zhan Peng Lee; Andre Lin; Calvin Tan', 'doi': 'https://doi.org/10.48550/arXiv.2505.10792', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2505.10792v2', 'source': '/content/pdfs/RAG-6.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15'}, page_content='Filename: {filename}\\nInformation: {content}\\n[The End of Provided Information]\\n[Question]\\n{question}\\n[The Start of Assistant’s Response]\\n{response}\\n[The End of Assistant’s Response]\\n15')]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a collection\n",
        "collection = client.collections.create(\n",
        "    name=\"PDFDocuments\",\n",
        "    vectorizer_config=weaviate.classes.config.Configure.Vectorizer.none()\n",
        ")"
      ],
      "metadata": {
        "id": "0zH5mnte1nmn"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert chunks manually\n",
        "for i, doc in enumerate(docs):\n",
        "    collection.data.insert(\n",
        "        properties={\"text\": doc.page_content, \"source\": doc.metadata.get(\"source\")},\n",
        "        vector=embeddings.embed_query(doc.page_content)\n",
        "    )"
      ],
      "metadata": {
        "id": "dqOUbl2K12w9"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# similarity search"
      ],
      "metadata": {
        "id": "7603N5oQ4PwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is RAG?\"\n",
        "query_vector = embeddings.embed_query(query)\n",
        "\n",
        "results = collection.query.near_vector(\n",
        "    near_vector=query_vector,\n",
        "    limit=3\n",
        ")\n",
        "\n",
        "for o in results.objects:\n",
        "    print(\"🔹\", o.properties[\"text\"][:400])\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6Dxwf2g3DcN",
        "outputId": "9fe17156-f6f6-4f55-a3b7-1ad90c57318c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 into the model to guide its response (Zhou et al., 2024).\n",
            "A standard RAG system operates in two phases:\n",
            "• Retrieval. A retriever model selects the top-k most relevant documents for a given query.\n",
            "• Generation. A language model generates a response conditioned on both the query and the\n",
            "retrieved documents.\n",
            "The appeal of RAG lies in its ability to dynamically access up-to-date or domain-specific inf\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 the model with imperfect RAG samples that mimic real-world retrieval scenarios. We constructed a\n",
            "diverse dataset covering legal documents, scientific literature, books, and web data, each paired with\n",
            "∗Work was done during an internship at Pints AI\n",
            "2https://github.com/Pints-AI/Finetune-Bench-RAG\n",
            "3https://huggingface.co/datasets/pints-ai/Finetune-RAG\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 varied and robust scenarios that the model can train on.\n",
            "• Compute requirements: While our method is simpler and less resource-intensive than\n",
            "alternatives such as full retraining or reinforcement learning, it still requires access to a\n",
            "high-memory GPU (e.g., H100) to fine-tune long-context models with large batch sizes.\n",
            "This may limit accessibility for some users or institutions.\n",
            "7.3 Future Work\n",
            "T\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template=\"\"\"You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrived context to answer the question.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Use ten sentences maximum and keep the answer concise.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\""
      ],
      "metadata": {
        "id": "tWpfz8654sP1"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "2RzozOUk5-DO"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GsU3BJ56G44",
        "outputId": "59c6a88b-fd9d-4c81-e54c-9c86604d7aa9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks.\\nUse the following pieces of retrived context to answer the question.\\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\nUse ten sentences maximum and keep the answer concise.\\n{context}\\nQuestion: {question}\\nHelpful Answer:\"), additional_kwargs={})])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFaceHub"
      ],
      "metadata": {
        "id": "d4jT20rP6Ky8"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the model using Hugging-FACE API"
      ],
      "metadata": {
        "id": "Jygwseqg7yO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "huggingfacehub_api_token= userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "4uptCCl-7jel"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = HuggingFaceHub(\n",
        "    huggingfacehub_api_token=huggingfacehub_api_token,\n",
        "    repo_id=\"mistralai/Mistral-7B-v0.1\",\n",
        "    model_kwargs={\"temperature\":1, \"max_length\":180}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2YW2qTk7EMi",
        "outputId": "3f779a54-163f-4251-928b-6ec4c9692bf3"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2949055110.py:1: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
            "  model = HuggingFaceHub(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate response"
      ],
      "metadata": {
        "id": "Rpgj27sY75mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema.output_parser import StrOutputParser"
      ],
      "metadata": {
        "id": "w23hSsl473oZ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output parser\n",
        "output_parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "GS9RiR4S8ctn"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# creating a custom retriever"
      ],
      "metadata": {
        "id": "g8k3mneu9Uug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document\n",
        "\n",
        "def v4_retriever_texts(query: str, collection, embeddings, k=3):\n",
        "    # Embed the query\n",
        "    query_vector = embeddings.embed_query(query)\n",
        "\n",
        "    # Query Weaviate v4\n",
        "    results = collection.query.near_vector(\n",
        "        near_vector=query_vector,\n",
        "        limit=k\n",
        "    )\n",
        "\n",
        "    # Return list of strings\n",
        "    return [obj.properties[\"text\"] for obj in results.objects]\n"
      ],
      "metadata": {
        "id": "N6_DIEnu8yQJ"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever= v4_retriever_texts(query, collection, embeddings, k=3)\n",
        "\n",
        "rag_chain =(\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | output_parser\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "8mrcVifc8Hbe",
        "outputId": "411fa4b8-41c3-44b8-e74e-e567cc305a00"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'list'>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3699478949.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m rag_chain =(\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;34m{\u001b[0m\u001b[0;34m\"context\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretriever\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"question\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRunnablePassthrough\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m|\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m|\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m__ror__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    656\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mRunnable\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \"\"\"\n\u001b[0;32m--> 658\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mRunnableSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoerce_to_runnable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m     def pipe(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36mcoerce_to_runnable\u001b[0;34m(thing)\u001b[0m\n\u001b[1;32m   6211\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mRunnableLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Callable[[Input], Output]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6212\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6213\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Runnable[Input, Output]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnableParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6214\u001b[0m     msg = (\n\u001b[1;32m   6215\u001b[0m         \u001b[0;34mf\"Expected a Runnable, callable or dict.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, steps__, **kwargs)\u001b[0m\n\u001b[1;32m   3794\u001b[0m         \u001b[0mmerged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3795\u001b[0m         super().__init__(\n\u001b[0;32m-> 3796\u001b[0;31m             \u001b[0msteps__\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcoerce_to_runnable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmerged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3797\u001b[0m         )\n\u001b[1;32m   3798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36mcoerce_to_runnable\u001b[0;34m(thing)\u001b[0m\n\u001b[1;32m   6216\u001b[0m         \u001b[0;34mf\"Instead got an unsupported type: {type(thing)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6217\u001b[0m     )\n\u001b[0;32m-> 6218\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'list'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is RAG-system?\"\n",
        "retrieved_texts = retriever._invoke(query)  # returns list of strings\n",
        "\n",
        "# Combine context into a single string\n",
        "context = \"\\n\".join(retrieved_texts)\n",
        "\n",
        "prompt_input = {\"context\": context, \"question\": query}\n",
        "\n",
        "result = rag_chain.invoke(prompt_input)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "fOtuHQOj8XSI",
        "outputId": "86cd765b-9614-4cf2-a049-709f8e6931a5"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'V4WeaviateRetriever' object has no attribute '_invoke'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2601915299.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"What is RAG-system?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mretrieved_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretriever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# returns list of strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Combine context into a single string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretrieved_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    989\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m                         \u001b[0;31m# this is the current error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 991\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{type(self).__name__!r} object has no attribute {item!r}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'V4WeaviateRetriever' object has no attribute '_invoke'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d7AfoWjk92Rq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}